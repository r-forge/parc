\section{The snow Package}
\label{sec:snow}
The aim of simple network of workstations
(\pkg{snow}---\cite{rossini03snow}, \cite{tierney07snow}) is to
provide a simple parallel computing environment in R. To make a
collection of computers to appear as a virtual cluster in R \pkg{snow}
three different message passing environments can be used:

\begin{itemize}
\item PVM via R package rpvm (see section \ref{sec:rpvm})
\item MPI via R package Rmpi (see section \ref{sec:Rmpi})
\item SOCK via TCP sockets
\end{itemize}

The details of which mechanism is used and how it is used are hidden
from the high level user.
After setting up this virtual cluster developing parallel R functions
can be achieved via an standardized interface to the computation
nodes.
Moreover, when using \pkg{snow} one can rely on a good handful of
high level functions. This makes it rather easy to use the underlying
parallel computational engine.
Indeed \pkg{snow} uses existing interfaces to R namely \pkg{Rmpi} when
using MPI (see Section~\ref{sec:Rmpi}), \pkg{rpvm} when using PVM (see
Section~\ref{sec:rpvm}) and a new possibilty of message passing namely
TCP sockets, which is a rather simple way of achieving communication
between nodes (in most application this is not the optimal way).
What follows is a description of high level functions supplied by the
\pkg{snow} package. They are assigned to one of the topics:

\begin{itemize}
\item Initialization
\item Built-in High Level Functions
\item Fault Tolerance
\end{itemize}

\subsection{Initialization}

Initializing a \pkg{snow} cluster is rather easy if the system is
prepared accordingly. When using MPI (achieved through \pkg{Rmpi}) a
LAM/MPI environment has to be booted prior starting the virtual
cluster (see section \ref{sec:Rmpi}). Is PVM the method of choice the
\pkg{rpvm} package must be available and an appropriate PVM has to be
started (see section \ref{sec:rpvm}). For both MPI and PVM the
parallel environment 
can be configured through a grid engine (see appendix
\ref{app:gridengine}). TCP sockets can be set up directly using the
package. MPI or PVM have the possibility to query the status of the
parallel environment. This can be done using the functions supplied
from the corresponding package.


\pkg{snow} management functions:

\begin{description}
\item[\code{makecluster(spec, type = getClusterOption(``type''))}]
  starts a cluster of type \code{type} with \code{spec} numbers of
  slaves. If the cluster is of connection type SOCK then \code{spec}
  must be a charactor vector containing the hostnames of the
  slavenodes to join the cluster. The return value is a list
  containing the cluster specifications. This object is necessary in
  further function calls.
\item[\code{stopCluster(cl)}] stops a cluster specified in \code{cl}.
\end{description}


\begin{Example} Start/stop cluster in \pkg{snow} \newline
running on cluster@WU using the node.q -- the parallel environment was
started with the SGE using 8 nodes.

\begin{Schunk}
\begin{Sinput}
> library("snow")
> set.seed(1782)
> n <- 8
> cl <- makeCluster(n, type = "MPI")
\end{Sinput}
\begin{Soutput}
	8 slaves are spawned successfully. 0 failed.
\end{Soutput}
\begin{Sinput}
> stopCluster(cl)
\end{Sinput}
\begin{Soutput}
[1] 1
\end{Soutput}
\end{Schunk}
\label{ex:snowstartstop}
\end{Example}

\subsection{Built-in High Level Functions}

\pkg{snow} provides a good handful of high-level functions: 


%% TODO
\begin{description}
\item[\code{clusterEvalQ(cl, expr)}] evaluates an R expression
  \code{expr} on
  each cluster node provided by \code{cl}. 
\item[\code{clusterCall(cl, fun, ...)}] calls a function
  \code{fun} with arguments \ldots on each node found in \code{cl}
  and returns a list of the results.
\item[\code{clusterApply(cl, x, fun, ...)}] applies a function
  \code{fun} with additional arguments \ldots to a specific part of
  a vector \code{x}. The return value is of type list with the same
  length as of \code{x}. The length of
  \code{x} must not exceed the 
  number of R slaves spawned as each element of the vector is used
  exactly by one slave. To achieve some sort of load balancing please
  use the corresponding apply functions below.
\item[\code{clusterApplyLB(cl, x, fun, ...)}] is a load balancing
  version of \code{clusterApply()} which applies a function
  \code{fun} with additional arguments \ldots to a specific part of
  a vector \code{x} with the difference that the length of
  \code{x} can exceed the number of cluster nodes. If a node
  finished with the computation the next job is placed on the
  available node. This is repeated until all jobs have completed.
\item[\code{clusterExport(cl, list)}] broadcasts a list of global
  variables on the master (\code{list}) to all slaves.
\item[\code{parApply(cl, x, fun, ...)}] is one of the parallel
  versions of the \code{apply} functions available in R. We refer to
  the package documentation (\cite{tierney07snow}) for further details.
\item[\code{parMM(cl, A,B)}] is a simple parallel implementation of
  matrix multiplication. 
\end{description}


\begin{Example} Using high-level functions of snow\newline
running on cluster@WU using the node.q -- the parallel environment was
started with SGE using 8 nodes

\begin{Schunk}
\begin{Sinput}
> n <- 8
> cl <- makeCluster(n, type = "MPI")
\end{Sinput}
\begin{Soutput}
	8 slaves are spawned successfully. 0 failed.
\end{Soutput}
\begin{Sinput}
> x <- rep(n, n)
> rows <- clusterApply(cl, x, runif)
> X <- matrix(unlist(rows), ncol = n, byrow = TRUE)
> X
\end{Sinput}
\begin{Soutput}
           [,1]      [,2]     [,3]      [,4]      [,5]      [,6]       [,7]
[1,] 0.03582783 0.3065854 0.228593 0.2512860 0.1055006 0.1938628 0.05882335
[2,] 0.03582783 0.3065854 0.228593 0.2512860 0.1055006 0.1938628 0.05882335
[3,] 0.03582783 0.3065854 0.228593 0.2512860 0.1055006 0.1938628 0.05882335
[4,] 0.03582783 0.3065854 0.228593 0.2512860 0.1055006 0.1938628 0.05882335
[5,] 0.03582783 0.3065854 0.228593 0.2512860 0.1055006 0.1938628 0.05882335
[6,] 0.03582783 0.3065854 0.228593 0.2512860 0.1055006 0.1938628 0.05882335
[7,] 0.03582783 0.3065854 0.228593 0.2512860 0.1055006 0.1938628 0.05882335
[8,] 0.03582783 0.3065854 0.228593 0.2512860 0.1055006 0.1938628 0.05882335
          [,8]
[1,] 0.3884308
[2,] 0.3884308
[3,] 0.3884308
[4,] 0.3884308
[5,] 0.3884308
[6,] 0.3884308
[7,] 0.3884308
[8,] 0.3884308
\end{Soutput}
\end{Schunk}
\label{ex:snowapply}
\end{Example}

\subsection{Fault Tolerance}



\subsection{Conclusion}

The routines available in package \pkg{snow} are easy to understand
and use, provided that there is a corresponding communication
environment set up. Generally, the user need not know the underlying
parallel infrastructure, she just ports her sequential code so that it
uses the functions supplied by \pkg{snow}. All in all as the title
suggests simple network of workstations is simple to get started with
and is simple with respect to the possibilities of parallel
computations. 

For further interface functions supplied by the \pkg{snow} package, a
more detailed 
description and further examples please consult the package description
\cite{tierney07snow}.

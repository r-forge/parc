%% For standalone compile

\documentclass[12pt,a4paper]{report}
% zur Kontrolle des Umbruchs Klassenoption draft verwenden
%\usepackage[ngerman]{babel}
\usepackage[latin1]{inputenc}
%\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{latexsym}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{ifpdf}
\usepackage{url}
%% \usepackage{/usr/share/R/share/texmf/Sweave} see below new environment
\usepackage[round]{natbib}
\usepackage{algorithm}
\usepackage{algorithmic}

% Seitenlayout
% DIV# gibt den Divisor für die Layoutberechnung an.
% Vergrößern des Divisors vergrößert den Textbereich.
% BCOR#cm gibt die Breite des Bundstegs an.
%\usepackage[DIV11,BCOR2cm]{typearea}

% Abstand obere Blattkante zur Kopfzeile ist 2.54cm - 15mm
\setlength{\topmargin}{-5mm}


%% commands, environments, etc.
\let\code=\texttt
\newcommand{\pkg}[1]{{\normalfont\fontseries{b}\selectfont #1}}
\newcommand{\proglang}[1]{\textsf{#1}}
\newcommand{\class}[1]{`\code{#1}'}


\newtheorem{Example}{Example}[chapter]

%% example for newenvironment
%% \newenvironment{Proof}{\noindent{Proof:}}{%
%%    \hspace*{\fill}$\Box$\par\vskip2ex}

\RequirePackage{fancyvrb}
\RequirePackage{color}

\newenvironment{Schunk}{\par\begin{minipage}{\textwidth}}{\end{minipage}}

\definecolor{Sinput}{rgb}{0,0,0.56}
\definecolor{Scode}{rgb}{0,0,0.56}
\definecolor{Soutput}{rgb}{0.56,0,0}

\DefineVerbatimEnvironment{Sinput}{Verbatim}
{formatcom={\color{Sinput}},fontsize=\small}
\DefineVerbatimEnvironment{Soutput}{Verbatim}
{formatcom={\color{Soutput}},fontsize=\small}
\DefineVerbatimEnvironment{Scode}{Verbatim}
{formatcom={\color{Scode}},fontsize=\small}


\begin{document}

%%% standalone compile

%% new commands for algorithmics environment
\newcommand{\NROW}{\textbf{nrow}}
\newcommand{\NCOL}{\textbf{ncol}}

\chapter{Matrix Multiplication}
\label{chap:matrix}
\section{Introduction}

If we think of applications in parallel computing matrix
multiplication comes into mind. Because of its nature it is a prime
example for data parallelism. There has been done a lot of work in
this area. E.g., \cite{golub96mc} give a comprehensive introduction to the
field of matrix computation. 


In this chapter a short introduction to one of the fundamental methods
in linear algebra namely matrix multiplication is
given. Subsequently an implementation of a serial version of the matrix
multiplication algorithm is compared with implementations from high
performance linear algebra libraries. The remaining parts of this
chapter deals with the parallel implementation of matrix
multiplication and uses it as a task to benchmark parallel
multiplication routines provided from packages presented in
Chapter~\ref{chap:Rhpc}. Algorithms from the R package \pkg{paRc} are
explained in more detail as they have been developed in the course of
this thesis. Eventually, results of the comparisons are presented.

\section{Notation}

$ \mathbb{R} $ denotes the set of real numbers and $ \mathbb{R}^{m
  \times n} $ the vector space of all m-by-n real matrices.

$$ A \in \mathbb{R}^{m \times n} \Longleftrightarrow A = (a_{ij}) = 
\left( \begin{array}{ccc}
a_{11} & \ldots & a_{1n} \\
\vdots &        & \vdots \\
a_{m1} & \ldots & a_{mn}
\end{array} \right)
a_{ij} \in \mathbb{R}
 $$
The lower case letter of the letter which denotes the matrix with
subsripts $ij$ refers to the entry in the matrix. 

\subsection{Column and Row Partitioning}

A matrix $A$ can be accessed through its rows as it is a stack of row
vectors:
$$ A \in \mathbb{R}^{m \times n} \Longleftrightarrow A = 
\left( \begin{array}{c}
a_{1}^T \\
\vdots \\
a_{m}^T 
\end{array} \right)
a_{k} \in \mathbb{R}^n 
$$
This is called a \textit{row partition} of $A$.


If $A \in \mathbb{R}^{m \times n}$ the $k$th row of $A$ can be notated
as $A(k,)$ (according to the row access in R). I.e.,

$$ A(k,) = \left( \begin{array}{ccc}
a_{k1}, & \ldots, & a_{kn}
\end{array} \right)
$$
The other alternative is to see a matrix as a collection of column
vectors:
$$ A \in \mathbb{R}^{m \times n} \Longleftrightarrow A = 
\left( \begin{array}{ccc}
a_{1}, & \ldots, & a_{n}
\end{array} \right)
a_{k} \in \mathbb{R}^m 
 $$
This is called a \textit{column partition} of $A$.

If $A \in \mathbb{R}^{m \times n}$ the $k$th column of $A$ can be notated
as $A(,k)$ (according to the column access in R). I.e.,

$$ A(,k) = \left( \begin{array}{c}
a_{1k} \\
\vdots \\
a_{mk} 
\end{array} \right)
 $$

\subsection{Block Notation}

Block matrices are central in many algorithms. They have become very
important in high performance computing because it enables
easy distributing of data. 

In general a $m$ by $n$ matrix $A$ can be partitioned to obtain

$$ 
A = \left( \begin{array}{ccc}
A_{11} & \ldots & A_{1q} \\
\vdots &        & \vdots \\
A_{p1} & \ldots & A_{pq}
\end{array} \right)
$$

$A_{ij}$ is the $(i,j)$ block or submatrix with dimensions $m_i$ by
$n_j$ of $A$. $\sum_{i=1}^p m_i = m$ and $\sum_{j=1}^q n_j = n$. 
We can say that $A = A_{ij}$ is a $p$ by $q$ block
matrix.   

Now we can see that column and row partitionings are special cases of
block matrices.

\section{Basic Matrix Multiplication Algorithm}


\subsection{Dot Product Matrix Multiply}
In the usual matrix multiplication procedure the array $C$ is computed
through dot products one at a time from left to right and top to
bottom order.

 ($\mathbb{R}^{m \times r} \times
\mathbb{R}^{r \times n} \to \mathbb{R}^{m \times n}$)

$$ C = AB \Longrightarrow c_{ij} = \sum_{k=1}^r a_{ik}b_{kj} $$

Algorithm~\ref{alg:basicmm} shows the implementation of the basic dot
product matrix multiplication.

\begin{algorithm}
\caption{Basic matrix multiplication algorithm}
\label{alg:basicmm}
\begin{algorithmic}[1]

  \REQUIRE $A \in \mathbb{R}^{m \times r}$ and $B \in \mathbb{R}^{r
    \times n}$.

  \STATE $m \leftarrow \NROW (A)$
  \STATE $r \leftarrow \NCOL (A)$
  \STATE $n \leftarrow \NROW (B)$

  \FOR{$i = 1:m$}
    \FOR{$j = 1:n$}
       \FOR{$k = 1:r$}
         \STATE $C(i,j) \leftarrow A(i,k)B(k,j)$
       \ENDFOR
    \ENDFOR
  \ENDFOR
  \RETURN $C$

\end{algorithmic}
\end{algorithm}

%\subsubsection{Gaxpy Matrix Multiply}

%\subsubsection{Outer Product MAtrix Multiply}

\subsection{C Implementation}

It is rather easy to implement Algorithm~\ref{alg:basicmm} in C. 
The following example shows the serial implementation of the dot
product matrix multiplication in package \pkg{paRc}. It can be called
from R using \code{serial.matrix.mult(X,Y)} where \code{X} and
\code{y} are the matrices to be multiplied:

\begin{Example}
\label{ex:serialmm}
\begin{Scode}
void Serial_matrix_mult( double *x, int *nrx, int *ncx,
			 double *y, int *nry, int *ncy,
			 double *z) {
  int i, j, k;
  double sum;

  for(i = 0; i < *nrx; i++)
    for(j = 0; j < *ncy; j++){
      sum = 0.0;
      for(k = 0; k < *ncx; k++)
	sum += x[i + k**nrx]*y[k + j**nry];
      z[i + j**nrx] = sum;
    }
}
\end{Scode}
\end{Example}

\subsection{Using Basic Linear Algebra Subprograms}
\label{sec:blas}
In Section~\ref{sec:processorandmemory} we pointed out that
parallelism can be achieved on the instruction level. Furthermore, we
discussed  the importance of data locality in memory hierarchies. The
Basic Linear Algebra Subprograms (BLAS) standard proposed by
\cite{lawson79bla} consist of several basic operations (e.g., vector
addition, dot product, etc.) utilizing the underlying architecture
optimal. Most attention is paid to the principle of data locality as
it will promise the highest performance gain. This can be achieved by
moving the block of data once up the memory hierarchie, performing all
necessary operations on it and moving the data back to the main memory
and proceeding with the next block. The BLAS routines were used in the
development of linear algebra libraries for solving a number of
standard problems.

We used the following BLAS libraries in benchmarks of  the matrix
multiplication problem:

\begin{description}
\item[refblas] is the official implementation from netlib
  (\url{http://www.netlib.org}).
\item[Intel MKL] is a library designed to offer high performance
  linear algebra routines on Intel architecture (\cite{intel07MKL}. It
  is freely available for 
  non-commercial use from \url{http://www.intel.com}. 
\item[GotoBLAS] is currently the fastest implementation of the Basic
  Linear Algebra Subprograms (\cite{goto07gotoblas}). This library is
  threaded which means that it will use all available number of
  processors on a target computer. It is freely available for
  non-commercial purposes from
  \url{http://www.tacc.utexas.edu/resources/software/}.
\end{description}

%% subsection:A Comparison of Basic Algorithms with BLAS
\input{section_mmBLAS.tex}

\section{Parallel Matrix Multiplication}
\label{sec:parmm}

There are many algorithms for
parallelizing matrix multiplication available.

\subsubsection{MPI}

\begin{Scode}
mm.Rmpi <- function(X, Y, n_cpu = 1, spawnRslaves=FALSE) {
  dx <- dim(X) ## dimensions of matrix A
  dy <- dim(Y) ## dimensions of matrix B
  ## Input validation
  matrix.mult.validate(X, Y, dx, dy)
  
  if( n_cpu == 1 )
    return(X%*%Y)
  if( spawnRslaves == TRUE )
    mpi.spawn.Rslaves(nslaves = n_cpu)

  ## broadcast data and functions necessary on slaves
  mpi.bcast.Robj2slave(Y) 
  mpi.bcast.Robj2slave(X) 
  mpi.bcast.Robj2slave(n_cpu)
  
  nrows_on_slaves <- ceiling(dx[1]/n_cpu)
  nrows_on_last <- dx[1] - (n_cpu - 1)*nrows_on_slaves
  mpi.bcast.Robj2slave(nrows_on_slaves)
  mpi.bcast.Robj2slave(nrows_on_last)
  mpi.bcast.Robj2slave(mm.Rmpi.slave)

  ## start partial matrix multiplication on slaves
  mpi.bcast.cmd(mm.Rmpi.slave())

  ## gather partial results from slaves
  local_mm <- NULL
  mm <- mpi.gather.Robj(local_mm, root=0, comm=1)
  out <- NULL

  ## Rmpi returns a list when the vectors have different length (local_mm = NULL)
  for(i in 1:n_cpu)
    out <- rbind(out,mm[[i+1]])
  
  if( spawnRslaves == TRUE )
    mpi.close.Rslaves()
  out

\end{Scode}

\begin{Scode}
mm.Rmpi.slave <- function(){
  commrank <- mpi.comm.rank() -1
  if(commrank==(n_cpu - 1))
    local_mm <- X[(nrows_on_slaves*commrank + 1):(nrows_on_slaves*commrank + nrows_on_last),]%*%Y
  else
    local_mm <- X[(nrows_on_slaves*commrank + 1):(nrows_on_slaves*commrank + nrows_on_slaves),]%*%Y
  mpi.gather.Robj(local_mm,root=0,comm=1)    
}
\end{Scode}

\subsubsection{PVM}
\subsubsection{OpenMP}

\begin{Scode}
void OMP_matrix_mult( double *x, int *nrx, int *ncx,
		      double *y, int *nry, int *ncy,
		      double *z) {
  int i, j, k;
  double tmp, sum;

#pragma omp parallel for private(sum) shared(x, y, z, j, k, nrx, nry, ncy, ncx)
  for(i = 0; i < *nrx; i++)
    for(j = 0; j < *ncy; j++){
      sum = 0.0;
      for(k = 0; k < *ncx; k++) 
	sum += x[i + k**nrx]*y[k + j**nry];
      z[i + j**nrx] = sum;
    }
}
\end{Scode}

\subsection{Fox or other}

\subsubsection{MPI}
\subsubsection{PVM}
\subsubsection{OpenMP}

%% Results
%%\input{section_mm_results.tex}


%% standalone compile
\end{document}
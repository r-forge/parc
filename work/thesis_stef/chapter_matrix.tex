%% For standalone compile

\documentclass[12pt,a4paper]{report}
% zur Kontrolle des Umbruchs Klassenoption draft verwenden
%\usepackage[ngerman]{babel}
\usepackage[latin1]{inputenc}
%\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{latexsym}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{ifpdf}
\usepackage{url}
%% \usepackage{/usr/share/R/share/texmf/Sweave} see below new environment
\usepackage[round]{natbib}
\usepackage{algorithm}
\usepackage{algorithmic}

% Seitenlayout
% DIV# gibt den Divisor für die Layoutberechnung an.
% Vergrößern des Divisors vergrößert den Textbereich.
% BCOR#cm gibt die Breite des Bundstegs an.
%\usepackage[DIV11,BCOR2cm]{typearea}

% Abstand obere Blattkante zur Kopfzeile ist 2.54cm - 15mm
\setlength{\topmargin}{-5mm}


%% commands, environments, etc.
\let\code=\texttt
\newcommand{\pkg}[1]{{\normalfont\fontseries{b}\selectfont #1}}
\newcommand{\proglang}[1]{\textsf{#1}}
\newcommand{\class}[1]{`\code{#1}'}


\newtheorem{Example}{Example}[chapter]

%% example for newenvironment
%% \newenvironment{Proof}{\noindent{Proof:}}{%
%%    \hspace*{\fill}$\Box$\par\vskip2ex}

\RequirePackage{fancyvrb}
\RequirePackage{color}

\newenvironment{Schunk}{\par\begin{minipage}{\textwidth}}{\end{minipage}}

\definecolor{Sinput}{rgb}{0,0,0.56}
\definecolor{Scode}{rgb}{0,0,0.56}
\definecolor{Soutput}{rgb}{0.56,0,0}

\DefineVerbatimEnvironment{Sinput}{Verbatim}
{formatcom={\color{Sinput}},fontsize=\small}
\DefineVerbatimEnvironment{Soutput}{Verbatim}
{formatcom={\color{Soutput}},fontsize=\small}
\DefineVerbatimEnvironment{Scode}{Verbatim}
{formatcom={\color{Scode}},fontsize=\small}


\begin{document}

%%% standalone compile

%% new commands for algorithmics environment
\newcommand{\NROW}{\textbf{nrow}}
\newcommand{\NCOL}{\textbf{ncol}}
\newcommand{\OMPFOR}{\textbf{!\$omp parallel for}}
\newcommand{\OMPPRIV}{\textbf{private}}
\newcommand{\OMPSHARE}{\textbf{shared}}

\chapter{Matrix Multiplication}
\label{chap:matrix}
\section{Introduction}

If we think of applications in parallel computing matrix
multiplication comes into mind. Because of its nature it is a prime
example for data parallelism. There has been done a lot of work in
this area. E.g., \cite{golub96mc} give a comprehensive introduction to the
field of matrix computations. 


In this chapter a short introduction to one of the fundamental methods
in linear algebra namely matrix multiplication is
given. Subsequently an implementation of a serial version of the matrix
multiplication algorithm is compared with implementations from high
performance linear algebra libraries. The remaining parts of this
chapter deals with the parallel implementation of matrix
multiplication and uses it as a task to benchmark parallel
multiplication routines provided from packages presented in
Chapter~\ref{chap:Rhpc}. Algorithms from the R package \pkg{paRc} are
explained in more detail as they have been developed in the course of
this thesis. Eventually, results of the comparisons are presented.

\section{Notation}

$ \mathbb{R} $ denotes the set of real numbers and $ \mathbb{R}^{m
  \times n} $ the vector space of all m-by-n real matrices.

$$ A \in \mathbb{R}^{m \times n} \Longleftrightarrow A = (a_{ij}) = 
\left( \begin{array}{ccc}
a_{11} & \ldots & a_{1n} \\
\vdots &        & \vdots \\
a_{m1} & \ldots & a_{mn}
\end{array} \right)
a_{ij} \in \mathbb{R}
 $$
The lower case letter of the letter which denotes the matrix with
subsripts $ij$ refers to the entry in the matrix. 

\subsection{Column and Row Partitioning}

A matrix $A$ can be accessed through its rows as it is a stack of row
vectors:
$$ A \in \mathbb{R}^{m \times n} \Longleftrightarrow A = 
\left( \begin{array}{c}
a_{1}^T \\
\vdots \\
a_{m}^T 
\end{array} \right)
a_{k} \in \mathbb{R}^n 
$$
This is called a \textit{row partition} of $A$.


If $A \in \mathbb{R}^{m \times n}$ the $k$th row of $A$ can be notated
as $A(k,)$ (according to the row access in R). I.e.,

$$ A(k,) = \left( \begin{array}{ccc}
a_{k1}, & \ldots, & a_{kn}
\end{array} \right)
$$
The other alternative is to see a matrix as a collection of column
vectors:
$$ A \in \mathbb{R}^{m \times n} \Longleftrightarrow A = 
\left( \begin{array}{ccc}
a_{1}, & \ldots, & a_{n}
\end{array} \right)
a_{k} \in \mathbb{R}^m 
 $$
This is called a \textit{column partition} of $A$.

If $A \in \mathbb{R}^{m \times n}$ the $k$th column of $A$ can be notated
as $A(,k)$ (according to the column access in R). I.e.,

$$ A(,k) = \left( \begin{array}{c}
a_{1k} \\
\vdots \\
a_{mk} 
\end{array} \right)
 $$

\subsection{Block Notation}

Block matrices are central in many algorithms. They have become very
important in high performance computing because it enables
easy distributing of data. 

In general a $m$ by $n$ matrix $A$ can be partitioned to obtain

$$ 
A = \left( \begin{array}{ccc}
A_{11} & \ldots & A_{1q} \\
\vdots &        & \vdots \\
A_{p1} & \ldots & A_{pq}
\end{array} \right)
$$

$A_{ij}$ is the $(i,j)$ block or submatrix with dimensions $m_i$ by
$n_j$ of $A$. $\sum_{i=1}^p m_i = m$ and $\sum_{j=1}^q n_j = n$. 
We can say that $A = A_{ij}$ is a $p$ by $q$ block
matrix.   

Now we can see that column and row partitionings are special cases of
block matrices.

\section{Basic Matrix Multiplication Algorithm}

In linear algebra matrix multiplication is one of the key figures. In
this section the $n$ by $n$ matirx multiplication problem $C = AB$ is
presented using the dot product matrix multiply version. There are
others like the saxpy and outer product method which are
mathematically equivalent but have very different levels of
performance as of the different ways they access memory. Nevertheless,
we chose the dot product version for illustrating the implementation
of the serial version of the matrix multiplication. A good
introduction to the other algorithms gives \cite{golub96mc}. 

\subsection{Dot Product Matrix Multiply}
In the usual matrix multiplication procedure the array $C$ is computed
through dot products one at a time from left to right and top to
bottom order.

 ($\mathbb{R}^{m \times r} \times
\mathbb{R}^{r \times n} \to \mathbb{R}^{m \times n}$)

$$ C = AB \Longrightarrow c_{ij} = \sum_{k=1}^r a_{ik}b_{kj} $$

Algorithm~\ref{alg:basicmm} shows the implementation of the basic dot
product matrix multiplication.

\begin{algorithm}
\caption{Basic matrix multiplication algorithm}
\label{alg:basicmm}
\begin{algorithmic}[1]

  \REQUIRE $A \in \mathbb{R}^{m \times r}$ and $B \in \mathbb{R}^{r
    \times n}$.
  \ENSURE $C \in \mathbb{R}^{m \times n}$

  \STATE $m \leftarrow \NROW (A)$
  \STATE $r \leftarrow \NCOL (A)$
  \STATE $n \leftarrow \NROW (B)$

  \FOR{$i = 1:m$}
    \FOR{$j = 1:n$}
       \FOR{$k = 1:r$}
         \STATE $C(i,j) \leftarrow A(i,k)B(k,j)$
       \ENDFOR
    \ENDFOR
  \ENDFOR
  \RETURN $C$

\end{algorithmic}
\end{algorithm}

%\subsubsection{Gaxpy Matrix Multiply}

%\subsubsection{Outer Product MAtrix Multiply}

\subsection{C Implementation}

It is rather easy to implement Algorithm~\ref{alg:basicmm} in C. 
Example~\ref{ex:serialmm} shows the serial implementation of the dot
product matrix multiplication in package \pkg{paRc}. It can be called
from R using \code{serial.matrix.mult(A,B)} where \code{A} and
\code{B} are the matrices to be multiplied.

\begin{Example}
\label{ex:serialmm}
\begin{Scode}
void Serial_matrix_mult( double *x, int *nrx, int *ncx,
			 double *y, int *nry, int *ncy,
			 double *z) {
  int i, j, k;
  double sum;

  for(i = 0; i < *nrx; i++)
    for(j = 0; j < *ncy; j++){
      sum = 0.0;
      for(k = 0; k < *ncx; k++)
	sum += x[i + k**nrx]*y[k + j**nry];
      z[i + j**nrx] = sum;
    }
}
\end{Scode}
\end{Example}

\subsection{Using Basic Linear Algebra Subprograms}
\label{sec:blas}
In Section~\ref{sec:processorandmemory} we pointed out that
parallelism can be achieved on the instruction level. Furthermore, we
discussed  the importance of data locality in memory hierarchies. The
Basic Linear Algebra Subprograms (BLAS) standard proposed by
\cite{lawson79bla} consist of several basic operations (e.g., vector
addition, dot product, etc.) utilizing the underlying architecture
optimal. Most attention is paid to the principle of data locality as
it will promise the highest performance gain. This can be achieved by
moving the block of data once up the memory hierarchie, performing all
necessary operations on it and moving the data back to the main memory
and proceeding with the next block. The BLAS routines were used in the
development of linear algebra libraries for solving a number of
standard problems.

We used the following BLAS libraries in benchmarks of  the matrix
multiplication problem:

\begin{description}
\item[refblas] is the official implementation from netlib
  (\url{http://www.netlib.org}).
\item[Intel MKL] is a library designed to offer high performance
  linear algebra routines on Intel architecture (\cite{intel07MKL}. It
  is freely available for 
  non-commercial use from \url{http://www.intel.com}. 
\item[GotoBLAS] is currently the fastest implementation of the Basic
  Linear Algebra Subprograms (\cite{goto07gotoblas}). This library is
  threaded which means that it will use all available number of
  processors on a target computer. It is freely available for
  non-commercial purposes from
  \url{http://www.tacc.utexas.edu/resources/software/}.
\end{description}

%% subsection:A Comparison of Basic Algorithms with BLAS
\input{section_mmBLAS.tex}

\section{Parallel Matrix Multiplication}
\label{sec:parmm}

There are many specialized algorithms for parallel matrix multiplication
available. E.g., Parallel Universal Matrix Multiplication Algorithms
(PUMA---\cite{choi93pumma}) or Fox's Algorithm (\cite{fox87mah}) to
mention a few of them. But they are not within the scope of this thesis
as we want to compare the different non-specialized implementations in the
R packages with each other.

In this section we show how the dot product matrix multiplication can
easily be parallelized using OpenMP. Furthermore, the more
sophisticated implementations using the MPI and PVM interfaces is
presented. Eventually, these implementations are compared to those
available in the packages \pkg{Rmpi}, \pkg{rpvm} and \pkg{snow}.   

\subsection{OpenMP}

As we mentioned in earlier chapters auto-parallelization using
implicit parallel programming is a promising concept. A simple and
scalable programming model and to begin parallelizing existing code
without having to completely rewrite it is OpenMP~(see section
\ref{sec:OpenMP}). OpenMP directives and routines extend the
programming languages FORTRAN and C or C++ respectively to express
shared memory parallelism.

\subsubsection{OpenMP Directives}

OpenMP directives for C/C++ are specified with the \textbf{\code{pragma}}
preprocessing directive (\cite{openMP05}). This means that each
OpenMP directive starts with \textbf{\code{pragma omp}} followed by
directive name and clauses. 
 
\subsubsection{Parallel Loop Construct}

The parallel loop construct is a combination of the fundamental
construct \textbf{\code{parallel}} which starts parallel execution and
a loop construct specifying that the iterations of the loop are
distributed across the threads. The syntax of the parallel loop
construct \code{\textbf{for}} is as follows:

\begin{itemize}
\item \code{\textbf{#pragma parallel for} \textit{optional clauses}} 
\end{itemize} 

The \textit{optional clauses} can be any of the clauses accepted by the
\code{\textbf{parallel}} or \code{\textbf{for}} directives. For
example \textit{optional clauses} can be data sharing attribute
clauses like the following:

\begin{description}
\item[\code{shared(\textit{list})}] declares one or more list items to
  be shared among all the threads,
\item[\code{private(\textit{list})}] declares one or more list items
  to be private to a thread.
\end{description}

The \code{\textbf{for}} directive places restrictions on the structure
of the corresponding \code{for} loop. For example, the iteration variable is
a signed integer, has to be either incremented or decremented in each
loop iteration and is private to each thread. Otherwise threading does
not work, as the loops would not have a clear state.

For more information on other directives, constructs, clauses or
restrictions to the for loop we refer to \cite{openMP05}.

\subsubsection{Implementation}

With the above information given we can parallelize the matrix
multiplication rather easy.

Algorithm~\ref{alg:ompmm} shows how the basic matrix multiplication
(see Algorithm~\ref{alg:basicmm}) can be extended to achieve shared
memory parallelism using shared variables. 

\begin{algorithm}
\caption{OpenMP matrix multiplication algorithm}
\label{alg:ompmm}
\begin{algorithmic}[1]

  \REQUIRE $A \in \mathbb{R}^{m \times r}$ and $B \in \mathbb{R}^{r
    \times n}$.
  \ENSURE $C \in \mathbb{R}^{m \times n}$

  \STATE $m \leftarrow \NROW (A)$
  \STATE $r \leftarrow \NCOL (A)$
  \STATE $n \leftarrow \NROW (B)$

  \STATE \OMPFOR \OMPSHARE($A$, $B$, $C$, $j$, $k$)
  \FOR{$i = 1:m$}
    \FOR{$j = 1:n$}
       \FOR{$k = 1:r$}
         \STATE $C(i,j) \leftarrow A(i,k)B(k,j)$
       \ENDFOR
    \ENDFOR
  \ENDFOR
  \RETURN $C$

\end{algorithmic}
\end{algorithm}

Example~\ref{ex:ompmm} shows the implementation for
Algorithm~\ref{alg:ompmm} in C to achieve loop level parallelism. This
routine can be called from R
using the R function \code{omp.matrix.mult(A, B, n\_cpu)}, where \code{A} and
\code{B} are the matrices to be multiplied and \code{n\_cpu} is the
number of CPUs to be used.

\begin{Example}
\label{ex:ompmm}
\begin{Scode}
void OMP_matrix_mult( double *x, int *nrx, int *ncx,
		      double *y, int *nry, int *ncy,
		      double *z) {
  int i, j, k;
  double tmp, sum;

#pragma omp parallel for private(sum) shared(x, y, z, j, k, nrx, nry, ncy, ncx)
  for(i = 0; i < *nrx; i++)
    for(j = 0; j < *ncy; j++){
      sum = 0.0;
      for(k = 0; k < *ncx; k++) 
	sum += x[i + k**nrx]*y[k + j**nry];
      z[i + j**nrx] = sum;
    }
}
\end{Scode}
\end{Example}

As OpenMP makes use of shared variables (no message passing) it can
only be compared to other paradigms on a shared memory
machine. Results are presented in Section~\ref{sec:res_shared}.

\subsection{MPI}

Creating parallel applications using message passing is more
complex as sending and receiving of data has to be done by hand. The
developer has to be aware of blocking communication and how to
synchronize processes. Furthermore, the sequential program has to be
either decomposed with respect to data or with respect to
functionality.

For matrix multiplication one wants to apply the same
program to different data because a matrix can be decomposed
easily to blocks of matrices. Each block is send to one slave which
performs the matrix multiplication on the given data block.


\subsubsection{Implementation}

We used the master-slave parallel programming paradigm (crowd
computation) to organize our tasks. This means that a master process
exists which is responsible for I/O, process spawning, distributing
and collection of the data. Only the slaves carry out the
computation.

Package \pkg{Rmpi} is used to interface the MPI communication
layer. In addition to the multiplication routine which uses MPI provided by the
packages \pkg{snow} a separate implementation has been developed for
package \pkg{paRc} to become familiar with message passing in MPI.


Examples~\ref{ex:MPImaster} and~\ref{ex:MPIslave}

\begin{Example}
\label{ex:MPImaster}
\begin{Scode}

mm.Rmpi <- function(X, Y, n_cpu = 1, spawnRslaves=FALSE) {
  dx <- dim(X) ## dimensions of matrix A
  dy <- dim(Y) ## dimensions of matrix B
  ## Input validation
  matrix.mult.validate(X, Y, dx, dy)
  
  if( n_cpu == 1 )
    return(X%*%Y)
  ## spawn R slaves?
  if( spawnRslaves == TRUE )
    mpi.spawn.Rslaves(nslaves = n_cpu)

  ## broadcast data and functions necessary on slaves
  mpi.bcast.Robj2slave(Y) 
  mpi.bcast.Robj2slave(X) 
  mpi.bcast.Robj2slave(n_cpu)
  
  ## how many rows on slaves
  nrows_on_slaves <- ceiling(dx[1]/n_cpu)
  nrows_on_last <- dx[1] - (n_cpu - 1)*nrows_on_slaves
  ## broadcast number of rows to multiply and slave foo
  mpi.bcast.Robj2slave(nrows_on_slaves)
  mpi.bcast.Robj2slave(nrows_on_last)
  mpi.bcast.Robj2slave(mm.Rmpi.slave)

  ## start partial matrix multiplication on slaves
  mpi.bcast.cmd(mm.Rmpi.slave())

  ## gather partial results from slaves
  local_mm <- NULL
  mm <- mpi.gather.Robj(local_mm, root=0, comm=1)
  out <- NULL

  ## Rmpi returns a list when the vectors have different length
  for(i in 1:n_cpu)
    out <- rbind(out,mm[[i+1]])
  
  if( spawnRslaves == TRUE )
    mpi.close.Rslaves()
  
  out
\end{Scode}
\end{Example}

\begin{Example}
\label{ex:MPIslave}
\begin{Scode}
mm.Rmpi.slave <- function(){
  commrank <- mpi.comm.rank() -1
  if(commrank==(n_cpu - 1))
    local_mm <- X[(nrows_on_slaves*commrank + 1):(nrows_on_slaves*commrank + nrows_on_last),]%*%Y
  else
    local_mm <- X[(nrows_on_slaves*commrank + 1):(nrows_on_slaves*commrank + nrows_on_slaves),]%*%Y
  mpi.gather.Robj(local_mm,root=0,comm=1)    
}
\end{Scode}
\end{Example}


\subsection{PVM Implementation}

\subsection{Comparison on a Shared Memory Platform}
\label{sec:res_shared}

\subsection{Comparison on a Distributed Memory Platform}

%% Results
%%\input{section_mm_results.tex}


%% standalone compile
\end{document}
\chapter{Parallel Matrix Multiplication}
\label{chap:matrix}
\section{Introduction}

If we think of applications in parallel computing matrix
multiplication comes into mind. Because of its nature it is prime
example for data parallelism. There are many algorithms for
parallelizing matrix multiplication available.


In this chapter a short introduction to matrix multiplication is
given. Subsequently implementations of a selection of parallel
algorithms are presented. Eventually results of the comparison are
presented.


\section{Notation}

$ \mathbb{R} $ denotes the set of real numbers and $ \mathbb{R}^{m
  \times n} $ the vector space of all m-by-n real matrices.

$$ A \in \mathbb{R}^{m \times n} \Longleftrightarrow A = (a_{ij}) = 
\left( \begin{array}{ccc}
a_{11} & \ldots & a_{1n} \\
\vdots &        & \vdots \\
a_{m1} & \ldots & a_{mn}
\end{array} \right)
a_{ij} \in \mathbb{R}
 $$

The lower case letter of the letter which denotes the matrix with
subsripts $ij$ refers to the entry in the matrix. 

\subsubsection{Column and Row Partitioning}

A matrix $A$ can be accessed through its rows as it is a stack of row
vectors:
$$ A \in \mathbb{R}^{m \times n} \Longleftrightarrow A = 
\left( \begin{array}{c}
a_{1}^T \\
\vdots \\
a_{m}^T 
\end{array} \right)
a_{k} \in \mathbb{R}^n 
 $$

This is called a \textit{row partition} of $A$.


If $A \in \mathbb{R}^{m \times n}$ the $k$th row of $A$ can be notated
as $A(k,)$ (according to the row access in R). I.e.,

$$ A(k,) = \left( \begin{array}{ccc}
a_{k1}, & \ldots, & a_{kn}
\end{array} \right)
$$


The other alternative is to see a matrix as a collection of column
vectors:

$$ A \in \mathbb{R}^{m \times n} \Longleftrightarrow A = 
\left( \begin{array}{ccc}
a_{1}, & \ldots, & a_{n}
\end{array} \right)
a_{k} \in \mathbb{R}^m 
 $$

This is called a \textit{column partition} of $A$.


If $A \in \mathbb{R}^{m \times n}$ the $k$th column of $A$ can be notated
as $A(,k)$ (according to the row access in R). I.e.,

$$ A(,k) = \left( \begin{array}{c}
a_{1k} \\
\vdots \\
a_{mk} 
\end{array} \right)
 $$

\subsubsection{Block Notation}

Block matrices are central in many algorithms. They have become very
important in high performance computing because it enables
easy distributing of data. 

In general a $m$ by $n$ matrix $A$ can be partitioned to obtain

$$ 
A = \left( \begin{array}{ccc}
A_{11} & \ldots & A_{1q} \\
\vdots &        & \vdots \\
A_{p1} & \ldots & A_{pq}
\end{array} \right)
$$

$A_{ij}$ is the $(i,j)$ block or submatrix with dimensions $m_i$ by
$n_j$ of $A$. $\sum_{i=1}^p m_i = m$ and $\sum_{j=1}^q n_j = n$. 
We can say that $A = A_{ij}$ is a $p$ by $q$ block
matrix.   

Now we can see that column and row partitionings are special cases of
block matrices.

\section{Naive Parallel Algorithm}

\subsection{Basic Multiplication Algorithm}

In the usual matrix multiplication procedure the array $C$ is computed
through dot products one at a time from left to right and top to
bottom order.

 ($\mathbb{R}^{m \times r} \times
\mathbb{R}^{r \times n} \to \mathbb{R}^{m \times n}$)

$$ C = AB \Longrightarrow c_{ij} = \sum_{k=1}^r a_{ik}b_{kj} $$


%\subsubsection{Gaxpy Matrix Multiply}

%\subsubsection{Outer Product MAtrix Multiply}

\section{Implementation}

\subsection{Naive Parallel Algorithm}




\subsubsection{MPI}

\begin{Scode}
mm.Rmpi <- function(X, Y, n_cpu = 1, spawnRslaves=FALSE) {
  dx <- dim(X) ## dimensions of matrix A
  dy <- dim(Y) ## dimensions of matrix B
  ## Input validation
  matrix.mult.validate(X, Y, dx, dy)
  
  if( n_cpu == 1 )
    return(X%*%Y)
  if( spawnRslaves == TRUE )
    mpi.spawn.Rslaves(nslaves = n_cpu)

  ## broadcast data and functions necessary on slaves
  mpi.bcast.Robj2slave(Y) 
  mpi.bcast.Robj2slave(X) 
  mpi.bcast.Robj2slave(n_cpu)
  
  nrows_on_slaves <- ceiling(dx[1]/n_cpu)
  nrows_on_last <- dx[1] - (n_cpu - 1)*nrows_on_slaves
  mpi.bcast.Robj2slave(nrows_on_slaves)
  mpi.bcast.Robj2slave(nrows_on_last)
  mpi.bcast.Robj2slave(mm.Rmpi.slave)

  ## start partial matrix multiplication on slaves
  mpi.bcast.cmd(mm.Rmpi.slave())

  ## gather partial results from slaves
  local_mm <- NULL
  mm <- mpi.gather.Robj(local_mm, root=0, comm=1)
  out <- NULL

  ## Rmpi returns a list when the vectors have different length (local_mm = NULL)
  for(i in 1:n_cpu)
    out <- rbind(out,mm[[i+1]])
  
  if( spawnRslaves == TRUE )
    mpi.close.Rslaves()
  out

\end{Scode}

\begin{Scode}
mm.Rmpi.slave <- function(){
  commrank <- mpi.comm.rank() -1
  if(commrank==(n_cpu - 1))
    local_mm <- X[(nrows_on_slaves*commrank + 1):(nrows_on_slaves*commrank + nrows_on_last),]%*%Y
  else
    local_mm <- X[(nrows_on_slaves*commrank + 1):(nrows_on_slaves*commrank + nrows_on_slaves),]%*%Y
  mpi.gather.Robj(local_mm,root=0,comm=1)    
}
\end{Scode}

\subsubsection{PVM}
\subsubsection{OpenMP}

\begin{Scode}
void OMP_matrix_mult( double *x, int *nrx, int *ncx,
		      double *y, int *nry, int *ncy,
		      double *z) {
  int i, j, k;
  double tmp, sum;

#pragma omp parallel for private(sum) shared(x, y, z, j, k, nrx, nry, ncy, ncx)
  for(i = 0; i < *nrx; i++)
    for(j = 0; j < *ncy; j++){
      sum = 0.0;
      for(k = 0; k < *ncx; k++) 
	sum += x[i + k**nrx]*y[k + j**nry];
      z[i + j**nrx] = sum;
    }
}
\end{Scode}

\subsection{Fox or other}

\subsubsection{MPI}
\subsubsection{PVM}
\subsubsection{OpenMP}

%% Results
\input{section_mm_results.tex}

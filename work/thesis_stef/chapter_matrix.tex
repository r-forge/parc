\documentclass[12pt,a4paper]{report}
% zur Kontrolle des Umbruchs Klassenoption draft verwenden
%\usepackage[ngerman]{babel}
\usepackage[latin1]{inputenc}
%\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{latexsym}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{ifpdf}
\usepackage{url}
%% \usepackage{/usr/share/R/share/texmf/Sweave} see below new environment
\usepackage[round]{natbib}
\usepackage{algorithm}
\usepackage{algorithmic}

% Seitenlayout
% DIV# gibt den Divisor für die Layoutberechnung an.
% Vergrößern des Divisors vergrößert den Textbereich.
% BCOR#cm gibt die Breite des Bundstegs an.
%\usepackage[DIV11,BCOR2cm]{typearea}

% Abstand obere Blattkante zur Kopfzeile ist 2.54cm - 15mm
\setlength{\topmargin}{-5mm}


%% commands, environments, etc.
\let\code=\texttt
\newcommand{\pkg}[1]{{\normalfont\fontseries{b}\selectfont #1}}
\newcommand{\proglang}[1]{\textsf{#1}}
\newcommand{\class}[1]{`\code{#1}'}


\newtheorem{Example}{Example}[chapter]

%% example for newenvironment
%% \newenvironment{Proof}{\noindent{Proof:}}{%
%%    \hspace*{\fill}$\Box$\par\vskip2ex}

\RequirePackage{fancyvrb}
\RequirePackage{color}

\newenvironment{Schunk}{\par\begin{minipage}{\textwidth}}{\end{minipage}}

\definecolor{Sinput}{rgb}{0,0,0.56}
\definecolor{Scode}{rgb}{0,0,0.56}
\definecolor{Soutput}{rgb}{0.56,0,0}

\DefineVerbatimEnvironment{Sinput}{Verbatim}
{formatcom={\color{Sinput}},fontsize=\small}
\DefineVerbatimEnvironment{Soutput}{Verbatim}
{formatcom={\color{Soutput}},fontsize=\small}
\DefineVerbatimEnvironment{Scode}{Verbatim}
{formatcom={\color{Scode}},fontsize=\small}


%%% standalone compile

%% new commands for algorithmics environment
\newcommand{\NROW}{\textbf{nrow}}
\newcommand{\NCOL}{\textbf{ncol}}
\newcommand{\OMPFOR}{\textbf{!\$omp parallel for }}
\newcommand{\OMPPRIV}{\textbf{private}}
\newcommand{\OMPSHARE}{\textbf{shared}}
\newcommand{\SERIALMM}{\textbf{serial.mult}}
\begin{document}
\chapter{Matrix Multiplication}
\label{chap:matrix}
\section{Introduction}

If we think of applications in parallel computing matrix
multiplication comes into mind. Because of its nature it is a prime
example for data parallelism. There has been done a lot of work in
this area. E.g., \cite{golub96mc} give a comprehensive introduction to the
field of matrix computations. 


In this chapter a short introduction to one of the fundamental methods
in linear algebra namely matrix multiplication is
given. Subsequently an implementation of a serial version of the matrix
multiplication algorithm is compared with implementations from high
performance linear algebra libraries. The remaining part of this
chapter deals with the parallel implementations of matrix
multiplication and uses them as a task to benchmark parallel
multiplication routines provided from packages presented in
Chapter~\ref{chap:Rhpc}. Algorithms from the R package \pkg{paRc} are
explained in more detail as they have been developed in the course of
this thesis. Eventually, results of the comparisons are presented.

\section{Notation}

$ \mathbb{R} $ denotes the set of real numbers and $ \mathbb{R}^{m
  \times n} $ the vector space of all $m$ by $n$ real matrices.

$$ A \in \mathbb{R}^{m \times n} \Longleftrightarrow A = (a_{ij}) = 
\left( \begin{array}{ccc}
a_{11} & \ldots & a_{1n} \\
\vdots &        & \vdots \\
a_{m1} & \ldots & a_{mn}
\end{array} \right)
a_{ij} \in \mathbb{R}
 $$
The lower case letter of the letter which denotes the matrix with
subsripts $ij$ refers to the entry in the matrix. 

\subsection{Column and Row Partitioning}

A matrix $A$ can be accessed through its rows as it is a stack of row
vectors:
$$ A \in \mathbb{R}^{m \times n} \Longleftrightarrow A = 
\left( \begin{array}{c}
a_{1}^T \\
\vdots \\
a_{m}^T 
\end{array} \right)
a_{k} \in \mathbb{R}^n 
$$
This is called a \textit{row partition} of $A$.


If $A \in \mathbb{R}^{m \times n}$ the $k$th row of $A$ can be notated
as $A(k,)$ (according to the row access in R). I.e.,

$$ A(k,) = \left( \begin{array}{ccc}
a_{k1}, & \ldots, & a_{kn}
\end{array} \right)
$$
The other alternative is to see a matrix as a collection of column
vectors:
$$ A \in \mathbb{R}^{m \times n} \Longleftrightarrow A = 
\left( \begin{array}{ccc}
a_{1}, & \ldots, & a_{n}
\end{array} \right)
a_{k} \in \mathbb{R}^m 
 $$
This is called a \textit{column partition} of $A$.

If $A \in \mathbb{R}^{m \times n}$ the $k$th column of $A$ can be notated
as $A(,k)$ (according to the column access in R). I.e.,

$$ A(,k) = \left( \begin{array}{c}
a_{1k} \\
\vdots \\
a_{mk} 
\end{array} \right)
 $$

\subsection{Block Notation}
\label{sec:blocknot}
Block matrices are central in many algorithms. They have become very
important in high performance computing because it enables
easy distributing of data. 

In general a $m$ by $n$ matrix $A$ can be partitioned to obtain

$$ 
A = \left( \begin{array}{ccc}
A_{11} & \ldots & A_{1q} \\
\vdots &        & \vdots \\
A_{p1} & \ldots & A_{pq}
\end{array} \right)
$$

$A_{ij}$ is the $(i,j)$ block or submatrix with dimensions $m_i$ by
$n_j$ of $A$. $\sum_{i=1}^p m_i = m$ and $\sum_{j=1}^q n_j = n$. 
We can say that $A = A_{ij}$ is a $p$ by $q$ block
matrix.   

It can be seen that column and row partitionings are special cases of
block matrices.

\section{Basic Matrix Multiplication Algorithm}

Matrix multiplication is one of the key figures in linear algebra. In
this section the matrix multiplication problem $C = AB$ is
presented using the dot product matrix multiply version. There are
others like the saxpy and outer product method which are
mathematically equivalent but have different levels of
performance as of the different ways they access memory. Nevertheless,
we chose the dot product version for illustrating the implementation
of the serial version of the matrix multiplication. A good
introduction to the other algorithms is given by \cite{golub96mc}. 

\subsection{Dot Product Matrix Multiply}
In the usual matrix multiplication procedure the array $C$ is computed
through dot products one at a time from left to right and top to
bottom order.
\newline

$\mathbb{R}^{m \times r} \times
\mathbb{R}^{r \times n} \to \mathbb{R}^{m \times n}$

$$ C = AB \Longrightarrow c_{ij} = \sum_{k=1}^r a_{ik}b_{kj} $$

Algorithm~\ref{alg:basicmm} shows the implementation of the basic dot
product matrix multiplication.

\begin{algorithm}
\caption{Basic matrix multiplication algorithm}
\label{alg:basicmm}
\begin{algorithmic}[1]

  \REQUIRE $A \in \mathbb{R}^{m \times r}$ and $B \in \mathbb{R}^{r
    \times n}$.
  \ENSURE $C \in \mathbb{R}^{m \times n}$

  \STATE $m \leftarrow \NROW (A)$
  \STATE $r \leftarrow \NCOL (A)$
  \STATE $n \leftarrow \NROW (B)$

  \FOR{$i = 1:m$}
    \FOR{$j = 1:n$}
       \FOR{$k = 1:r$}
         \STATE $C(i,j) \leftarrow A(i,k)B(k,j)$
       \ENDFOR
    \ENDFOR
  \ENDFOR

\end{algorithmic}
\end{algorithm}

%\subsubsection{Gaxpy Matrix Multiply}

%\subsubsection{Outer Product MAtrix Multiply}

\subsection{C Implementation}

It is rather easy to implement Algorithm~\ref{alg:basicmm} in C. 
Example~\ref{ex:serialmm} shows the serial implementation of the dot
product matrix multiplication in package \pkg{paRc}. It can be called
from R using \code{serial.matrix.mult(A,B)} where \code{A} and
\code{B} are the matrices to be multiplied.

\begin{Example}
\label{ex:serialmm}
\begin{Scode}
void Serial_matrix_mult( double *x, int *nrx, int *ncx,
			 double *y, int *nry, int *ncy,
			 double *z) {
  int i, j, k;
  double sum;

  for(i = 0; i < *nrx; i++)
    for(j = 0; j < *ncy; j++){
      sum = 0.0;
      for(k = 0; k < *ncx; k++)
	sum += x[i + k**nrx]*y[k + j**nry];
      z[i + j**nrx] = sum;
    }
}
\end{Scode}
\end{Example}

\subsection{Using Basic Linear Algebra Subprograms}
\label{sec:blas}
In Section~\ref{sec:processorandmemory} we pointed out that
parallelism can be achieved on the instruction level. Furthermore, we
discussed  the importance of data locality in memory hierarchies. The
Basic Linear Algebra Subprograms (BLAS) standard proposed by
\cite{lawson79bla} consist of several basic operations (e.g., vector
addition, dot product, etc.) utilizing the underlying architecture
optimal. Most attention is paid to the principle of data locality as
it will promise the highest performance gain. This can be achieved by
moving the block of data once up the memory hierarchie, performing all
necessary operations on it and moving the data back to the main memory
and proceeding with the next block. The BLAS routines were used in the
development of linear algebra libraries for solving a number of
standard problems.

We used the following BLAS libraries in benchmarks of  the matrix
multiplication problem:

\begin{description}
\item[refblas] is the official implementation from netlib
  (\url{http://www.netlib.org}).
\item[Intel MKL] is a library designed to offer high performance
  linear algebra routines on
 Intel architecture (\cite{intel07MKL}--- available
  freely for non-commercial use from \url{http://www.intel.com}. 
\item[GotoBLAS] is currently the fastest implementation of the Basic
  Linear Algebra Subprograms (\cite{goto07gotoblas}). This library is
  threaded, which means that it will use all available number of
  processors on a target computer. It is freely available for
  non-commercial purposes from
  \url{http://www.tacc.utexas.edu/resources/software/}.
\end{description}

%% subsection:A Comparison of Basic Algorithms with BLAS
\input{section_mmBLAS.tex}

\section{Parallel Matrix Multiplication}
\label{sec:parmm}

There are many specialized algorithms for parallel matrix multiplication
available. E.g., Parallel Universal Matrix Multiplication Algorithms
(PUMMA---\cite{choi93pumma}) or Fox's Algorithm (\cite{fox87mah}) to
mention a few of them. But they are not within the scope of this thesis
as we want to compare the different non-specialized implementations in the
R packages with each other.

In this section we show how the dot product matrix multiplication can
easily be parallelized using OpenMP. Furthermore, the more
sophisticated implementations using the MPI and PVM interfaces
available in \pkg{paRc} are
presented. Eventually, these implementations are compared to those
available in the package \pkg{snow}.   

\subsection{OpenMP}

As we mentioned in earlier chapters auto-parallelization using
implicit parallel programming is a promising concept. OpenMP is a simple and
scalable programming model and with the aim to parallelize existing code
without having to completely rewrite~(see section
\ref{sec:OpenMP}). OpenMP directives and routines extend the
programming languages FORTRAN and C or C++ respectively to express
shared memory parallelism.

\subsubsection{OpenMP Directives}

OpenMP directives for C/C++ are specified with the \textbf{\code{#pragma}}
preprocessing directive (\cite{openMP05}). This means that each
OpenMP directive starts with \textbf{\code{#pragma omp}} followed by
directive name and clauses. 
 
\subsubsection{Parallel Loop Construct}

The parallel loop construct is a combination of the fundamental
construct \textbf{\code{parallel}} which starts parallel execution and
a loop construct specifying that the iterations of the loop are
distributed across the threads. The syntax of the parallel loop
construct \code{\textbf{for}} is as follows:

\begin{itemize}
\item \code{\textbf{\#pragma parallel for} \textit{optional clauses}} 
\end{itemize} 

The \textit{optional clauses} can be any of the clauses accepted by the
\code{\textbf{parallel}} or \code{\textbf{for}} directives. For
example \textit{optional clauses} can be data sharing attribute
clauses like the following:

\begin{description}
\item[\code{shared(\textit{list})}] declares one or more list items to
  be shared among all the threads,
\item[\code{private(\textit{list})}] declares one or more list items
  to be private to a thread.
\end{description}

The \code{\textbf{for}} directive places restrictions on the structure
of the corresponding loop. For example, the iteration variable is
a signed integer, has to be either incremented or decremented in each
loop iteration, and is private to each thread. Otherwise threading does
not work properly, as loop iterations cannot be decomposed to create
parallel threads.

For further information on other directives, constructs, clauses or
restrictions to the \code{for} loop we refer to \cite{openMP05}.

\subsubsection{Implementation}

With the above information given we can parallelize the matrix
multiplication rather easy.

Algorithm~\ref{alg:ompmm} shows how the basic matrix multiplication
(see Algorithm~\ref{alg:basicmm}) can be extended to achieve shared
memory parallelism using shared variables. 

\begin{algorithm}
\caption{OpenMP matrix multiplication algorithm}
\label{alg:ompmm}
\begin{algorithmic}[1]

  \REQUIRE $A \in \mathbb{R}^{m \times r}$ and $B \in \mathbb{R}^{r
    \times n}$.
  \ENSURE $C \in \mathbb{R}^{m \times n}$

  \STATE $m \leftarrow \NROW (A)$
  \STATE $r \leftarrow \NCOL (A)$
  \STATE $n \leftarrow \NROW (B)$

  \STATE \OMPFOR \OMPSHARE($A$, $B$, $C$, $j$, $k$)
  \FOR{$i = 1:m$}
    \FOR{$j = 1:n$}
       \FOR{$k = 1:r$}
         \STATE $C(i,j) \leftarrow A(i,k)B(k,j)$
       \ENDFOR
    \ENDFOR
  \ENDFOR

\end{algorithmic}
\end{algorithm}

Example~\ref{ex:ompmm} shows the implementation for
Algorithm~\ref{alg:ompmm} in C to achieve loop level parallelism. This
routine can be called from R
using the R function \code{omp.matrix.mult(A, B, n\_cpu)}, where \code{A} and
\code{B} are the matrices to be multiplied and \code{n\_cpu} is the
number of CPUs to be used.

\begin{Example}
\label{ex:ompmm}
\begin{Scode}
void OMP_matrix_mult( double *x, int *nrx, int *ncx,
		      double *y, int *nry, int *ncy,
		      double *z) {
  int i, j, k;
  double tmp, sum;

#pragma omp parallel for private(sum) shared(x, y, z, j, k, nrx, nry, ncy, ncx)
  for(i = 0; i < *nrx; i++)
    for(j = 0; j < *ncy; j++){
      sum = 0.0;
      for(k = 0; k < *ncx; k++) 
	sum += x[i + k**nrx]*y[k + j**nry];
      z[i + j**nrx] = sum;
    }
}
\end{Scode}
\end{Example}

As OpenMP makes use of shared variables (no message passing) it can
only be compared to other paradigms on a shared memory
machine. Results are presented in Section~\ref{sec:res_shared}.

\subsection{MPI}
\label{sec:mmMPI}

Creating parallel applications using message passing is more
complex as sending and receiving of data has to be done by hand. The
developer has to be aware of blocking communication and how to
synchronize processes. Furthermore, the sequential program has to be
either decomposed with respect to data or with respect to
functionality.

For matrix multiplication one wants to apply the same
program to different data because a matrix can be decomposed
easily to blocks of matrices. Each block is send to one slave which
performs the matrix multiplication on the given data block.

Recalling the block notation from Section~\ref{sec:blocknot} a $m
\times n$ matrix can be partitioned to obtain

$$ A = \left( \begin{array}{ccc}
A_{1} \\
\vdots\\
A_{p}
\end{array} \right)
$$

$A_{i}$ is the $i$th block or submatrix with dimensions $m_i$ by
$n$ of $A$ where $\sum_{i=1}^p m_i = m$. 
We can say that $A = A_{i}$ is a $m_i$ by $n$ block
matrix. 

The number of block matrices is given by the number of available
CPUs $p$. With this information given we can write down the matrix
multiplication for a specific node:

$$ C_{i} = A_{i}B $$

$C_{i}$ represents the local result of the multiplication. Clearly
this is not the best way to subdivide matrices in blocks ($B$ has to
be sent completely to all nodes) but is a good start to become
familiar with data decomposition.

One issue, how many rows are send to each slave, remains. If the
number of rows $m$ of the matrix $A$ can be evenly divided by the number
of processors $p$ then $m/p$ rows are sent to each slave. Otherwise
the block matrices have to be defined such that 
$p - 1$ slaves receive $\lceil m/p \rceil$ and the remaining processor
will receive $m - (p - 1) \lceil m/p \rceil$ rows.

This decomposition is used for the implementation of matrix
multiplication using MPI and PVM in \pkg{paRc}.

\subsubsection{Implementation}

We used the master-slave parallel programming paradigm (crowd
computation) to organize our tasks. This means that a master process
exists which is responsible for I/O, process spawning, distributing
and collection of the data. Only the slaves carry out the
computation.

Package \pkg{Rmpi} is used to interface the MPI communication
layer. In addition to the high level MPI multiplication routine 
 provided by the package \pkg{snow} a separate implementation has
been developed for package \pkg{paRc} to become familiar with message
passing in MPI. 



\begin{algorithm}
\caption{MPI matrix multiplication algorithm---master}
\label{alg:mpimaster}
\begin{algorithmic}[1]

  \REQUIRE $A \in \mathbb{R}^{m \times r}$, $B \in \mathbb{R}^{r
    \times n}$ and $p$.
  \ENSURE $C \in \mathbb{R}^{m \times n}$

  \STATE $m \leftarrow \NROW (A)$
  
  \STATE $n_{slave} \leftarrow \lceil m/p \rceil$
  \STATE $n_{last} \leftarrow m - (p - 1) n_{slave} $
  
  \STATE spawn $p$ slave processes
  \FOR{$i = 1:p$}
    \STATE send $A$, $B$, $n_{slave}$, $n_{last}$ and function
    \SERIALMM to process $i$. start multiplication on process $i$
  \ENDFOR
  
  \FOR{$i = 1:p$}
    \STATE collect local result $C_{i}$ from slaves
  \ENDFOR
  
  \STATE combine $C_{i}$ to $C$

\end{algorithmic}
\end{algorithm}

\begin{algorithm}
\caption{MPI matrix multiplication algorithm---slave}
\label{alg:mpislave}
\begin{algorithmic}[1]

  \REQUIRE $A_{(m)} \in \mathbb{R}^{m \times r}$, $B \in \mathbb{R}^{r
    \times n}$, $n_{slave}$, $n_{last}$ and $p$
  \ENSURE $C \in \mathbb{R}^{m \times n}$

  \STATE $rank \leftarrow rank - 1$

  \STATE $C_{(rank)} \leftarrow A_{(rank)}B$


  \FOR{$i = 1:p$}
    \STATE collect local result $C_{i}$ from slaves
  \ENDFOR
  
  \STATE combine $C_{i}$ to $C$
  
\end{algorithmic}
\end{algorithm}


Examples~\ref{ex:MPImaster} and~\ref{ex:MPIslave} show the R
implementation of Algorithm~\ref{alg:mpimaster} and
Algorithm~\ref{alg:mpislave} respectively. After validating the input
\code{n\_ncpu} number of R slaves are spawned. The master slave
calculates and broadcasts all the necessary data to the slaves. Then
the slaves perform the actual computations and return the resulting
local matrices to the master. The local solutions are combined to a
single matrix which is the solution of the parallel matrix
multiplication.

\begin{Example}
\label{ex:MPImaster}
\begin{Scode}

mm.Rmpi <- function(X, Y, n_cpu = 1, spawnRslaves=FALSE) {
  dx <- dim(X) ## dimensions of matrix A
  dy <- dim(Y) ## dimensions of matrix B
  ## Input validation
  matrix.mult.validate(X, Y, dx, dy)
  
  if( n_cpu == 1 )
    return(X%*%Y)
  ## spawn R slaves?
  if( spawnRslaves == TRUE )
    mpi.spawn.Rslaves(nslaves = n_cpu)

  ## broadcast data and functions necessary on slaves
  mpi.bcast.Robj2slave(Y) 
  mpi.bcast.Robj2slave(X) 
  mpi.bcast.Robj2slave(n_cpu)
  
  ## how many rows on slaves
  nrows_on_slaves <- ceiling(dx[1]/n_cpu)
  nrows_on_last <- dx[1] - (n_cpu - 1)*nrows_on_slaves
  ## broadcast number of rows to multiply and slave foo
  mpi.bcast.Robj2slave(nrows_on_slaves)
  mpi.bcast.Robj2slave(nrows_on_last)
  mpi.bcast.Robj2slave(mm.Rmpi.slave)

  ## start partial matrix multiplication on slaves
  mpi.bcast.cmd(mm.Rmpi.slave())

  ## gather partial results from slaves
  local_mm <- NULL
  mm <- mpi.gather.Robj(local_mm, root=0, comm=1)
  out <- NULL

  ## Rmpi returns a list when the vectors have different length
  for(i in 1:n_cpu)
    out <- rbind(out,mm[[i+1]])
  
  if( spawnRslaves == TRUE )
    mpi.close.Rslaves()
  
  out
\end{Scode}
\end{Example}

\begin{Example}
\label{ex:MPIslave}
\begin{Scode}
mm.Rmpi.slave <- function(){
  commrank <- mpi.comm.rank() -1
  if(commrank==(n_cpu - 1))
    local_mm <- X[(nrows_on_slaves*commrank + 1):(nrows_on_slaves*commrank + nrows_on_last),]%*%Y
  else
    local_mm <- X[(nrows_on_slaves*commrank + 1):(nrows_on_slaves*commrank + nrows_on_slaves),]%*%Y
  mpi.gather.Robj(local_mm,root=0,comm=1)    
}
\end{Scode}
\end{Example}

\subsection{PVM}

PVM in R can be used with the package \pkg{rpvm}. In addition to the
high level PVM multiplication routine provided by the package
\pkg{snow} a separate implementation has been developed for package
\pkg{paRc} to become familiar with message passing in PVM, which is
slightly different from MPI. 

Like the MPI implementation above the matrix multiplication is
organized according 
to the master-slave programming model. The difference to MPI is the
lack of its ability to remotely execute commands. Therefore when
spawning R processes the complete source has to be available to the
slave on creation (i.e., from a file). The algorithms for matrix
multiplication are the same as shown in Section~\ref{sec:mmMPI}.

The PVM equivalents to the master and slave code of MPI are shown in
Examples~\ref{ex:PVMmaster} and~\ref{ex:PVMslave}.
After validating the input \code{n\_cpu} number of R slaves are
spawned. Immediately after invoking this command the R slaves source
the code from a file \code{"mm\_slave.R"} (shown in Example
\ref{ex:PVMslave}). The master slave
calculates the necessary data and stores them in a buffer. This buffer
is send to all of the slaves. Then
the slaves perform the actual computations (after receiving and
unpacking the data of the buffer) and return the resulting
local matrices to the master. The local solutions are combined to a
single matrix which is the solution of the parallel matrix
multipliaction.


\begin{Example}
\label{ex:PVMmaster}
\begin{Scode}
mm.rpvm <- function(X, Y, n_cpu = 1) {

  dx <- dim(X) ## dimensions of matrix A
  dy <- dim(Y) ## dimensions of matrix B

  ## Input validation
  matrix.mult.validate(X,Y,dx,dy)

  ## Tags for message sending
  WORKTAG <- 17
  RESULTAG <- 82
  
  if(n_cpu == 1)
    return(X%*%Y)

  mytid <- .PVM.mytid()
  children <- .PVM.spawnR(ntask = n_cpu, slave = "mm_slave.R")
  if (all(children < 0)) {
    cat("Failed to spawn any task: ", children, "\n")
    .PVM.exit()
  }
  else if (any(children < 0)) {
    cat("Failed to spawn some tasks.  Successfully spawned ",
        sum(children > 0), "tasks\n")
    children <- children[children > 0]
  }

  nrows_on_slaves <- ceiling(dx[1]/n_cpu)
  nrows_on_last <- dx[1] - (n_cpu - 1)*nrows_on_slaves

  ## distribute data
  for (id in 1:length(children)) {
    .PVM.initsend()
    .PVM.pkint(id)
    .PVM.pkint(n_cpu)
    .PVM.pkint(nrows_on_slaves)
    .PVM.pkint(nrows_on_last)
    .PVM.pkdblmat(X)
    .PVM.pkdblmat(Y)
    .PVM.send(children[id], WORKTAG)
   }

  ## receive partial results
  partial.results <- list()
  for (child in children) {
    .PVM.recv(-1, RESULTAG)
    rank <- .PVM.upkint()
    partial.results[[rank]] <- .PVM.upkdblmat()
  }
  .PVM.exit()
  ## return in matrix form
  out <- NULL
  for(i in 1:n_cpu)
    out <- rbind(out,partial.results[[i]])
  out
}
\end{Scode}
\end{Example}


\begin{Example}
\label{ex:PVMslave}
\begin{Scode}
library("rpvm")

WORKTAG <- 17
RESULTAG <- 82

myparent  <- .PVM.parent ()

## Receive work from parent (a matrix)
buf <- .PVM.recv (myparent, WORKTAG)
rank <- .PVM.upkint() - 1
n_cpu <- .PVM.upkint()
nrows_on_slaves <- .PVM.upkint()
nrows_on_last <- .PVM.upkint()
X <- .PVM.upkdblmat()
Y <- .PVM.upkdblmat()

if(rank==(n_cpu - 1))
  local_mm <- X[(nrows_on_slaves*rank + 1):(nrows_on_slaves*rank + nrows_on_last),]%*%Y
if(rank<(n_cpu - 1))
  local_mm <- X[(nrows_on_slaves*rank + 1):(nrows_on_slaves*rank + nrows_on_slaves),]%*%Y

## Send result back
.PVM.initsend()
.PVM.pkint(rank + 1)
.PVM.pkdblmat(local_mm)
.PVM.send (myparent, RESULTAG)

## Exit PVM
.PVM.exit ()
## Exit R
q (save="no")
\end{Scode}
\end{Example}


%% Results
\input{section_mm_results.tex}


%% standalone compile
\end{document}

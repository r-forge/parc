\chapter{Parallel Computing}
\label{chap:parallelcomputing}
\section{Introduction}

In 1965 a director in the Fairchild Semiconductor division of
Fairchild Camera and Instrument Corporation predicted: "The complexity
(of integrated circuits, annotation of the author)
for minimum costs has increased at a rate of roughly two per year
[\ldots] over the short term this rate can be expected to continue, if
not increase."(~\cite{moore65integratedcircuits}) This soon became known
as Moore's Law. And 
indeed for nearly fourty years the advance predicted by Moore has
taken place (the overall growth of the world's most powerful computers
has approximately doubled each 18 months).

Furthermore, with greater speed more memory is needed, otherwise you have
the situation that a computer capable of performing trillions of
operations in each second only has access to a small memory which is
rather useless for solving data driven calculations. In addition to
that the amount of data being available to analysts today has rapidly
increased. Data-collection and -mining is done nearly everywhere. This
bulk of data implicates that enough memory is available to process it.

Now, more than 40 years later "desktop" systems with two to four
processors and lots of memory are available, which means that they can
be compared with high performance workstations of just a few years
ago.

Factors which constitute this trend are~(\cite{C1quant07}):
\begin{itemize}
\item a slowdown in uniprocessor performance arising from  diminishing
  returns in exploiting instruction-level-parallelism,
\item a growth in servers and server performance,
\item a growth in data-intensive applications,
\item the insight that increasing performance on the desktop is less
  important,
\item an improved understanding of how to use multiprocessors
  effectively, especially in server environments where there is
  significant thread-level parallelism,
\item the advantages of leveraging a design investment by replication
  rather than unique design--all multiprocessor designs provide such leverage.
\end{itemize}

With the need to solve large problems and the availability of adequate
workstations, large-scale parallel computing has become more and more
important. Some examples to illustrate this need in science and
research include:
\begin{itemize}
\item data mining for large data sets,
\item algorithms for solving NP-complete problems,
\item climate prediction,
\item computations in biology, chemistry and physics,
\item cryptography,
\item and astronomy.
\end{itemize}

The R project for Statistical Computing and Graphics is an open-source
software environment available for different
platforms~\cite{Rcore07R}. With the 
development mentioned above there have come up some extensions (called
packages) for High Performance Computing.

In this chapter the fundamentals of parallel computing are
presented. It provides a general overview of the field of High
Performance Computing and how performance of applications can be
analyzed in this field.

\section{Computer Architecture}
In this section computer architectures are briefly described. This
knowledge is of great advantage to
understand how performance can be maximized when using parallel
computing. First it is important to know how to make an application fast
on a single processor and second how to supply data efficiently to
these processors. The beginning of this chapter deals with a taxonomy of
the design alternatives for multiprocessors. After that a brief look
into processor design and memory hierarchies is given. The section
closes with a description of high performance computing servers.

\subsection{Beyond Flynn's Taxonomy}

Providing a high-level standard for programming HPC applications is a
major challenge nowadays. There is a variety of architectures for
large-scale computing. They all have specific features and 
therefore there should be a taxonomy in which such architectures can
be classified. About fourty years ago, \cite{flynn72sco}~classified
architectures on the
presence of single or multiple either instructions or data streams
known as Flynn's taxonomy:
\begin{description}
\item[Single Instruction Single Data (SISD)]
  This type of architecture of CPUs (called uniprocessors),
  developed by the 
  mathematician John von Neumann, was the standard for a long
  time. These Computers are also known as serial computers. 
\item[Multiple Instruction Single Data (MISD)] The theoritical possibility
  of applying multiple instructions on a single datum is generally
  impractical.
\item[Single Instruction Multiple Data (SIMD)] 
  A single instruction is applied by multiple processors to different
  data in parallel (data-level parallelism). 
\item[Multiple Instruction Multiple Data (MIMD)] Processors apply
  different instructions on different data (thread-level
  parallelism). See section~\ref{sec:mimd_computers} for details. 
\end{description}

But these distinctions are insufficient for classifying modern
computers according to~\cite{duncan90survey}. There are e.g.
pipelined vector processors capable of concurrent arithmetic execution and
manipulating hundreds of vector elements in parallel.

Therefore,~\cite{duncan90survey} defines that a parallel architecture
provides an explicit, high-level framework for 
the development of parallel programming solutions by providing
multiple processors that cooperate to solve problems through
concurrent execution:

\begin{description}
\item[Synchronos architectures] coordinate concurrent operations in
  lockstep through global clocks, central control unit, or vector unit
  controllers. These architectures involve pipelined vector processors
  (characterized by multiple, pipelined functional units, which
  implement arithmetic and Boolean operations), SIMD architectures
  (typically a control unit broadcasting a single instruction to
  all processors executing the instruction on local data) and systolic
  architectures (pipelined multiprocessors in which data flows from
  memory through a network of processors back to memory synchronized
  by a global clock. 
\item[MIMD architectures] consist of multiple processors applying
  different instructions on local (different) data. The MIMD models
  are asynchronous computers although they may be synchronized by
  messages passing through an interconnection network (or by accessing
  data in a shared memory). The advantage is the possibility of
  executing largely independent subcalculations. These architectures
  can further be classified in distributed or shared memory systems
  whereas the first achieves interprocess communication via 
  interconnection network and the last via global memory each
  processor can address.
\item[MIMD-based architectural paradigms] involve MIMD/SIMD hybrids,
  dataflow architectures, reduction machines, and wavefront arrays. A
  MIMD architecture can be called a MIMD/SIMD hybrid if parts of the
  MIMD architecture are controlled in SIMD fashion (some sort of
  master/slave relation). In dataflow architectures instructions are
  enabled for execution as soon as all of their operands become
  available whereas in reduction architectures an instruction is
  enabled for execution when its results are required as operands for
  another instruction already enabled for execution. Wavefront array
  processors are a mixture of systolic data piplining and asynchronous
  dataflow execution.
\end{description}

\subsection{Processor and memory}

A reduction of the execution time can be achieved with parallel
computing and this is the most important method for improving
performance. But a programmer can only achieve best performance when
he is aware of the underlying hardware. In fact parallelism can be
exploited at different levels starting with the processor
itself. Furthermore, processors have to be provided with data
sufficiently fast. If the memory is too slow having a faster processor
doesn't pay. Normally memory which delivers good performance is too
expensive to be economically feasible. This situation has led to the
development of memory hierarchies.

\subsubsection{Instruction-Level Parallelism}
A technique called pipelining allows a processor to execute different
stages of different instructions at the same time.
For example in the classic von Neumann architecture (SISD), processors
complete consecutive instructions in a fetch-execute
cycle: Each instruction is fetched, decoded, then data is fetched and
the decoded instruction is applied on the data.
A simple implementation where every instruction takes at most 5 clock
cycles can be as follows (for details see \cite{CAquant07appendixA}).
\begin{description}
\item[Instruction fetch cycle (F)] The current instruction is fetched from
  memory.
\item[Instrucion decode/register fetch cycle (D)] Decode instructions and
  read registers corresponding to register source from the register
  file.
\item[Execution/effective address cycle (X)] The arithmetic logic unit
  (ALU) operates on the operands prepared in the prior cycle.
\item[Memory access (M)] Load or store instructions.
\item[Write-back cycle (W)] Write the result into the register file.
\end{description}

Pipelining allows parallel processing of these
instructions. I.e. while one instruction is being decoded, another
instruction may be fetched and so on. On each clock cycle simply a new
instruction is started. Therefore after 5 cycles the pipe is filled
and on each subsequent clock cycle an instruction is finished. The
results of this execution pattern is shown in table
\ref{tab:pipelining}.

\begin{table}[h!b!p!]
\caption{Pipelining}
\begin{tabular}{|l|c|c|c|c|c|c|c|c|c|}
\hline
%\multicolumn{9}{|c|} \textbf{Cycle} \\
 & \textbf{Cycle} \\
\textbf{Instruction Number}
	& 1
	& 2
	& 3
	& 4
	& 5
	& 6
	& 7
	& 8
	& 9
        \\ \hline 
Instruction $i$   & F & D & X & M & W &   &   &   &   \\ \hline
Instruction $i+1$ &   & F & D & X & M & W &   &   &   \\ \hline
Instruction $i+2$ &   &   & F & D & X & M & W &   &   \\ \hline
Instruction $i+3$ &   &   &   & F & D & X & M & W &   \\ \hline
Instruction $i+4$ &   &   &   &   & F & D & X & M & W \\ \hline
\end{tabular}
\label{tab:pipelining}
\end{table}

Pipelining only works if every instruction does not depend on its
predecessor. If so, instructions may completely or partially executed
in parallel -- instruction-level parallelism.
Pipelining can be
exploited either in hardware or in software. In the hardware-based
approach (the market dominating approach) the hardware helps to
discover and achieve parallelism dynamically. In the software-based
approach parallelism is exploited statically at compile time through the
compiler. 

\subsubsection{Memory hierarchies}

Programs tend to reuse data and instructions they have used
recently. Therefore one wants to predict what instructions and data a
program will use in the near future. This is also known as the
Principle of Locality (\cite{C1quant07}). The best way to
make a program fast would be to take advantage of the fastest memory
available. But this would make the price of a computer extremely
high.

Memory hierarchies have become an economical solution in a
cost-performance sense. Between the processor and the main memory one
or more levels of fast (smaller in size) memory is inserted -- cache
memory. Fulfilling the principle of locality, cache memory stores the
instructions and data which are predicted to be used next or in
subsequent steps. This increases the throughput of the data provided that
the predictions made were correct.

\subsection{High performance computing server}

Traditionally computers can be categorized as follows:
\begin{itemize}
\item personal computers
\item workstations
\item mini computers
\item mainframe computers
\item high performance computing servers (super computers)
\end{itemize}

This has changed rapidly in the last years, as the requirements of
people went through an interesting process. Microprocessor-based
computers dominate the market. PCs and workstations have emerged as
major products in the computer industry. Servers based on
microprocessors have replaced minicomputers and mainframes have almost
been replaced with networks of multiprocessor workstations.

We are now interested in the last category namely high performance
computing servers, because these computers have the highest
computation power and they are mainly used
for scientific or technical applications. The number of processors
such a computer can have reaches from 100 to several thousand (mainly
64 bit processors).
You can divide this category of computers in ``vector computers'' or ``parallel
computers''. The amount of memory they have varies according to the type of
these servers or applications, which is running on them. The maximum
amount of memory can absolutely be beyond several tera bytes.  

This development started in the 1990's where parallel programming 
became more and more important. With the increasing need of powerful
hardware computer vendors started to build supercomputers capable of
executing more and more instructions in parallel. In the beginning
vector supercomputing system using a single shared memory were widely
used to run large-scale applications. Due to the fact that vector
systems were rather expensive in both purchasing and operation and
bus-based shared-memory parallel systems were technologically bounded
to smaller CPU counts, the first distributed-memory parallel platforms (DMPs)
came up. The big advantage was that they were inexpensive and could be
built in many different sizes. DMPs could be rather small like a
connection of workstations connected via a network to form a cluster of
workstations (COW) or arbitrarily large. In fact individual
organizations could buy clusters of a size that suited their
budgets. The big disadvantage DMPs have is the higher transportation
time of messages passing through a network in comparison to a shared
memory system. Although the technology has improved (Infiniband), there
is still space for improvements. 

Current HPC systems are still formed of vector supercomputers
(providing highest level of performance, but only for certain
applications), SMPs with two to over 100 hundred CPUs and DMPs. As
more than one CPU has become commonplace in desktop machines, clusters
can nowadays provide both small SMPs and big DMPs. When it comes to a
decition which architecture to buy, companies, laboratories or
universities often choose to use a cluster of workstations because the
cost of networking has decreased as mentioned before and performance
has increased since systems of this kind already dominate the Top500
list. Furthermore, COWs are so successful because they can be
configured from commodity chips and network and set up with the open-source
Linux operating system and therefore offer high performance for a
reasonable price.

New developments show that there are interesting alternatives. Some
HPC platforms offer a global addressing scheme and therefore enables
CPUs to directly access the entire system's memory. This can be done
with ccNUMA (cache coherent nonuniform memory access) systems
(CITATION MISSING) or
%cite hp sgi paper and sgi.pdf-luthien)
global arrays~(\cite{nieplocha96gan}). %(see section \ref{globalArrays}).
They have become an important improvement to distributed memory
platforms because amongst others it enables compiler driven
parallelizing techniques like OpenMP (see section~\ref{sec:OpenMP})to be
used on these systems. 

\section{MIMD computers}
\label{sec:mimd_computers}

On MIMD architectural based computers one has the highest flexibility
in parallel computing. Whereas in SIMD based architectures a single
instruction is applied to multiple data the MIMD paradigm allows to
apply different instructions to different data at the same time. But
SIMD architectures have become popular again recently. The Cell
processor of IBM is one of the promising CPUs of its kind. The focus in
this thesis is on MIMD computers though.

There are two different categories in which such a system can be
classified. The classification is based on how the CPU access
memory. One possibility is to have distributed memory platforms (DMPs). Each
processor has its own memory and when it comes to sharing of data, an
interconnection network is needed. Exchange of data or synchronizing
is done through message passing.

The other possibility is to have shared memory platforms (SMPs). All
processors or computation nodes can access the same memory. The
advantage is the better performance like in DMPs because messages doesn't
have to send via a slow network. Each processor is able to
fetch the necessary data directly from memory. But this kind of platform
has a big limitation. The costs of having as much memory as a cluster
of workstation would provide are enormous. That is why cluster- or
grid-computing has become so popular.

A further possibility is to have some sort of a hybrid platform. On
the one hand you have several shared memory platforms with a few
computation cores on each and on the other hand they are connected via
an interconnection network. 

\subsection{Shared Memory Systems}
\label{sec:sharedmemorysystems}
In a shared memory system multiple processors share one global
memory. They are connected to this global memory mostly via bus
technology. A big limitation of this type of machine is that though they have
a fast connection to the shared memory, saturation of the bandwidth can
hardly be avoided. Therefore scaling shared memory architectures to
more than 32 processors is rather difficult. Recently this type of
architecture has gained more attention again. Consumer PCs with two to four
processors have become common since the last year. High-end parallel
computers use 16 up to 64 processors. Recent developments try to
achieve easy parallelization of existing applications through
parallelizing compilers (ie. OpenMP see section \ref{sec:OpenMP}).

\subsection{Distributed Memory Systems}
\label{sec:distributedmemorysystems}
Since the developments done by the NASA in the 1990's (see below),
this kind of MIMD architecure has become really popular.
Distributed memory system can
entirely be build out of stock hardware and therefore give cheap
computational power. Furthermore, a big advantage is that this
architecture can easily be scaled up to several hundreds or thousands
of processors. A disadvantage of this type of systems is that
communication between the nodes is achieved through common network
technology. Therefore sending messages is rather expensive in terms of
latency and bandwidth. Another disadvantage is the complexity of
programming parallel applications. There exist several implementations
or standards which assist the developer in achieving message-passing
(for MPI see section \ref{sec:MPI} or PVM see section \ref{sec:PVM}).

\subsubsection{Beowulf clusters}
In 1995 NASA scientist completed the Beowulf project, which was a
milestone toward cluster computing.

Beowulf Clusters are scalable performance clusters based on commodity
hardware, on a private system network, with open source software
(Linux) infrastructure. The designer can improve performance
proportionally with added machines. The commodity hardware can be any
of a number of mass-market, stand-alone compute nodes as simple as two
networked computers each running Linux and sharing a file system or as
complex as 1024 nodes with a high-speed, low-latency network (from
\url{http://www.beowulf.org/}).

Since then building an efficient low-cost distributed memory computer
has not been difficult anymore. Indeed it has become the way to build
MIMD computers.

\subsubsection{The Grid}

Another (new) type of achieving high performance computing is to use a
Grid. In a Grid, computer which participate are connected to each other
via the internet (or other wide area networks). These distributed
resources are shared via a 
Grid-software. The software monitors separate resources and if
necessary supplies them with jobs. The research project SETI@home can
be said to be the first grid-like computing endeavor. Over 3 million
users work together in processing data acquired by the Acribo radio
telescope, searching for extraterrestial signals. Most of the power is
gained by utilizing unused computers (ie. with a screen saver).

The idea behind it is that computers connected to a grid can combine
their computational power with other available computing resources in the
Grid. Then, researchers or customers are able to use the combined
resources for their project. The aim of development of Grid computing
is to find certain standards so that Grid computing becomes popular
not only in certain academic institutes but also in industry. Today
there are several initiatives in europe. For example D-Grid
(\url{http://www.d-grid.de}) funded
by by BMBF, the Federal Ministry of Education and Research of Germany
or the Austrian Grid (\url{http://www.austriangrid.at}) funded by the
bm:bwk, the Federal Ministry for Education, Science and Culture of
Austria.

In a Grid more attention has to be made on heterogeneous network
computing than in smaller DMPs. Computers connected to a grid are made
from different vendors, have different architectures and different
compilers. All in all they may differ in the following areas:

\begin{itemize}
\item architecture
\item computational speed
\item machine load
\item network load and performance
\item filesystem
\item data format
\end{itemize}

A Grid middleware software like the open-source software ``globus
toolkit'' (\url{http://www.globus.org/}) interconnects all the (heterogenous)
distributed resources in a network.  
All interfaces between these heterogeneous components have to be standardized
and thus enable full interoperability between them.

\section{Parallel Programming Models}
\label{sec:programming_models}

Programmers who like to create an application which should run in
parallel have to face a challenging task. They have to distribute all
the computation involved to a large number of processors. It is
important that this is done in a way so that each of these computation
nodes performs roughly the same work. Furthermore, developers have to
ensure that the data required for the computation is available to the
processor with as little delay as possible. Therefore some sort of
coordination is required for the locality of data in the memory.
One might think that this could be hard work, and indeed it is. But
there are programming models for HPC already available which make life
easier.

But first a fundamental differentiation of parallel programs is going
to be explained. Programs by HPC developers can be done either
as the single program multiple data (SPMD) or multiple program
multiple data paradigm (MPMD).

\subsubsection{Single Program Multiple Data}

With SPMD there is only one program available
to all computation nodes. Data is distributed equally amongst all
nodes in the program. Then each node applies the same program on its
own data. You shouldn't confuse SPMD with the SIMD paradigm as in the
latter case the
distribution of the data is done prior execution of the program.

\subsubsection{Multiple Program Multiple Data}

With MPMD you have a thread like paradigm. On each node it is possible
to run different programs with either the same or different data. This
is what one would asume when speaking of MIMD architectures.  

\subsubsection{History}
When parallel computing came up there have been a lot of programming
languages developed. Only few of them experienced any real use. There
have also been many implementations of computer vendors for their own
machines. For a long time there has been no standard in sight as no
agreement between hardware vendors and developers emerged. It was
common that application developers have to write seperate HPC
applications for each architecture.

In the 1990's broad commonity efforts produced the first defacto
standards for parallel computing. Commonalities were identified
between all the implementations available and the field of parallel
computing had been understood better. Since then libraries and certain
implementations as well as compiler driven parallelizer have been
developed.


The programming models presented in this section are common and widely
used. Starting with the first generation the Message Passing Interface
(MPI - see~\cite{forum94:MPI}) and Parallel Virtual Machine (PVM -
see~\cite{geist94pvm})
are going to be described. The second generation of HPC programming
models involve OpenMP (see~\cite{openMP05}) among others. These
programming models are not the only ones 
but are, as mentioned above, commonly used. 

\subsection{Fundamentals of message-passing}
\label{sec:messagepassing}

On a distributed memory machine
(section~\ref{sec:distributedmemorysystems})data has to be transmitted
from one computation node to the other to carry out computations in
parallel. The most commonly used method for distributing data among
the nodes is message-passing.
  
A message-passing function is a function which explicitly transmits
data from one process to another. With these
functions, creating parallel programs can be extremely
efficient. Developers don't have to care about low level
message-passing anymore.

\subsubsection{Buffering}
\subsubsection{Blocking and non-blocking communication}
\subsubsection{Load balancing}
\subsubsection{Collective Communication}

\subsection{The Message Passing Interface (MPI)}
\label{sec:MPI}

In 1994 the Message Passing Interface Forum (MPIF) has defined a set
of library interface standards for message passing. Over 40
organizations participated in the discussion which started in
1993. The aim was to develop a widely standard for writing
message-passing programs. The standard was called the Message Passing
Interface (see \cite{forum94:MPI}).

As mentioned in section \ref{sec:programming_models} there
were a lot of different programming models and languages for high
performance computing available. With MPI a standard was established
that should be practical, portable, efficient and flexible. After
releasing Version 1.0 in 1994 the MPIF continued to correct errors and
made clarifications in the MPI document so that in June 1995 Version
1.1 of MPI had been released.

Further corrections and clarifications have been made to version 1.2 of
MPI and with Version 2 completely new types of functionality has been
added (see \cite{forum94:MPI-2} for details).

MPI is now a library providing higher level
routines and abstractions built upon lower level message-passing
routines which can be called from various programming
languages like C or Fortran. It contains all the infrastructure for
inter process communication. Last but not least an interface wrapper
is available for R. MPI communication can be set up using the R extension
``Rmpi'' (see section \ref{sec:Rmpi} for details).

\subsubsection{Advantages}
\begin{itemize}
\item portability through clearly defined sets of routines -- a lot of
  implementations are available for many different platforms
\item better performance compared to PVM as MPI's implementations are
  adapted to the underlying architecture.
\item interoperability within the same implementation of MPI
\item dynamic process creation since MPI-1.2
\item good scalability
\end{itemize} 

\subsubsection{Disadvantages}
\begin{itemize}
\item MPI isn't a self contained software package
\item development of parallel programs is difficult
\item no resource management like PVM 
\item heterogeneity only within an architecture
\end{itemize}


For a detailed comparison of PVM and MPI see~\cite{geist96pam}
and~\cite{gropp02ggd}.

\subsubsection{Implementations of the MPI standard}

As MPI is only a standard, specific implemenations are needed to use
message-passing routines as library calls in programming languages
like C or Fortran. There has been great effort to provide good
implementations and indeed there are a lot of them available. But
only a few of them are widely used. This is because of the fact that
the primary goal was portability. A lot of different platforms are now
supported. What follows is a description of commonly used and well
known MPI implementations.

\begin{description}
\item[LAM/MPI] is an open-source implementation of MPI. It runs on a
  network of UNIX machines connected via a local area network or via
  the Internet. On each node runs one UNIX daemon which is a
  micro-kernel plus a dozen system processes responsible for network
  communication among other things. The micro-kernel is
  responsible for the communication between local processes
  (see~\cite{burns94lam}). The LAM/MPI implementation includes all of
  the MPI-1.2 but not everything of the MPI-2 standard. LAM/MPI is
  available at~\url{http://www.lam-mpi.org/}.
\item[MPICH] is a freely available complete implementation of the MPI
  specification (MPICH1 includes all MPI-1.2 and MPICH2 all MPI-2
  specifications). The main goal was to deliver high performance with
  respect to portability. ``CH'' in MPICH stands for ``Chameleon''
  which means adaptability to one's environment and high performance
  as chameleons are fast (see~\cite{gropp96mpich}). MPICH had been
  developed during the discussion process of the MPIF and had immediately been
  available to the community after the standard had been
  confirmed. There are many other implementaions of MPI based on MPICH
  available because of its high and easy portability. Sources and
  Windows binaries are available
  from~\url{http://www-unix.mcs.anl.gov/mpi/mpich/}.
\item[Open MPI] has been build upon three MPI Implementations like
  LAM/MPI among the others. It is a new open-source MPI-2
  Implementation. The goal of Open MPI is to implement the full
  MPI-1.2 and MPI-2 specifications focusing on production-quality and
  performance (see~\cite{gabriel04:_open_mpi}). Open MPI can be
  downloaded from~\url{http://www.open-mpi.org/}.
\end{description}

For setting up these implementations in Debian GNU Linux and cluster
jobscripts see Appendix~\ref{app:mpi_imp}. 

\subsection{Parallel Virtual Machine (PVM)}
\label{sec:PVM}

The PVM system is another but own implementation of a functionally
complete message-passing model. It is designed to link computing
resources for example in a heterogenous network and to provide
developers with a parallel platform. As the message-passing model
seemed to be the paradigm of choice in the late 80's the PVM project
was created in 1989 to allow developers to exploit distributed
computing across a wide variety of computer types. A key concept in
PVM is that it makes a collection of computers appear as one large
\textit{virtual} machine, hence its name~(\cite{geist94pvm}). The PVM
system can be found on~\url{http://www.csm.ornl.gov/pvm/}. How to
setup this system on a linux machine and how to start a PVM job on a
cluster is shown in Appendix~\ref{app:mpi_imp}.

\subsubsection{Advantages}
\begin{itemize}
\item virtual machine concept
\item good support of heterogenous NOWs
\item high portability -- available for many different platforms
\item resource management, load balancing and process control
\item self-contained software package
\item dynamic process creation
\item support for fault tolerant programming
\item good scalability
\end{itemize}

\subsubsection{Disadvantages}
\begin{itemize}
\item more overhead because of portability
\item development of parallel programs is difficult
\item not as flexible as MPI as PVM is a specific implementation
\item not as many message-passing functionality as MPI has
\end{itemize}


For a detailed comparison of PVM and MPI see~\cite{geist96pam}
and~\cite{gropp02ggd}.

\subsection{OpenMP}
\label{sec:OpenMP}

A completely other approach in the parallel programming model context
is the use of shared memory. Message-passing is built upon this shared
memory model which means that every processor has direct access to the
memory of every other processor in the system
(see section~\ref{sec:sharedmemorysystems}). This class of
multiprocessor is also called Scalable Shared Memory Multiprocessor
(SSMP).

Like the development of the MPI specifications the development
of OpenMP started for one simple reason: portability. Prior the
introduction of OpenMP as an industry standard every vendor of shared
memory systems has created its own proprietary extension for
developers. Therefore a lot of people interested in parallel
programming used portabel message-passing models like MPI (see
section~\ref{sec:MPI}) or PVM (see section~\ref{sec:PVM}). As there
was an increasing demand for a simple and scalable programming model and
the desire to begin parallelizing existing code without having to
completely rewrite it, the OpenMP Architecture Review Board released a
set of compiler directives and callable runtime library routines
called the OpenMP API (see~\cite{openMP05}). This directives and
routines extend the
programming languages Fortran and C or C++ respectively to express
shared memory parallelism.

Furthermore this programming model is based on the fork/join execution
model which makes it easy to get parallelism out of a sequential
program. Therefore unlike in message-passing the program or algorithm
need not to be completely decomposed for parallel execution. Given a
working sequential program it is not difficult to incrementally
parallelize individual loops and thereby realize a performance gain in
a multiprocessor system~(\cite{dagum1997opi}). The specification and
further information can be found on the following website:
\url{http://www.openmp.org/}.

\subsubsection{Advantages}

\begin{itemize}
\item writing parallel programs with OpenMP is easy -- incrementally
  parallelizing of sequential code is possible
\item dynamic creation of threads
\item unified code -- if OpenMP is not available directives are
  treaten as comments 
\item 
\end{itemize}

\subsubsection{Disadvantages}

\begin{itemize}
\item requires a compiler which supports OpenMP
\item a shared memory platform is needed for efficient programs
\item a lot more overhead compared to message-passing function because
  of implicit programming
\item the use of OpenMP depends on the availabilty of a corresponding
  compiler
\item scalability is limited by the architecture

\end{itemize}


%%\subsection{Global Arrays}
%%\label{globalArrays}
%% 

\section{Performance Analysis}
\label{sec:perf_analysis}



\section{Hardware and Software used}
\label{sec:hardwaresoftware}

\subsection{Hardware}
The Hardware used for the applications presented in this thesis is
going to be described in this section.
At the Department of Statistics and Mathematics of the Vienna
University of Economics and Business Administration a high performance
computing server, a cluster of Intel workstations (called cluster@WU), is
available.
Furthermore, for experimenting and interactive testing an AMD Opteron
server with four computation cores has been used.   

\subsubsection{Cluster@WU}
Cluster@WU is a shared platform among several departments
of the university which are represented through an research institute
called research institute for computational methods
(FIRM~--~\url{http://www.wu-wien.ac.at/firm}). All of the programs
have been tested as well as benchmarked on this cluster of
workstations running the Sun Grid 
Engine (see section~\ref{app:gridengine} for more information on the
grid engine). With a total of 152 64-bit computation nodes and a total
of 336 gigabytes of RAM, Cluster@WU is by now amongst the fastest
supercomputers in Austria.

The cluster consists of four workstation with four cores each and 16
gigabytes of RAM. They are called ``bignodes'' as they offer more
power to the grid user. The queue for running applications on these
bignodes is called \texttt{bignode.q}. If a shared memory program is
to be run (i.e. an OpenMP program), bignodes are the computers of choice.
The other nodes consist of dual core cpus and lesser
memory. They are combined in the queue
\texttt{node.q}. Table~\ref{tab:clusterWU} provides detailed
information about the specs of the queues.

\begin{table}[h!b!p!]
\caption{cluster@WU specification}
\label{tab:clusterWU}
\begin{tabular}{|l|l|}
\hline
\textbf{bignode.q -- 4 nodes}\\
\hline
2  & Dual Core Intel XEON CPU 5140 @ 2.33 GHz\\
16 & GB RAM\\
\hline
\textbf{node.q -- 68 nodes}\\
\hline
1 & Intel Core 2 Duo CPU 6600 @ 2.4 GHz\\
4 & GB RAM\\
\hline
\end{tabular}
\end{table}

\subsubsection{AMD Opteron Server}
The Opteron machine has four cores which
have access to a total of 12 gigabytes of shared memory. The
specification of this machine shows table~\ref{tab:opteronserver}.

\begin{table}[h!b!p!]
\caption{Opteron server specification}
\label{tab:opteronserver}
\begin{tabular}{|l|l|}
\hline
\textbf{Opteron server -- 1 machine}\\
\hline
2  & Dual Core AMD Opteron @ 2.4 GHz\\
12 & GB RAM\\
1.2& TB RAID 5 storage\\
\hline
\end{tabular}
\end{table}

\subsection{Software}

\begin{description}
\item[Sun Grid Engine]
\item[Compiler] gcc icc
\item[R] gcc icc
\item[pvm]
\item[lam]
\end{description}

\section{Conclusion}

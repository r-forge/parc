\chapter{Parallel Computing}
\section{Introduction}

In 1965 a director in the Fairchild Semiconductor division of
Fairchild Camera and Instrument Corporation predicted: "The complexity
(of integrated circuits, annotation of the author)
for minimum costs has increased at a rate of roughly two per year
[\ldots] over the short term this rate can be expected to continue, if
not increase." \cite{moore} This soon became known as Moore's Law. And
indeed for nearly fourty years the advance predicted by Moore has
taken place (the overall growth of the world's most powerful computers
has approximately doubled each 18 months).

Furthermore greater speed implicates greater memory, otherwise you have
the situation that a computer capable of performing trillions of
operations in each second only has access to a small memory which is
rather useless for solving data driven calculations.

Now, more than 40 years later "desktop" systems with two to four
processors and lots of memory are available, which means that they can
be compared with high performance workstations of just a few years
ago.

Factors which constitute this trend are (\cite{architecture}):
\begin{itemize}
\item a slowdown in uniprocessor performance arising from  diminishing
  returns in exploiting instruction-level-parallelism
\item a growth in servers and server performance
\item a growth in data-intensive applications
\item the insight that increasing performance on the desktop is less
  important
\item an improved understanding of how to use multiprocessors
  effectively, especially in server environments where there is
  significant thread-level parallelism
\item the advantages of leveraging a design investment by replication
  rather than unique design--all multiprocessor designs provide such leverage
\end{itemize}

With the need to solve large problems and the availability of adequate
workstations, large-scale parallel computing has become more and more
important. Some examples to illustrate this need in science and
research include:
\begin{itemize}
\item data mining for large data sets
\item algorithms for solving NP-complete problems
\item climate prediction
\item computations in biology, chemistry and physics
\item cryptography
\item astronomy
\end{itemize}

The R project for Statistical Computing and Graphics is an open-source
software environment available for different platforms \cite{R}. With the
development mentioned above there have some extensions (called
packages) for High Performance Computing come up.

In this chapter the fundamentals of parallel computing are
presented. It provides a general overview of the field of High
Performance Computing and the possibilities to use R in this area.


\section{Computer Architecture}
In this section computer architectures are briefly described to
understand how performance can be maximized when using parallel
computing. First one need to know how one can make an application fast
on a single processor and second how can data efficiently supplied to
these processors. The beginning of this chapter deals with a taxonomy of
the design alternatives for multiprocessors. After that a brief look
into processor design and memory hierarchies is given. The section closes with a description of high performance computing
servers.

\subsection{Beyond Flynn's Taxonomy}

Providing a high-level standard for programming HPC applications is a
major challenge nowadays. There are a variety of architectures for
large-scale computing. They all have specific features and 
therefore there should be a taxonomy in which such architectures can
be classified. About fourty years ago, \cite{flynn72sco} classified
architectures on the
presence of single or multiple either instructions or data streams
known as Flynn's taxonomy:
\begin{description}
\item[Single Instruction Single Data (SISD)]
  This type of architecture of CPUs (called uniprocessors),
  developed by the 
  mathematician John von Neumann, was the standard for a long
  time. These Computers are also known as serial computers. 
\item[Multiple Instruction Single Data (MISD)] the theoritical possibility
  of applying multiple instructions on a single datum is generally
  impractical.
\item[Single Instruction Multiple Data (SIMD)] 
  a single instruction is applied by multiple processors to different
  data in parallel (data-level parallelism). 
\item[Multiple Instruction Multiple Data (MIMD)] processors apply
  different instructions on different data (thread-level
  parallelism). See section \ref{sec:mimd_computers} for details. 
\end{description}

But these distinctions are insufficient for classifying modern
computers according to \cite{duncan90survey}. For example there are
pipelined vector processors capable of concurrent arithmetic execution and
manipulating hundreds of vector elements in parallel.

Therefore \cite{duncan90survey} defines that a parallel architecture
provides an explicit, high-level framework for 
the development of parallel programming solutions by providing
multiple processors that cooperate to solve problems through
concurrent execution:

\begin{description}
\item[Synchronos architectures] coordinate concurrent operations in
  lockstep through global clocks, central control unit, or vector unit
  controllers. These architectures involve pipelined vector processors
  (characterized by multiple, pipelined functional units, which
  implement arithmetic and Boolean operations), SIMD architectures
  (typically a control unit broadcasting a single instruction to
  all processors executing the instruction on local data) and Systolic
  architectures (pipelined multiprocessors in which data flows from
  memory through a network of processors back to memory synchronized
  by a global clock. 
\item[MIMD architectures] consist of multiple processors applying
  different instructions on local (different) data. The MIMD models
  are asynchronous computers although they may be synchronized by
  messages passing through an interconnection network (or by accessing
  data in a shared memory). The advantage is the possibility of
  executing largely independent subcalculations. These architectures
  can further be classified in distributed or shared memory systems
  whereas the first achieves interprocess communication via an
  interconnection network and the last via a global memory each
  processor can address.
\item[MIMD-based architectural paradigms] involve MIMD/SIMD hybrids,
  dataflow architectures, reduction machines, and wavefront arrays. A
  MIMD architecture can be called a MIMD/SIMD hybrid if parts of the
  MIMD architecture are controlled in SIMD fashion (some sort of
  master/slave relation). In dataflow architectures instructions are
  enabled for execution as soon as all of their operands become
  available whereas in reduction architectures an instruction is
  enabled for execution when its results are required as operands for
  another instruction already enabled for execution. Wavefront array
  processors are a mixture of systolic data piplining and asynchronous
  dataflow execution.
\end{description}

\subsection{Processor and memory}



\subsection{High performance computing server}

Traditionally computers can be categorized as follows:
\begin{itemize}
\item personal computers
\item workstations
\item mini computers
\item mainframe computers
\item High performance computing servers (super computers)
\end{itemize}

This has changed rapidly in the last years, as the requirements of
people went through an interesting process. Microprocessor-based
computers dominate the market. PCs and workstations have emerged as
major products in the computer industry. Servers based on
microprocessors have replaced minicomputers and mainframes have almost
been replaced with networks of multiprocessor workstations.

We are now interested in the last category namely high performance
computing servers, because these computers have the highest
computation power and they are mainly used
for scientific or technical applications. The number of processors
such a computer can have reaches from 100 to several thousand (mainly
64 bit processors).
You can divide this category of computers in ``vector computers'' or ``parallel
computers''. The amount of memory they have varies according to the type of
these servers or application, which is running on them. The maximum
amount of memory can absolutely be beyond several tera bytes.  

This development started in the 1990's where parallel programming has
become more and more important. With the increasing need of powerful
hardware computer vendors started to build supercomputers capable of
executing more and more instructions in parallel. In the beginning
vector supercomputing system using a single shared memory were widely
used to run large-scale applications. Due to the fact that vector
systems were rather expensive in both purchasing and operation and
bus-based shared-memory parallel systems were technologically bounded
to smaller CPU counts the first distributed-memory parallel platforms (DMPs)
came up. The big advantage was that they were inexpensive and could be
built in many different sizes. DMPs could be rather small like a
connection of workstations connected via a network to form a cluster of
workstations (COW) or arbitrarily large. In fact individual
organizations could buy clusters of a size that suited their
budgets. The big disadvantage DMPs have is the higher transportation
time of messages passing through a network in comparison to a shared
memory system. Although the technology has improved (Infiniband) there
is still space for improvements. 

Current HPC systems are still formed of vector supercomputers
(providing highest level of performance, but only for certain
applications), SMPs with two to over 100 hundred CPUs and DMPs. As
more than one CPU has become commonplace in desktop machines clusters
can nowadays provide both small SMPs and big DMPs. When it comes to a
decition which architecture to buy companies, laboratories or
universities often choose to use a cluster of workstations because the
cost of networking has decreased as I mentioned before and performance
has increased since systems of this kind already dominate the Top500
list. Furthermore, COWs are so successful because they can be
configured from commodity chips and network and built the open-source
Linux operating system and therefore offer high performance for a
really good price.

New developments show that there are interesting alternatives. Some
HPC platforms offer a global addressing scheme and therefore enables
CPUs to directly access the entire system's memory. This can be done
with ccNUMA (cache coherent nonuniform memory access) systems (cite hp
sgi paper and sgi.pdf-luthien) or
global arrays (see section \ref{globalArrays})
They have become an important improvement to distributed memory
platforms because amongst others it enables compiler driven
parallelizing techniques like OpenMP (see section \ref{OpenMP})to be
used on these systems. 

\section{MIMD computers}
\label{sec:mimd_computers}


\textbf{TODO: distributed and shared memeory systems}

heterogeneous network computing see geist et al p.2

\section{Parallel Programming Models}
\label{sec:programming_models}

Programmers who like to create an application which should run in
parallel have to face a challenging task. They have to distribute all
the computation involved to a large number of processors. It is
important that this is done in a way so that each of these computation
nodes performs roughly the same work. Furthermore, developers have to
ensure that the data required for the computation is available to the
processor with as little delay as possible. Therefore some sort of
coordination is required for the locality of data in the memory.
One might think that this could be hard work, and indeed it is. But
there are programming models for HPC already available which make life
easier.

When parallel computing came up there have been a lot of programming
languages developed. Only few of them experienced any real use. There
have also been many implementations of computer vendors for their own
machines. For a long time there has been no standard in sight as no
agreement between hardware vendors and developers emerged. It was
common that application developers have to write seperate HPC
applications for each architecture.

In the 1990's broad commonity efforts produced the first defacto
standards for parallel computing. Commonalities were identified
between all the implementations available and the field of parallel
computing had been understood better. Since then libraries and certain
implementations as well as compiler driven parallelizer have been
developed.

The programming models presented in this section are common and widely
used. Starting with the first generation the Message Passing Interface
(MPI - see \cite{forum94:MPI}) and Parallel Virtual Machine (PVM - see
\cite{geist94pvm})
are going to be described. The second generation of HPC programming
models involve OpenMP (see \cite{openMP05}) and Global Arrays (GA - see
\cite{nieplocha96gan}). These programming models are not the only ones
but are commonly used. 

\subsection{The Message Passing Interface (MPI)}
\label{sec:MPI}

In 1994 the Message Passing Interface Forum (MPIF) has defined a set
of library interface standards for message passing. Over 40
organizations participated in the discussion which started in
1993. The aim was to develop a widely standard for writing
message-passing programs. The standard was called the Message Passing
Interface (see \cite{forum94:MPI}).

As mentioned in section \ref{sec:programming_models} before there
were a lot of different programming models and languages for high
performance computing available. With MPI a standard was established
that should be practical, portable, efficient and flexible. After
releasing Version 1.0 in 1994 the MPIF continued to correct errors and
made clarifications in the MPI document so that in June 1995 Version
1.1 of MPI had been released.

Further corrections and clarifications have been made Version 1.2 of
MPI and with Version 2 completely new types of functionality has been
added (see \cite{forum94:MPI-2} for details).

MPI is now a library which can be called from various programming
languages like C or Fortran. It contains all the infrastructure for
inter process communication.

\textbf{TODO: Advantages/Disadvantages}

\subsubsection{Why using message-passing?}
\label{sec:why_m-p}

If one uses a distributed memory machine (DMP) as platform for
carrying out computations in parallel, data hs to be transmitted from
one computation node to the other. With MPI there are higher level
routines and abstractions available which build upon lower level
message-passing routines.
  
A message-passing function is a function which explicitly transmits
data from one process to another. With these
functions, creating parallel programs can be extremely
efficient. Developers don't have to care about low level
message-passing anymore.

\subsubsection{Implementations of the MPI standard}

\textbf{TODO: introduction to implementations}

\begin{description}
\item[LAM/MPI] is an open-source implementation of MPI. It runs on a
  network of UNIX machines connected via a local area network or via
  the Internet. On each node runs one UNIX daemon which is a
  micro-kernel plus a dozen system processes responsible for network
  communication among other things. The micro-kernel is
  responsible for the communication between local processes (see
  \cite{burns94lam}). The LAM/MPI implementation includes all of the
  MPI-1.2 but not everything of the MPI-2 standard. LAM/MPI is
  available at \url{http://www.lam-mpi.org/}.
\item[MPICH] is a freely available complete implementation of the MPI
  specification (MPICH1 includes all MPI-1.2 and MPICH2 all MPI-2
  specifications). The main goal was to deliver high performance with
  respect to portability. ``CH'' in MPICH stands for ``Chameleon''
  which means adaptability to one's environment and high performance
  as chameleons are fast (see \cite{gropp96mpich}). MPICH had been
  developed during the discussion process of the MPIF and had immediately been
  available to the community after the standard had been
  confirmed. There are many other implementaions of MPI based on MPICH
  available because of its high and easy portability. Sources and
  Windows binaries are available from
  \url{http://www-unix.mcs.anl.gov/mpi/mpich/}.
\item[Open MPI] has been build upon three MPI Implementations like
  LAM/MPI among the others. It is a new open-source MPI-2
  Implementation. The goal of Open MPI is to implement the full
  MPI-1.2 and MPI-2 specifications focusing on production-quality and
  performance (see \cite{gabriel04:_open_mpi}). Open MPI can be
  downloaded from  \url{http://www.open-mpi.org/}.
\end{description}

for setting up these implementations in Debian GNU Linux and cluster
jobscripts see Appendix \ref{app:mpi_imp}. 

\subsection{Parallel Virtual Machine (PVM)}
\label{sec:PVM}

The PVM system is another but own implementation of a functionally
complete message-passing model. It is designed to link computing
resources for example in a heterogenous network and to provide
developers with a parallel platform. As the message-passing model
seemed to be the paradigm of choice in the late 80's the PVM project
was created in 1989 to allow developers to exploit distributed
computing across a wide variety of computer types. A key concept in
PVM is that it makes a collection of computers appear as one large
\textit{virtual} machine, hence its name \cite{geist94pvm}. The PVM
system can be found on \url{http://www.csm.ornl.gov/pvm/}. In Appendix
\ref{app:mpi_imp} one can find on how to setup this system on a linux
machine and how to start a PVM job on a cluster.

\textbf{TODO: Advantages/Disadvantages}


\subsection{OpenMP}
\label{sec:OpenMP}

A completely other approach in the parallel programming model context
is the use of shared memory. Message-passing is built upon this shared
memory model that means every processor has direct access to the
memory of every other processor in the system. This class of
multiprocessor is also called Scalable Shared Memory Multiprocessor
(SSMP). Like the development of the MPI specifications the development
of OpenMP started for one simple reason: portability. Prior the
introduction of OpenMP as an industry standard every vendor of shared
memory systems has created its own proprietary extension for
developers. Therefore a lot of people interested in parallel
programming used portabel message-passing models like MPI (see
Section \ref{sec:MPI}) or PVM (see Section \ref{sec:PVM}). As there
was an increasing demand for a simple and scalable programming model and
the desire to begin parallelizing existing code without having to
completely rewrite it, the OpenMP Architecture Review Board released a
set of compiler directives and callable runtime library routines
called the OpenMP API (see \cite{openMP05}). This directives and routines extend the
programming languages Fortran and C or C++ respectively to express
shared memory parallelism.
Furthermore this programming model is based on the fork/join execution
model which makes it easy to get parallelism out of a sequential
program. Therefore unlike in message-passing the program or algorithm
need not to be completely decomposed for parallel execution. Given a
working sequential program it is not difficult to incrementally
parallelize individual loops and thereby realize a performance gain in
a multiprocessor system (\cite{dagum1997opi}). The specification and
further information can be found on the following website:
\url{http://www.openmp.org/}.

\textbf{TODO: Advantages/Disadvantages}

\subsection{Global Arrays}
\label{globalArrays}


\section{Performance Analysis}
\label{sec:perf_analysis}

\section{Implementation of parallel applications in R}
\label{sec:parallel_R}

\subsection{Example: Matrix multiplication}
\subsection{The Rmpi package}
\subsection{The Rpvm package}
\subsection{SNOW package}
\subsection{Building an OpenMP shared library}
\subsection{Comparison}

\section{Hardware used}

\textbf{TODO: own section? short description of machines, details in
  appendix}

The Hardware used for the applications presented in this thesis is
going to be described in this section.
At the Department of Statistics and Mathematics of the Vienna
University of Economics and Business Administration a high performance
computing server, a cluster of workstations, is
available. Cluster@WU is a shared Platform among several departments
of the university which are represented through a research institute
for computational methods. 
\begin{table}

\end{table}
R-Forge specs

2 x Dual Core AMD Opteron 2.4 GHz
12 GB RAM
1.2 TB RAID 5

Cluster specs (node.q, bignode, bignode.q)

4 Nodes (bignodes):
2 x Dual Core Intel XEON CPU 5140 @ 2.33 GHz
16 GB RAM

68 Nodes:
1 x Intel Core 2 Duo CPU 6600 @ 2.4 GHz
4 GB RAM


\section{Conclusion}

TODO: performance in a closer sense

      benchmarks

      LINPACK benchmark (implement LINPACK Java in R
      http://www.top500.org/about/linpack

      performance in a wider sense

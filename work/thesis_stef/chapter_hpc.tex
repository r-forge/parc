\chapter{Parallel Computing}
\label{chap:parallelcomputing}
\section{Introduction}
\label{sec:parallelintro}
In 1965 a director in the Fairchild Semiconductor division of
Fairchild Camera and Instrument Corporation predicted: ``The complexity
(of integrated circuits, annotation of the author)
for minimum costs has increased at a rate of roughly two per year
[\ldots] over the short term this rate can be expected to continue, if
not increase.''(~\cite{moore65integratedcircuits}) This soon became known
as Moore's Law. And 
indeed for nearly forty years the advance predicted by Moore has
taken place (the overall growth of the world's most powerful computers
has approximately doubled every 18 months).

Furthermore, with greater speed more memory is needed, otherwise one has
the situation that a computer capable of performing trillions of
operations in every second only has access to a small memory which is
rather useless for performing huge data driven calculations. In addition to
that the amount of data being available to analysts today has rapidly
increased. Data collection and mining is done nearly everywhere. This
bulk of data implies that enough memory is available to process
it. With this development a new field in computer science established
and has become increasingly important over the last decades---high
performance computing.

\subsubsection{High Performance Computing}

The term high performance computing (HPC) refers to the use of (parallel)
supercomputers and computer clusters (\cite{wiki:hpc}). Indeed it can
be explained as computing on a high performance computing
machine. This machines can be clusters of workstations or huge shared
memory machines (see Section~\ref{sec:hpcserver} for more details on
high performance computing servers).

Furthermore, HPC is a branch of computer science that concentrates on
developing high performance computers and software to run on these
computers. An important area of this discipline is the development of
parallel processing algorithms or software referred to as
\textbf{parallel computing}.
 
\subsubsection{Trends Constituting High Performance Computing}
Now, more than 40 years after Moore's testimony desktop systems
with two to four 
processors and lots of memory are available. This means that standard
stock computers can be compared with high performance workstations of
just a few years ago.

Factors which constitute this trend are~(\cite{C1quant07}):
\begin{itemize}
\item a slowdown in uniprocessor performance arising from diminishing
  returns in exploiting instruction level parallelism,
\item a growth in servers and server performance,
\item a growth in data intensive applications,
\item the insight that increasing performance on the desktop is less
  important,
\item an improved understanding of how to use multiprocessors
  effectively, especially in server environments where there is
  significant thread level parallelism,
\item the advantages of leveraging a design investment by replication
  rather than unique design---all multiprocessor designs provide such leverage.
\end{itemize}

With the need to solve large problems and the availability of adequate
workstations, large-scale parallel computing has become more and more
important. Some examples to illustrate this need in science and
industry include:

\begin{itemize}
\item data mining for large data sets,
\item algorithms for solving $\mathcal{NP}$-complete problems,
\item climate prediction,
\item computations in biology, chemistry and physics,
\item cryptography,
\item and astronomy.
\end{itemize}

The R project for Statistical Computing and Graphics is an open source
software environment available for different
platforms. With the 
development mentioned above there have come up some extensions (called
packages) for high performance computing. That means R is already
prepared to be used in this field. A more detailed overview of high
performance computing in connection with R can be found in a separate
Chapter~\ref{chap:Rhpc}.


In this chapter the fundamentals of parallel computing are
presented. It provides a general overview of the field of high
performance computing and how performance of applications can be
analyzed in this field.

\section{Terms and Definitions}
\label{sec:termsdef}


Before we go into further the terms, which are commonly used
when dealing with parallel computing, have to be
defined. It is important to know the difference between processes
and threads as they are the key figures in parallel computing.

Then, the difference between parallelism and concurrency is illustrated
as they are often confused or said to be the same.

Eventually
computational overhead and scalability are explained.


\subsection{Process vs. Thread}

Both threads and processes are methods of parallelizing an
application. The difference between them is the way they are
created and share resources.

A \textbf{process} is the execution of a list of statements (a sequential
program). Processes have their own state information, use their own 
address space and interact with other processes only via an
interprocess communication mechanism generally managed by the operating
system. A master process may spawn subprocesses which are logically
separated from the functionality of the master process and other
subprocesses.

In contrast to processes, \textbf{threads} are typically spawned from processes
for a short time to achieve a certain task and then
terminate---\textbf{fork-join principle}. Within a process threads 
share the same state and same memory space, and can communicate with
each other directly through shared variables.

\subsection{Parallelism vs. Concurrency}

\textbf{Parallelism} is physically simultaneous processing of multiple
threads or processes with the objective
to increase performance (this implies multiple processing elements)
whereas \textbf{concurrency} is logically simultaneous processing of
threads or processes regardless of a performance gain (this does not
imply multiple processing elements). I.e., concurrent execution of
processes can be interleaved execution on a single processing
element. Table~\ref{tab:concurrency} shows interleaved execution of 3
processes. In every cycle the process executed (X) changes: in the
first cycle process $1$ is executed, in the second cycle process $2$
and after process $3$ has started in cycle $3$, process $1$ continues
its execution in cycle $4$. Table~\ref{tab:parallelism} shows the
execution of three parallel processes running on a computer with three
processing elements. 

\begin{table}[h!b!p!]
\caption{Interleaved Concurrency}
\centering
\begin{tabular}{|l|c|c|c|c|c|c|c|c|c|}
\hline
 & \multicolumn{9}{|c|}{\textbf{Cycle}} \\
\textbf{Process}
	& 1
	& 2
	& 3
        & 4
        & 5
        & 6
        & 7
        & 8
        & 9
        \\ \hline 
Process $1$ & X &   &   & X &   &   & X &   &   \\ \hline
Process $2$ &   & X &   &   & X &   &   & X &   \\ \hline
Process $3$ &   &   & X &   &   & X &   &   & X \\ \hline
\end{tabular}
\label{tab:concurrency}
\end{table}

\begin{table}[h!b!p!]
\caption{Parallelism}
\centering
\begin{tabular}{|l|c|c|c|c|c|c|c|c|c|}
\hline
 & \multicolumn{9}{|c|}{\textbf{Cycle}} \\
\textbf{Process}
	& 1
	& 2
	& 3
        & 4
        & 5
        & 6
        & 7
        & 8
        & 9
        \\ \hline 
Process $1$ & X & X & X &   &   &   & X & X & X \\ \hline
Process $2$ &   &   & X & X & X & X & X & X & X \\ \hline
Process $3$ &   & X & X & X & X & X &   &   &   \\ \hline
\end{tabular}
\label{tab:parallelism}
\end{table}

In general, concurrent programs can be executed 
in parallel whereas parallel programs cannot be reduced to logically
concurrent processes. 

For description of concurrent programming languages and programming
models see \cite{gehani88:concurrentprog}.

\subsection{Computational Overhead}

An often used term when dealing with implementations of parallel
program is overhead.

\textbf{Overhead} is generally considered any combination of excess or indirect
computation time, memory, bandwidth, or other
resources that are required to be utilized or expanded to enable a
particular goal (\cite{wiki:overhead}).

In parallel computing this can be the excess of computation time to
send data between one process to another.

Overhead may also be a criterion to decide whether a feature should be
included or not in a parallel program or whether a parallel
programming paradigm should be used or not.

\subsection{Scalability}

When using more than one processor to carry out a computation one is
interested in the increase of performance which can be achieved by
adding CPUs. The capability of a system to increase the performance
under an increased load when resources (in this case CPUs) are added
is referred to as \textbf{scalability}. For example, when the number of
CPUs is doubled and the computation of the same task takes nearly half
the time, the 
system scales well. If this is not the case because it only reduces
time by 1 or 2 percent then bad scalability is given. 

\section{Computer Architecture}

In this section computer architectures are briefly described. This
knowledge is of major interest to
understand how performance can be maximized when using parallel
computing. First it is important to know how to make an application fast
on a single processor. As soon as this is done one may begin to
parallelize this application and think of how to supply data efficiently to
more than one processors. In this section we try to summarize the key
concepts in computer architecture and to show what we have to keep in
mind when parallelizing code.

The beginning of this section deals with a taxonomy of
the design alternatives for multiprocessors. After that a brief look
into processor design and memory hierarchies is given. This section
closes with a description of high performance computing servers.

\subsection{Beyond Flynn's Taxonomy}
\label{sec:flynn}
Providing a high level standard for programming HPC applications is a
major challenge nowadays. There is a variety of architectures for
large-scale computing. They all have specific features and 
therefore there should be a taxonomy in which such architectures can
be classified. About forty years ago, \cite{flynn72sco}~classified
architectures on the
presence of single or multiple either instructions or data streams
known as Flynn's taxonomy:
\begin{description}
\item[Single Instruction Single Data (SISD)]
  This type of architecture of CPUs (called uniprocessors),
  developed by the 
  mathematician John von Neumann, was the standard for a long
  time. These computers are also known as serial computers. 
\item[Multiple Instruction Single Data (MISD)] The theoretical possibility
  of applying multiple instructions on a single datum is generally
  impractical.
\item[Single Instruction Multiple Data (SIMD)] 
  A single instruction is applied by multiple processors to different
  data in parallel (data-level parallelism). 
\item[Multiple Instruction Multiple Data (MIMD)] Processors apply
  different instructions on different data (thread-level
  parallelism). See Section~\ref{sec:mimd_computers} for details. 
\end{description}

But these distinctions are insufficient for classifying modern
computers according to~\cite{duncan90survey}. There are e.g.
pipelined vector processors capable of concurrent arithmetic execution
and manipulating hundreds of vector elements in parallel.

Therefore,~\cite{duncan90survey} defines that a parallel architecture
provides an explicit, high level framework for 
the development of parallel programming solutions by providing
multiple processors that cooperate to solve problems through
concurrent execution:

\begin{description}
\item[Synchronous architectures] coordinate concurrent operations in
  lockstep through global clocks, central control unit, or vector unit
  controllers. These architectures involve pipelined vector processors
  (characterized by multiple, pipelined functional units, which
  implement arithmetic and Boolean operations), SIMD architectures
  (typically a control unit broadcasting a single instruction to
  all processors executing the instruction on local data) and systolic
  architectures (pipelined multiprocessors in which data flows from
  memory through a network of processors back to memory synchronized
  by a global clock). 
\item[MIMD architectures] consist of multiple processors applying
  different instructions on local (different) data. The MIMD models
  are asynchronous computers although they may be synchronized by
  messages passing through an interconnection network (or by accessing
  data in a shared memory). The advantage is the possibility of
  executing largely independent sub calculations. These architectures
  can further be classified in distributed or shared memory systems
  whereas the former achieves interprocess communication via 
  interconnection network and the latter via global memory each
  processor can address.
\item[MIMD-based architectural paradigms] involve MIMD/SIMD hybrids,
  dataflow architectures, reduction machines, and wavefront arrays. A
  MIMD architecture can be called a MIMD/SIMD hybrid if parts of the
  MIMD architecture are controlled in SIMD fashion (some sort of
  master/slave relation). In dataflow architectures instructions are
  enabled for execution as soon as all of their operands become
  available whereas in reduction architectures an instruction is
  enabled for execution when its results are required as operands for
  another instruction already enabled for execution. Wavefront array
  processors are a mixture of systolic data pipelining and asynchronous
  dataflow execution.
\end{description}

These paradigms have to be kept in mind when designing high performance
software as each architecture implies a specific way of writing
efficient code. Luckily, platform specific libraries and compilers
tuned to run optimal on the corresponding hardware are available to
developers. They help to reduce the amount of work necessary to
create a parallel application.

\subsection{Processor and Memory}
\label{sec:processorandmemory}
A reduction of the execution time can be achieved when taking
advantage of parallel
computing and this is an essential possibility for improving
performance. But a programmer can only achieve best performance when
he is aware of the underlying hardware. Before making use of
parallelism the performance of the sequential parts of the code has to
be maximized. This means the serial routines of a parallel program
have to be optimized to run on a single processor. In fact parallelism can be
exploited at different levels starting with the processor
itself (e.g., pipelining).

Furthermore, processors have to be provided with data
sufficiently fast. If the memory is too slow having a fast processor
does not pay. Typically memory which delivers good performance is too
expensive to be economically feasible. This situation has led to the
development of memory hierarchies.

\subsubsection{Instruction Level Parallelism}
A technique called \textbf{pipelining} allows a processor to execute different
stages of different instructions at the same time.
For example in the classic von Neumann architecture (SISD), processors
complete consecutive instructions in a fetch-execute
cycle: Each instruction is fetched, decoded, then data is fetched and
the decoded instruction is applied on the data.
A simple implementation where every instruction takes at most 5 clock
cycles can be as follows (for details see Appendix A of~\cite{C1quant07}).
\begin{description}
\item[Instruction fetch cycle (F)] The current instruction is fetched from
  memory.
\item[Instruction decode/register fetch cycle (D)] Decode instructions and
  read registers corresponding to register source from the register
  file.
\item[Execution/effective address cycle (X)] The arithmetic logic unit
  (ALU) operates on the operands prepared in the prior cycle.
\item[Memory access (M)] Load or store instructions.
\item[Write-back cycle (W)] Write the result into the register file.
\end{description}

Pipelining allows parallel processing of these
instructions. I.e., while one instruction is being decoded, another
instruction may be fetched and so on. On each clock cycle simply a new
instruction is started. Therefore after 5 cycles the pipe is filled
and on each subsequent clock cycle an instruction is finished. The
results of this execution pattern is shown in table
\ref{tab:pipelining}.

\begin{table}[h!b!p!]
\caption{Pipelining}
\centering
\begin{tabular}{|l|c|c|c|c|c|c|c|c|c|}
\hline
 & \multicolumn{9}{|c|}{\textbf{Cycle}} \\
\textbf{Instruction Number}
	& 1
	& 2
	& 3
	& 4
	& 5
	& 6
	& 7
	& 8
	& 9
        \\ \hline 
Instruction $i$   & F & D & X & M & W &   &   &   &   \\ \hline
Instruction $i+1$ &   & F & D & X & M & W &   &   &   \\ \hline
Instruction $i+2$ &   &   & F & D & X & M & W &   &   \\ \hline
Instruction $i+3$ &   &   &   & F & D & X & M & W &   \\ \hline
Instruction $i+4$ &   &   &   &   & F & D & X & M & W \\ \hline
\end{tabular}
\label{tab:pipelining}
\end{table}

Pipelining only works if every instruction does not depend on its
predecessor. If so, instructions may completely or partially be executed
in parallel---instruction level parallelism.
Pipelining can be
exploited either in hardware or in software. In the hardware based
approach (the market dominating approach) the hardware helps to
discover and achieve parallelism dynamically. In the software based
approach parallelism is exploited statically at compile time through the
compiler. 

\subsubsection{Memory Hierarchies}

The best way to
make a program fast would be to take advantage entirely of the fastest memory
available. But this would make the price of a computer extremely
high. Memory hierarchies have become an economical solution in a
cost-performance sense. Between the processor and the main memory one
or more levels of fast (smaller in size) memory is inserted---cache
memory.

Programs tend to reuse data and instructions they have used
recently. Therefore one wants to predict what instructions and data a
program will use in the near future and keep them in higher (faster)
levels of the memory hierarchy. This is also known as the
\textbf{principle of locality} (\cite{C1quant07}). Fulfilling the
principle of locality, cache memory stores the 
instructions and data which are predicted to be used next or in
subsequent steps. This increases the throughput of the data provided that
the predictions made were correct.

\subsection{High Performance Computing Server}
\label{sec:hpcserver}

High performance computing refers to the use of (parallel)
supercomputers and computer clusters (see
Section~\ref{sec:parallelintro}). This section gives an overview of
high performance computing servers and how they are classified
among all computer types.


Traditionally computers have been categorized as follows:
\begin{itemize}
\item personal computers
\item workstations
\item mini computers
\item mainframe computers
\item high performance computing servers (super computers)
\end{itemize}

This has changed rapidly in the last years, as the requirements of
people are now different than they were before. Microprocessor-based
computers dominate the market. PCs and workstations have emerged as
major products in the computer industry. Servers based on
microprocessors have replaced minicomputers and mainframes have almost
been replaced with networks of multiprocessor workstations.

We are now interested in the last category namely high performance
computing servers, because these computers have the highest
computational power and they are mainly used
for scientific or technical applications. The number of processors
such a computer can have reaches from hundred to several thousand. The
trend is to use 64 bit processors as this kind allows addressing more
memory (and they have become commonplace now). 
This category of computers can be divided into the following
categories: ``vector computers'' and ``parallel computers''.
The amount of memory they have varies according to the type of
these servers or applications which are running on them. The maximum
amount of memory can absolutely be beyond several tera bytes.  

This development started in the 1990's where parallel programming 
experienced major interest. With the increasing need of powerful
hardware, computer vendors started to build supercomputers capable of
executing more and more instructions in parallel. In the beginning
vector supercomputing systems using a single shared memory were widely
used to run large-scale applications. Due to the fact that vector
systems were rather expensive in both purchasing and operation and
bus based shared memory parallel systems were technologically bound
to smaller CPU counts, the first distributed memory parallel platforms (DMPs)
came up. The advantage of these platforms was that they were
inexpensive with respect to their computational power and could be 
built in many different sizes. DMPs could be rather small like a
few workstations connected via a network to form a cluster of
workstations (COW) or arbitrarily large. In fact, individual
organizations could buy clusters of a size that suited their
budgets. The major disadvantage DMPs have is the higher latency and
smaller throughput of messages passing through a network in comparison
to a shared memory system. Although the technology has improved
(Infiniband), there is still room for improvements. 

Current HPC systems are still formed of vector supercomputers
(providing highest level of performance, but only for certain
applications), SMPs with two to over 100 hundred CPUs and DMPs. As
more than one CPU has become commonplace in desktop machines, clusters
can nowadays provide both small SMPs and big DMPs. When it comes to a
decision which architecture to buy, companies, laboratories or
universities often choose to use a cluster of workstations because the
cost of networking has decreased as mentioned before and performance
has increased since systems of this kind already dominate the Top500
list. Furthermore, COWs are so successful because they can be
configured from commodity chips and network technology. Moreover they
are typically set up with the open source
Linux operating system and therefore offer high performance for a
reasonable price.

New developments show that there are interesting alternatives. Some
HPC platforms offer a global addressing scheme and therefore enable
CPUs to directly access the entire system's memory. This can be done
e.g., with ccNUMA (cache coherent nonuniform memory access) systems
(for more information see e.g., \cite{kleen05NUMA}) or
global arrays~(\cite{nieplocha96gan}).
They have become an important improvement because they allow a
processor to access local and remote memory with rather low latency.

Another advantage of having a global addressing scheme is that it
enables compiler driven parallelizing techniques like OpenMP (see
Section~\ref{sec:OpenMP}) to be used on these systems. Otherwise the
shared memory paradigm has to be maintained through a distributed
shared memory subsystem like in Cluster OpenMP
(\cite{hoeflinger06clusterOpenMP}).

\section{MIMD Computers}
\label{sec:mimd_computers}

Recalling Section~\ref{sec:flynn} we know that computer architecture
plays an important role for designing parallel programs and from  
Section~\ref{sec:hpcserver} we know how high performance computing
servers have developed since the last decade. We are now
interested in an architecture which supports parallel process execution
and how distributed processing entities can be connected to
form a single unit. According to Flynn's taxonomy this would be MIMD
or SIMD architectures combined in a shared or distributed memory system.


On MIMD architectural based computers one has the highest flexibility
in parallel computing. Whereas in SIMD based architectures a single
instruction is applied to multiple data the MIMD paradigm allows to
apply different instructions to different data at the same time. But
SIMD architectures have become popular again recently. The Cell
processor of IBM (\cite{kahle05cell}) is one of the promising CPUs of
its kind. The focus in 
this thesis is on MIMD computers though.


There are two different categories in which such a system can be
classified. The classification is based on how the CPU accesses
memory. One possibility is to have distributed memory platforms (DMPs). Each
processor has its own memory and when it comes to sharing of data, an
interconnection network is needed. Exchange of data or synchronizing
is done through message passing.

The other possibility is to have shared memory platforms (SMPs). All
processors or computation nodes can access the same memory. The
advantage is the better performance like in DMPs because messages do not
have to be sent via a slow network. Each processor is able to
fetch the necessary data directly from memory. But this kind of platform
has a big limitation. The costs of having as much memory as a cluster
of workstation would provide are enormous. That is why cluster or
grid computing has become so popular.

A further possibility is to have some sort of a hybrid platform. On
the one hand one has several shared memory platforms with a few
computation cores on each and on the other hand they are connected via
an interconnection network. 

\subsection{Shared Memory Systems}
\label{sec:sharedmemorysystems}
In a shared memory system multiple processors share one global
memory. They are connected to this global memory mostly via bus
technology. A big limitation of this type of machine is that though they have
a fast connection to the shared memory, saturation of the bandwidth can
hardly be avoided. Therefore scaling shared memory architectures to
more than 32 processors is rather difficult. Recently this type of
architecture has gained more attention again. Consumer PCs with two to four
processors have become common since the last year. High-end parallel
computers use 16 up to 64 processors. Recent developments try to
achieve easy parallelization of existing applications through
parallelizing compilers (i.e., OpenMP see Section~\ref{sec:OpenMP}).

\subsection{Distributed Memory Systems}
\label{sec:distributedmemorysystems}
Since the developments done by the NASA in the 1990's (the Beowulf
project, \cite{becker95bpw}),
this kind of MIMD architecture has become really popular.
Distributed memory systems can
entirely be built out of stock hardware and therefore give access to
cheap computational power. Furthermore, a big advantage is that this
architecture can easily be scaled up to several hundreds or thousands
of processors. A disadvantage of this type of systems is that
communication between the nodes is achieved through common network
technology. Therefore sending messages is rather expensive in terms of
latency and bandwidth. Another disadvantage is the complexity of
programming parallel applications. There exist several implementations
or standards which assist the developer in achieving message passing
(for MPI see Section~\ref{sec:MPI} or PVM see Section~\ref{sec:PVM}).

\subsubsection{Beowulf Clusters}
In 1995 NASA scientist completed the Beowulf project, which was a
milestone toward cluster computing.

``Beowulf Clusters are scalable performance clusters based on commodity
hardware, on a private system network, with open source software
(Linux) infrastructure. The designer can improve performance
proportionally with added machines. The commodity hardware can be any
of a number of mass-market, stand-alone compute nodes as simple as two
networked computers each running Linux and sharing a file system or as
complex as 1024 nodes with a high-speed, low-latency network.'' (from
\url{http://www.beowulf.org/})

Since then building an efficient low-cost distributed memory computer
has not been difficult anymore. Indeed it has become the way to build
MIMD computers.

\subsubsection{The Grid}

Another (new) type of achieving high performance computing is to use a
grid (\cite{foster99gbf}). In a grid, computers which participate are
connected to each other
via the Internet (or other wide area networks). These distributed
resources are shared via a 
grid software. The software monitors separate resources and if
necessary supplies them with jobs. The research project SETI@home
(\cite{korpela01shm} and \cite{anderson02she}) can
be said to be the first grid-like computing endeavor. Over 3 million
users work together in processing data acquired by the Acribo radio
telescope, searching for extraterrestrial signals. Most of the power is
gained by utilizing unused computers (e.g., with a screen saver).

The idea behind it is that computers connected to a grid can combine
their computational power with other available computing resources in the
grid. Then, researchers or customers are able to use the combined
resources for their projects. The aim of development of grid computing
is to find standards so that grid computing becomes popular
not only in academic institutes but also in industry. Today
there are several initiatives in Europe. For example D-Grid
(\url{http://www.d-grid.de}) funded
by by BMBF, the Federal Ministry of Education and Research of Germany
or the Austrian Grid (\url{http://www.austriangrid.at}) funded by the
bm.w\_f, the Federal Ministry for Science and Research of
Austria.

In a grid more attention has to be paid on heterogeneous network
computing than in smaller DMPs which typically consist of many machines
with the same architecture and are most likely from the same
vendor. Computers connected to a grid are made 
from different vendors, have different architectures and different
compilers. All in all they may differ in the following areas:

\begin{itemize}
\item architecture
\item computational speed
\item machine load
\item network load and performance
\item file system
\item data format
\end{itemize}

A Grid middleware software like the open-source software ``globus
toolkit'' (\cite{foster97gmi}---software available from
\url{http://www.globus.org/}) interconnects all the (heterogeneous)
distributed resources in a network.
All interfaces between these heterogeneous components have to be standardized
and thus enable full interoperability between them.

\section{Parallel Programming Models}
\label{sec:programming_models}

In Section~\ref{sec:hpcserver} and~\ref{sec:mimd_computers} we
presented the hardware used when dealing with parallel computing which is
more or less given to the developers. The major task for a programmer
is to use the given infrastructure and provide efficient solutions for
computational intensive applications.

Creating a parallel application is a challenging task for every
programmer. They have to distribute all
the computations involved to a large number of processors. It is
important that this is done in a way so that each of these computation
nodes performs roughly the same amount of work. Furthermore,
developers have to
ensure that data required for the computation is available to the
processor with as little delay as possible. Therefore some sort of
coordination is required for the locality of data in the memory (for
more information on the design of a parallel program see Section
\ref{sec:designofpar}). 
One might think that this would be hard work, and indeed it is. But
there are programming models for high performance parallel computing
already available which make the developer's life easier.


When parallel computing came up parallel  programming
languages like Concurrent C, Ada, or Linda (see i.e.,
\cite{gehani88:concurrentprog} and \cite{wilson96:parallelprog}) had been
developed. Only few of them experienced any real use. There
have also existed many implementations of computer vendors for their own
machines. For a long time there was no standard in sight as no
agreement between hardware vendors and developers emerged. It was
common that application developers had to write separate HPC
applications for each architecture.

In the 1990's broad community efforts produced the first defacto
standards for parallel computing. Commonalities were identified
between all the implementations available and the field of parallel
computing had been understood better. Since then libraries and concrete
implementations as well as compilers capable of automatic
parallelization have been developed.


The programming models presented in this section are common and widely
used. First we start with a description of fundamental concepts of
message passing as they are needed in subsequent sections. Then the
first generation, the Message Passing Interface 
(MPI---see~\cite{forum94:MPI}) and Parallel Virtual Machine
(PVM---see~\cite{geist94pvm}),
are described. The second generation of HPC programming
models involve OpenMP (see~\cite{openMP05}) among others. These
programming models are not the only ones 
but are, as mentioned above, commonly used. 

\subsection{Fundamentals of Message Passing}
\label{sec:messagepassing}

To understand subsequent sections it is important to know the exact
meaning of message passing and the existing building blocks.
On a distributed memory platform
(Section~\ref{sec:distributedmemorysystems}) data has to be transmitted
from one computation node to the other to carry out computations in
parallel. The most commonly used method for distributing data among
the nodes is message passing.
  
\begin{description}
\item[A message passing function] is a function which explicitly transmits
  data from one process to another. With these
  functions, creating parallel programs can be extremely
  efficient. Developers do not have to care about low level
  message passing anymore.
\end{description}

To identify a process in a communication environment message passing
libraries make use of identifiers. In MPI they are called ``ranks''
and in PVM ``TaskIDs''.

Often used concepts in message passing are buffering,
blocking/non-blocking communication, load balancing and collective
communication. 

\subsubsection{Buffering}

Communication can be buffered which means that e.g., the sending
process can continue execution and need not wait for the receiving
process signaling it is ready to receive. Otherwise one would have a
synchronous communication. 

\subsubsection{Blocking and Non-blocking Communication}

A communication is blocking if a receiving process has to wait if the
message from the sending process is not available. That means the
receiving process remains idle until the sending process starts
sending. In non-blocking communication the receiving process sends a
request to another process and continues executing. At a later time
the process checks if the message has arrived in the meantime. 

\subsubsection{Load Balancing}

To distribute data as evenly as possible among the processes, load
balancing is a good method. This can be achieved either through data
decomposition (identical programs or functions operate on different
portions of the data) or function decomposition (different programs or
functions perform different operations). For more details on workload
allocation see Section~4.2 in~\cite{geist94pvm}.

\subsubsection{Collective Communication}

In a parallel program we do not want that only one process does most of
the work. This is an issue if it comes to sending of data. Assume that
all of the data is available to one process (A) and it wants to send it to
all of the other. The last receiving process remains idle until
process A has sent data to all of the other processes. This can be
avoided when involving more than one process in sending of data. This
is called a broadcast.
A broadcast is a collective communication in which a single process
sends the same data to every other process. Depending on the topology
of the system there is an optimized broadcast available
(e.g., tree-structured communication). See Chapter~5
in~\cite{pacheco97mpi} for more details on collective communication in
MPI.

\subsection{The Message Passing Interface (MPI)}
\label{sec:MPI}

In 1994 the Message Passing Interface Forum (MPIF) has defined a set
of library interface standards for message passing. Over 40
organizations participated in the discussion which started in
1993. The aim was to develop a widely accepted standard for writing
message passing programs. The standard was called the Message Passing
Interface (see~\cite{forum94:MPI}).

As mentioned in Section~\ref{sec:programming_models} there
were lots of different programming models and languages for high
performance computing available. With MPI a standard was established
that should be practical, portable, efficient and flexible. After
releasing Version 1.0 in 1994 the MPIF continued to correct errors and
made clarifications in the MPI document so that in June 1995 Version
1.1 of MPI was released. Further corrections and clarifications have
been made to version 1.2 of 
MPI and with Version 2 completely new types of functionality were
added (see~\cite{forum94:MPI-2} for details).

MPI is now a library providing higher level
routines and abstractions built upon lower level message passing
routines which can be called from various programming
languages like C or Fortran. It contains all the infrastructure for
inter-process communication. Last but not least an interface wrapper
is available to R. MPI communication can be set up using the R extension
\pkg{Rmpi} (see Section~\ref{sec:Rmpi} for details).


Compared to other parallel programming MPI has the following
advantages and disadvantages:

\subsubsection{Advantages}
\begin{itemize}
\item portability through clearly defined sets of routines---a lot of
  implementations are available for many different platforms
\item better performance compared to PVM as MPI's implementations are
  adapted to the underlying architecture.
\item interoperability within the same implementation of MPI
\item dynamic process creation since MPI-1.2
\item good scalability
\end{itemize} 

\subsubsection{Disadvantages}
\begin{itemize}
\item MPI is not a self contained software package
\item development of parallel programs is difficult
\item no resource management like PVM 
\item heterogeneity only within an architecture
\end{itemize}


For a detailed comparison of PVM and MPI see~\cite{geist96pam}
and~\cite{gropp02ggd}.

\subsubsection{Implementations of the MPI Standard}
\label{sec:mpi-implementations}

As MPI is only a standard, specific implementations are needed to use
message passing routines as library calls in programming languages
like C or FORTRAN. There has been huge effort to provide good
implementations which are widely used. This is because of the fact that
the primary goal was portability. A lot of different platforms are now
supported. What follows is a description of commonly used and well
known MPI implementations.

\begin{description}
\item[LAM/MPI] is an open source implementation of MPI. It runs on a
  network of UNIX machines connected via a local area network or via
  the Internet. On each node runs a UNIX daemon which is a
  micro-kernel plus a few system processes responsible for network
  communication among other things. The micro-kernel is
  responsible for the communication between local processes
  (see~\cite{burns94lam}). The LAM/MPI implementation includes all of
  the MPI-1.2 but not everything of the MPI-2 standard. LAM/MPI is
  available at~\url{http://www.lam-mpi.org/}.
\item[MPICH] is a freely available complete implementation of the MPI
  specification (MPICH1 includes all MPI-1.2 and MPICH2 all MPI-2
  specifications). The main goal was to deliver high performance with
  respect to portability. ``CH'' in MPICH stands for ``Chameleon''
  which means adaptability to one's environment and high performance
  as chameleons are fast (see~\cite{gropp96mpich}). MPICH had been
  developed during the discussion process of the MPIF and had immediately been
  available to the community after the standard had been
  confirmed. There are many other implementations of MPI based on MPICH
  available because of its high and easy portability. Sources and
  Windows binaries are available
  from~\url{http://www-unix.mcs.anl.gov/mpi/mpich/}.
\item[Open MPI] has been built upon three MPI Implementations like
  LAM/MPI among the others. It is a new open-source MPI-2
  Implementation. The goal of Open MPI is to implement the full
  MPI-1.2 and MPI-2 specifications focusing on production-quality and
  performance (see~\cite{gabriel04:_open_mpi}). Open MPI can be
  downloaded from~\url{http://www.open-mpi.org/}.
\end{description}

For setting up these implementations in Debian GNU Linux and cluster
job scripts see Appendix~\ref{app:mpi_imp}. 

\subsection{Parallel Virtual Machine (PVM)}
\label{sec:PVM}

The PVM system is another implementation of a functionally
complete message passing model. It is designed to link computing
resources for example in a heterogenous network and to provide
developers with a parallel platform. As the message passing model
seemed to be the paradigm of choice in the late 80's the PVM project
was created in 1989 to allow developers to exploit distributed
computing across a wide variety of computer types. A key concept in
PVM is that it makes a collection of computers appear as one large
\textit{virtual} machine, hence its name~(\cite{geist94pvm}). The PVM
system can be found on~\url{http://www.csm.ornl.gov/pvm/}. How to
setup this system on a Linux machine and how to start a PVM job on a
cluster is shown in Appendix~\ref{app:mpi_imp}.


PVM has like other message passing implementations its advantages and
disadvantages:

\subsubsection{Advantages}
\begin{itemize}
\item virtual machine concept
\item good support of heterogeneous networks of workstations
\item high portability---available for many different platforms
\item resource management, load balancing and process control
\item self-contained software package
\item dynamic process creation
\item support for fault tolerant programming
\item good scalability
\end{itemize}

\subsubsection{Disadvantages}
\begin{itemize}
\item more overhead because of portability
\item development of parallel programs is difficult
\item not as flexible as MPI as PVM is a specific implementation
\item not as much message passing functionality as MPI has
\end{itemize}

\subsection{OpenMP}
\label{sec:OpenMP}

A completely different approach in the parallel programming model
context is the use of shared memory. Message passing is built upon
this shared memory model which means that every processor has direct
access to the memory of every other processor in the system
(see Section~\ref{sec:sharedmemorysystems}). This class of
multiprocessor is also called Scalable Shared Memory Multiprocessor
(SSMP).

Like the development of the MPI specifications the development
of OpenMP started for one simple reason: portability. Prior to the
introduction of OpenMP as an industry standard every vendor of shared
memory systems created its own proprietary extension for
developers. Therefore a lot of people interested in parallel
programming used portable message passing models like MPI (see
Section~\ref{sec:MPI}) or PVM (see Section~\ref{sec:PVM}). As there
was an increasing demand for a simple and scalable programming model and
the desire to begin parallelizing existing code without having to
completely rewrite it, the OpenMP Architecture Review Board released a
set of compiler directives and callable runtime library routines
known as the OpenMP API (see~\cite{openMP05}). This directives and
routines extend the
programming languages FORTRAN and C or C++ respectively to express
shared memory parallelism.

Furthermore this programming model is based on the fork/join execution
model which makes it easy to get parallelism out of a sequential
program. Therefore unlike in message passing the program or algorithm
need not to be completely decomposed for parallel execution. Given a
working sequential program it is not difficult to incrementally
parallelize individual loops and thereby realize a performance gain in
a multiprocessor system~(\cite{dagum1997opi}). The specification and
further information can be found on the following website:
\url{http://www.openmp.org/}.


OpenMP has a major advantage---it is easy to use. Further advantages
and disadvantages are:

\subsubsection{Advantages}

\begin{itemize}
\item writing parallel programs with OpenMP is easy---incrementally
  parallelizing of sequential code is possible
\item dynamic creation of threads
\item unified code---if OpenMP is not available directives are
  treated as comments 
\end{itemize}

\subsubsection{Disadvantages}

\begin{itemize}
\item the use of OpenMP depends on the availability of a corresponding
  compiler
\item a shared memory platform is needed for efficient programs
\item a lot more overhead compared to message passing functions because
  of implicit programming
\item scalability is limited by the architecture
\end{itemize}

\section{Design of Parallel Programs}
\label{sec:designofpar}
In this section design issues when dealing with parallel programming
are presented.

First, the granularity of parallel programs plays an important role when
designing a parallel application although the grain size is to some
extent limited by the architecture. It is a classification of the
numbers of instructions performed in parallel.

Second, a fundamental differentiation of parallel programs
(particularly on MIMD platforms) is explained. Programs are
categorized based on the organization of the computing tasks.

Eventually, this section concludes with a differentiation of how
parallelism can be achieved. It is a summary of the three fundamental
approaches of writing parallel programs.

\subsection{Granularity of Parallel Programs}

Having the parallel hardware available, the developer has to be aware
of the software like the available compilers and
libraries (parallel programming models) to use for parallel
computations.

What goes hand in hand with the decision for a parallel programming
model on a specific architecture is the granularity of the resulting
parallelism. Granularity is in this case referred to as the
number of instructions performed in parallel. This reaches from
fine-grained (instruction level) parallelism to coarse-grained (task
or procedure level) parallelism:
 
\subsubsection{Levels of Parallelism}
\begin{description}
\item[Instruction level parallelism] single instructions are processed
  concurrently.
\item[Data level parallelism] the same instruction is processed on
  different data at the same time.
\item[Loop level parallelism] a block instruction from a loop can be
  run for each loop iteration in parallel.
\item[Task level parallelism] larger program parts (mostly
  program procedures) are run in parallel.
\end{description}

With choosing a specific grain size the developer has to be aware of
the following tradeoff: a fine-grained program provides considerable
flexibility with respect to load 
balancing, but it also involves 
a high overhead of processing time due to a larger management need and
communication expense (\cite{sevcikova04simulations}).

Furthermore, the available hardware architecture has to be taken
into consideration. Fine-grained parallel programs would not benefit
much in a distributed memory system. Indeed the cost for transferring
data over the network would be too high with respect to the runtime of
the instruction itself. In contrast coarse grained parallelism would
perform better, as communication is held at a minimum as larger
program parts are run in parallel.

\subsection{Organization of Parallel Tasks}

Parallel programming paradigms can be classified according to the
process structure. According to \cite{geist94pvm} parallel programs can be
organized in three different programming approaches:

\begin{description}
\item[Crowd computation] is if a collection of closely related
  processes, typically executing the same code, perform computations
  on different portions of the workload. This paradigm can be further
  subdivided into
  \begin{itemize}
  \item \textit{Master-slave} or host-node model in which one process (the
    master) is responsible for process spawning (this means creation
    of a new process), initialization, collection and display of
    results. Slave programs perform the actual computation involved.
  \item \textit{Node-only} model have no master process but autonomous
    processes from which one takes over the non-computational parts
    in addition to the computation itself.
  \end{itemize}
\item[Tree computations] exhibit a tree-like process control
  structure. It can also be seen as a tree-like parent-child
  relationship.
\item[Hybrid] is a combination of the crowd and the tree model.
\end{description}

Furthermore, parallel programs can be further classified either as the
single program multiple data (SPMD) or multiple program multiple data
paradigm (MPMD). 

\begin{description}
\item[Single program multiple data] means that there is only one
  program available to all computation nodes. Data is distributed
  equally amongst all nodes in the program (\textbf{data
    decomposition}). Then each node applies the
  same program on its own part of the data. One should not confuse
  SPMD with the SIMD paradigm as in the latter case low level
  instructions (directly from the CPU) are applied to previously
  distributed data and therefore is another form of programming.
\item[Multiple program multiple data] is a thread like
  paradigm. On each node it is possible to run different programs with
  either the same or different data (\textbf{function
    decomposition}). Each program forms a separate
  functional unit. This is what one would assume when speaking of MIMD
  architectures but both models (SPMD and MPMD) form the MIMD
  programming paradigm. 
\end{description}

\subsection{Types  of Achieving Parallelism}

Providing parallelism to a program can be achieved with one of the
following approaches. They are different in the complexity of writing
parallel programs.

\begin{description}
\item[Implicit parallelism] is writing a sequential program and using
  an advanced compiler to create parallel executables. The developer
  does not need to know how a program has to be parallelized. This
  involves a lot of overhead as automated parallelization is difficult
  to achieve.
\item[Explicit parallelism with implicit decomposition] is writing a
  sequential program and marking regions for parallel processing. In
  this case the compiler does not need to analyze the code as the
  developer defines which regions are to be parallelized. With this
  type of parallelism not as much overhead as with implicit
  parallelism is involved. This can be achieved via extensions to
  standard programming languages (e.g., OpenMP). 
\item[Explicit parallelism] is writing explicit parallel tasks and
  handling communication between processes
  manually. Overhead is only produced through communication and thus
  offer the possibility of creating very efficient code. This type of
  parallelism is achieved through parallel programming libraries like
  MPI or PVM.
\end{description}


With the information given in this section we now know the design
issues of parallel programs. The next step is to analyze performance
of parallel applications.

\section{Performance Analysis}
\label{sec:perf_analysis}

In high performance computing one of the most important methods of
improving performance is taking advantage of parallelism. As explained
in Section~\ref{sec:processorandmemory} there are other possibilities
to make a program run fast like following the principle of locality
or making use of pipelining. This is somehow architecture
dependent and therefore developers should try to parallelize their
code. A rule of thumb is to make the frequent case fast, this often
involves parallelizing of this case. How these improvements can be
analyzed is discussed in this section.

\subsection{Execution Time and Speedup}
\label{sec:speedup}
The best way to analyze the performance of an application is to
measure the execution time. Consequently an application can be compared
with an improved version through the execution times.
The performance gain can then be expressed as shown in
equation~\ref{eq:speedup}. 

\begin{equation}
\label{eq:speedup}
Speedup = \frac{t_s}{t_e}
\end{equation}
where
\begin{description}
\item[$t_s$] denotes the execution time for a program without
  enhancements (serial version)
\item[$t_e$] denotes the execution time for a program using the
  enhancements (enhanced version)
\end{description}

\subsection{Amdahl's Law}

If we have a program running on a classical von Neumann machine with a
total execution time $t_s$ and the possibility to port this program to
a multiprocessor system with $p$ processors we would expect that the
parallel execution time $t_p$ would be as in equation~\ref{eq:naive_t}
\begin{equation}
\label{eq:naive_t}
t_p = \frac{t_s}{p}
\end{equation}
and consequently the speedup would be equal to the number of
processors $p$.
But this is an ideal scenario. Linear speedup can hardly be achieved
because not every part of a program can be parallelized in the same
way. In general different parts of an application are executed with
different speeds and/or use different resources.

To find an estimate of good quality the program has to be separated
into its different parts which in turn can be subject to
enhancements. \cite{amdahl67vsp} made significant initial work on this
topic.

If we assume that a fraction $f$ of an algorithm can be ideally
parallelized using $p$ processors whereas the remaining fraction $1 -
f$ cannot be 
parallelized the total execution time of the enhanced algorithm would
be as shown in equation~\ref{eq:t_amdahl}.

\begin{equation}
\label{eq:t_amdahl}
t_p = f \frac{t_s}{p} + (1 - f) t_s = \frac{t_s ( f + ( 1 - f) p )}{p}
\end{equation}

\textit{The speedup of an algorithm that results from increasing the speed of
its fraction $f$ is inversely proportional to the size of the fraction
that has to be executed at the slower speed} (interpretation from
Section~1.5 of \cite{kontoghiorghes06handbookpcstat}).

Equation~\ref{eq:amdahl_speedup} shows the corresponding speedup that
can be achieved. If $f$ equals one (the program can be ideally
parallelized) the speedup would equal the amount of processors $p$. If
$f$ is zero (the program cannot be parallelized) the speedup would be
one. 

\begin{equation}
\label{eq:amdahl_speedup}
speedup = \frac{p}{f + (1 - f)p}
\end{equation}

Normally, as $f < 1$ inequality~\ref{eq:amdahl_parallel} is true. It
has became known as \textit{Amdahl's Law for parallel
  computing}. Figure~\ref{fig:amdahl} shows the speedup possible for 1
to 10 processors if
the fraction of parallelizable code is 100\% (linear speedup), 90\%,
75\% and 50\%.

\begin{equation}
\label{eq:amdahl_parallel}
speedup < \frac{1}{1 - f}
\end{equation}

% Figure: Amdahl's law
\input{speedup.tex}

%%\subsection{Gustavson}
%%\cite{gustafson88ras}


\section{Hardware and Software used}
\label{sec:hardwaresoftware}

\subsection{Hardware}
The Hardware used for the applications presented in this thesis is
going to be described in this section.

At the Vienna University of Economics and Business Administration (WU)
a research institute called Research Institute for Computational
Methods (Forschungsinstitut f"ur Rechenintensive Methoden or FIRM for
short---\url{http://www.wu-wien.ac.at/firm}) hosts a high
performance computing server, a cluster of Intel workstations, known
as cluster@WU.
Furthermore, for experimenting and interactive testing an AMD Opteron
server with four computation cores is available at the
Department of Statistics and Mathematics. 

\subsubsection{Cluster@WU}
All programs
have been tested as well as benchmarked on this cluster of
workstations running the resource management system Sun Grid 
Engine~(\cite{gentzsch02sge} and~\cite{sge07}---see also
Appendix~\ref{app:gridengine} for more information on the 
grid engine). With a total of 152 64-bit computation nodes and a total
of 336 gigabytes of RAM, Cluster@WU is by now amongst the fastest
supercomputers in Austria.

The cluster consists of four workstation with four cores each and 16
gigabytes of RAM. They are called ``bignodes'' as they offer more
power to the grid user. The queue for running applications on these
bignodes is called \texttt{bignode.q}. If a shared memory program is
to be run (e.g., an OpenMP program), bignodes are the computers of choice.
The other nodes consist of dual core CPUs and less
memory. They are combined in the queue
\texttt{node.q}. Table~\ref{tab:clusterWU} provides detailed
information about the specs of the queues.

\begin{table}[h!b!p!]
\caption{cluster@WU specification}
\centering
\label{tab:clusterWU}
\begin{tabular}{|l|l|}
\hline
\multicolumn{2}{|c|}{\textbf{bignode.q -- 4 nodes}}\\
\hline
2  & Dual Core Intel XEON CPU 5140 @ 2.33 GHz\\
16 & GB RAM\\
\hline
\multicolumn{2}{|c|}{\textbf{node.q -- 68 nodes}}\\
\hline
1 & Intel Core 2 Duo CPU 6600 @ 2.4 GHz\\
4 & GB RAM\\
\hline
\end{tabular}
\end{table}

\subsubsection{AMD Opteron Server}
The Opteron machine has four cores which
have access to a total of 12 gigabytes of shared memory. The
specification of this machine shows table~\ref{tab:opteronserver}.

\begin{table}[h!b!p!]
\centering
\caption{Opteron server specification}
\label{tab:opteronserver}
\begin{tabular}{|l|l|}
\hline
\multicolumn{2}{|c|}{\textbf{Opteron server -- 1 machine}}\\
\hline
2  & Dual Core AMD Opteron @ 2.4 GHz\\
12 & GB RAM\\
1.2& TB RAID 5 storage\\
\hline
\end{tabular}
\end{table}

\subsection{Software}

The software presented in this section is used for developing and
running code of this thesis. The operating system is Debian GNU
Linux~(\url{http://www.debian.org}).
 
\begin{description}
\item[Sun Grid Engine] is an open source cluster resource management
  and scheduling software. On cluster@WU version 6.0 of the Grid
  Engine manages the remote execution of cluster jobs. It can be obtained
  from~\url{http://gridengine.sunsource.net/}, the commercial version
  Sun N1 Grid Engine can be found
  on~\url{http://www.sun.com/software/gridware/}. See
  Appendix~\ref{app:gridengine} how to use the Grid Engine with
  parallel environments.
\item[Compiler] used in this thesis are the Intel C++ Compiler
  (\cite{icc07}) the Intel Fortran Compiler (\cite{ifort07}) both in
  version 9.1 
  and the GNU Compiler Collection 4.1.2 (\cite{gcc07}). For compiling
  OpenMP code the 
  Intel compiler is being used since the GNU compiler supports these
  parallelizing techniques as of version 4.2 or later.
\item[R] is a free software environment for Statistical Computing and
  Graphics (\cite{Rcore07R}). R-patched 2.5.1 is used in this
  thesis. There is a separate
  chapter about R and high performance computing (Chapter~\ref{chap:Rhpc}).
\item[PVM] version 3.4 is used for running the corresponding programs
  (see Section~\ref{sec:PVM}).
\item[LAM/MPI] version 7.1.3 is used for running MPI programs (see~
  Section~\ref{sec:mpi-implementations}). 
\end{description}

\section{Conclusion}

This chapter provided an overview of the state of the art in parallel
computing. The ongoing development of computer architectures shows that
parallel computing is becoming increasingly important as the newest
commodity processors are already equipped with two to four computation
cores. At the same time in the high performance computing segment a
further substantial increase in parallel computational power is taken
place. An interesting development certainly is grid computing. Sharing
unused resources and on the other hand having access to a tremendous
amount of computational power may be the approach for large-scale
computing in the future. Increasing network bandwidth and a growing
amount of unused computational resources makes this possible.


Tools for creating programs exist for every kind of platform. The most
promising of them is certainly OpenMP which may be the paradigm of
choice for the mainstream. A lack in performance and the limitation to
shared memory platforms compared to message
passing environments has to be considered though. MPI and PVM are
still commonly used as they deliver highest performance on many
different platforms through their portability. A major disadvantage is
the complexity of writing programs using message passing. Years of
experience are needed to write efficient algorithms. Nevertheless, it
remains to be an interesting challenge.

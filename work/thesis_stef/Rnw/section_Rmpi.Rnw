\section{The Rmpi package}
\label{sec:Rmpi}
The Message Passing Interface (MPI) is a set of library interface
standards for message-passing and there are many implementations using
these standards (see also Section~\ref{sec:MPI}).
Rmpi is an interface to MPI (\cite{yu02Rmpi} and \cite{yu06Rmpi}). As
of the time of this writing \pkg{Rmpi} uses
the LAM implementation of MPI. For process spawning the standard
MPI-1.2 is required which is available in the LAM/MPI implementation 
as LAM/MPI (version 7.1.3) supports a large portions of the MPI-2
standard. This is necessary if one likes to use interactive spawning
of R processes. With MPI versions prior to MPI-1.2 separate R
processes have to be created by hand using mpirun (part of many MPI
implementations) for example.

\pkg{Rmpi} contains a lot of low level interface functions to the MPI
C-library available. 
Furthermore, a handful of high level functions are supplied. A
selection of functions is going to be presented in this section sorted
by the following topics:

\begin{itemize}
\item Initialization and Status Queries
\item Process Spawning and Communication
\item Built-in High Level Functions
\item Other Important Functions
\end{itemize}  

A windows implementation of this package (which uses MPICH2)
is also available.

\subsection{Initializing and Status Queries}

The LAM/MPI environment has to be booted prior to using any
message-passing library functions. One possibility is to use the
command line, the other is to load the Rmpi package. It automatically
sets up a (small--1 host) LAM/MPI environment (if the executables are
in the search path). 

When using the Sun Grid Engine (SGE) or other queueing systems to boot
the LAM/MPI parallel environment the developer is not engaged with
setting up and booting the environment anymore (see
appendix \ref{app:gridengine} on how to do this). On a cluster of
workstations this is the method of choice. 


Rmpi management and query functions:

\begin{description}
\item[\texttt{lamhosts()}] finds the hostname associated with its node
  number.
\item[\texttt{mpi.universe.size()}] returns the total number of CPUs
  available to the MPI environment (ie. in a cluster or in a parallel
  environment started by the grid engine).
\item[\texttt{mpi.finalize()}] cleans all MPI states.
\item[\texttt{mpi.exit()}] terminates the mpi communication
  environment and detaches the Rmpi package which makes reloading of
  the package Rmpi in the same session impossible.  
\item[\texttt{mpi.quit()}] terminates the mpi communication
  environment and quits R.  
\end{description}

Example~\ref{ex:Rmpi1}shows a how the configuration of the parallel
environment can be obtained. First it returns the hosts connected to
the parallel environment and then prints the number of CPUs available
in it. After a query if this process is the master process the
hostname where the current process runs on is returned. Finally the
parallel environment is stopped. 

\begin{Example} Simple queries using Rmpi \newline
running on cluster@WU using the node.q -- the parallel environment was
started with the SGE using 8 nodes.

<<echo=TRUE>>=
library("Rmpi")
set.seed(1782)
## Query the configuration of the parallel environment 
lamhosts()

## how many CPU's are available in the parallel environment
mpi.universe.size()
## is this process the master process?
mpi.is.master()
## what is the hostname of the master process
mpi.get.processor.name()

## stop LAM/MPI and exit R 
## mpi.quit()

@ 
\label{ex:Rmpi1}
\end{Example}

\subsection{Process Spawning and Communication}

In Rmpi it is easy to spawn R slaves and use them as workhorses. The
communication between all the involved processes is carried out in a
so called communicator (comm). All processes within the same
communicator are able to send or receive messages from other
processes. The processes are identified through their commrank.

\begin{description}

\item[\texttt{mpi.spawn.Rslaves(Rscript =
    system.file(nslaves =
    mpi.universe.size(), ...)}] spawns \texttt{nslaves} number of R
  workhorses to those hosts automatically chosen by MPI. For other
  arguments represented by \ldots to this function I refer to
  \cite{yu06Rmpi}.
\item[\texttt{mpi.close.Rslaves(dellog = TRUE, comm = 1)}] closes
  previously spawned R slaves and returns 1 if succesful.
\item[\texttt{mpi.comm.size()}] returns the total number of members in
  a communicator.
\item[\texttt{mpi.comm.rank()}] returns the rank (identifier) of the
  process in a communicator.
\item[\texttt{mpi.remote.exec(cmd, ..., comm = 1, ret = TRUE)}]
  executes a command \texttt{cmd} on R slaves with \ldots arguments to
  \texttt{cmd} and return executed results if \texttt{TRUE}.
\end{description}

\begin{Example}

<<echo=TRUE>>=
## spawn as many slaves as CPUs available in the parallel environment
## minus one representing the already running root process.
mpi.spawn.Rslaves(nslaves=mpi.universe.size())
## how many R processes are connected to the communicator?
mpi.comm.size()
## execute mpi.comm.rank() on the slaves
mpi.remote.exec(mpi.comm.rank())
## rank of master process
mpi.comm.rank()
## close Rslaves
mpi.close.Rslaves()

@ 
\label{ex:Rmpi2}
\end{Example}

In example~\ref{ex:Rmpi2} as many slaves are spawned as are available
in the parallel environment. The size of the communicator is returned
(1 master plus the spawned slaves) and a remote query of the commrank
is carried out. Before the slaves are closed the commrank of the
master is printed.
 
\subsection{Built-in High Level Functions}

Rmpi provides the following high level functions (this is only a selection): 

\begin{description}
\item[\texttt{mpi.apply(x, fun, ..., comm = 1)}] applies a function
  \texttt{fun} with additional arguments \ldots to a specific part of
  a vector \texttt{x}. The return value is of type list with the same
  length as of \texttt{x}. The length of
  \texttt{x} must not exceed the 
  number of R slaves spawned as each element of the vector is used
  exactly by one slave. To achieve some sort of load balancing please
  use the corresponding apply functions below.
\item[\texttt{mpi.applyLB(x, fun, ..., comm = 1)}] applies a function
  \texttt{fun} with additional arguments \ldots to a specific part of
  a vector \texttt{x}. There are a few more variants explained in
  \cite{yu06Rmpi}.
\item[\texttt{mpi.bcast.cmd(cmd = NULL, rank = 0, comm = 1)}]
  broadcasts a command \texttt{cmd} from the sender \texttt{rank} to
  all R slaves and evaluates it.
\item[\texttt{mpi.bcast.Robj(obj, rank = 0,comm = 1)}]
  broadcasts an R object \texttt{obj} from process rank \texttt{rank}
  to all other processes (master and slaves).
\item[\texttt{mpi.bcast.Robj2slave(obj, comm = 1)}] broadcasts an R
  object \texttt{obj} to all R slaves from the master process. 
\item[\texttt{mpi.parSim( ... )}] carries out a Monte Carlo simulation
  in parallel. For details on this function see the package manual
  (\cite{yu06Rmpi}) and
  the applications in chapter \ref{chap:options}.
\end{description}


\begin{Example} Using mpi.apply\newline
running on cluster@WU using the node.q -- the parallel environment was
started with SGE using 8 nodes

<<echo=TRUE>>=

n <- 8
# spawn n slaves
mpi.spawn.Rslaves(nslaves = n)
# build up a n x n matrix in parallel
x <- rep(n,n)
rows <- mpi.apply(x, runif)
X <- matrix(unlist(rows), ncol = n, byrow=TRUE)
round(X,3)
mpi.close.Rslaves()

@ 
\label{ex:Rmpi3}
\end{Example}

How to use the high level function \code{mpi.apply()} is shown in
Example~\ref{ex:Rmpi3}. A vector of $n$ random numbers is generated on
each of the $n$ slave and they are returned to the master as a
list (each list element representing one row). Finally a $n$x$n$
matrix is formed and printed. The output of the matrix shows for each
row the same random numbers. This is because of the fact, that each
slave has the same seed. This problem is more specific treated in
Chapter~\ref{chap:options}. For more information about parallel random
number generators see the descriptions of the packages \pkg{rsprng}
and \pkg{rlecuyer} in Section~\ref{sec:otherpackages}.

\subsection{Other Important Functions}

To complete the set of important functions supplied by the Rmpi
package the following functions have to be explained:

\begin{description}
\item[\texttt{.PVM.gather(x, count = length(x), msgtag, group,
    rootginst = 0}] gathers data distributed on the nodes (x) to a
  specific process (mostly the root) into a single array. It performs
  a send of messages from each member of a group of processes. A
  specific process (the root) accumulates this messages into a single vector.
\item[\texttt{.PVM.scatter(x, count, msgtag, group, rootqinst = 0}]
  sends to each member of a group a partition of  a vector x from a
  specified member of the group (mostly the root) where \texttt{count}
  is an integer specifying the number of elements to be sent to each
  member. 
\end{description}

\subsection{Conclusion}

The package \pkg{Rmpi} implements many of the routines available in
MPI-2. But there are some that have to be ommitted or are not included
because they are not needed for use in R (i.e., data management
routines are not necessary as R has its own tools for data handling).
A really interesting aspect of the \pkg{Rmpi} package is the
possibilty to spawn R slaves. That enables the user to interactively
define functions which can be executed remotely on the slaves in
parallel. An example how one can do this is shown in
Chapter~\ref{chap:matrix}, where the implementation of matrix
multiplication is shown. A major disadvantage is that MPI in its
current implementations lack in fault tolerance. This results in a
rather instable execution of MPI applications. Furthermore, debugging
is really difficult as there is no support for it in \pkg{Rmpi}.

All in all this package is a good start in creating parallel programs
as this can easy be achieved entirely in R.  

For further interface functions supplied by the Rmpi package, a more detailed
description and further examples please consult the package description
\cite{yu06Rmpi}.

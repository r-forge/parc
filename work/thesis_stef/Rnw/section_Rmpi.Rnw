\section{The Rmpi package}
\label{sec:Rmpi}
The Message Passing Interface (MPI) is a set of library interface
standards for message-passing and there are many implementations using
these standards (see also section \ref{sec:MPI}).
Rmpi is an interface to MPI (\cite{yu02Rmpi} and \cite{yu06Rmpi}). As
of the time of this writing Rmpi uses
the LAM implementation of MPI. For process spawning the standard
MPI-1.2 is required which is available in the LAM/MPI implementation 
as LAM/MPI (version 7.1.3) supports a large portions of the MPI-2
standard. There are 
a lot of low level interface functions to the MPI C-library available.
Furthermore, a handful of high level functions are supplied by the
Rmpi package. 

A windows implementation of this package (which uses MPICH2)
can also be obtained from

\subsection{Initializing and Status queries}

The LAM/MPI environment has to be booted prior to using any
message-passing library functions. One possibility is to use the
command line, the other is to load the Rmpi package. It automatically
sets up a (small--1 host) LAM/MPI environment (if the executables are
in the search path). 

When using the Sun Grid Engine (SGE) to boot the LAM/MPI parallel
environment the developer is not engaged with
setting up and booting the environment anymore (see
appendix \ref{app:gridengine} on how to do this). On a cluster of
workstations this is the method of choice. 


Rmpi management and query functions:

\begin{description}
\item[\texttt{lamhosts()}] finds the hostname associated with its node
  number.
\item[\texttt{mpi.universe.size()}] returns the total number of CPUs
  available to the MPI environment (ie. in a cluster or in a parallel
  environment started by the grid engine).
\item[\texttt{mpi.finalize()}] cleans all MPI states.
\item[\texttt{mpi.exit()}] terminates the mpi communication
  environment and detaches the Rmpi package which makes reloading of
  the package Rmpi in the same session impossible.  
\item[\texttt{mpi.quit()}] terminates the mpi communication
  environment and quits R.  
\end{description}
  

\textbf{Example:} Simple queries using Rmpi \newline
running on cluster@WU using the node.q -- the parallel environment was
started with the SGE using 8 nodes.

<<echo=TRUE>>=
library("Rmpi")
set.seed(1782)
## Query the configuration of the parallel virtual machine 
lamhosts()

## how many CPU's are available in the parallel environment
mpi.universe.size()
## is this process the master process?
mpi.is.master()
## what is the hostname of the master process
mpi.get.processor.name()

## stop LAM/MPI and exit R 
## mpi.quit()

@ 

\subsection{Process spawning and communication}

In Rmpi it is easy to spawn R slaves and use them as workhorses.

%% TODO: explain here or in MPI section more about comms ranks etc.
%% mpi.send recv bcast etc. explained here

\begin{description}

\item[\texttt{mpi.spawn.Rslaves(Rscript =
    system.file(nslaves =
    mpi.universe.size(), ...)}] spawns \texttt{nslaves} number of R
  workhorses to those hosts automatically chosen by MPI. For other
  arguments represented by \ldots to this function I refer to
  \cite{yu06Rmpi}.
\item[\texttt{mpi.close.Rslaves(dellog = TRUE, comm = 1)}] closes
  previously spawned R slaves and returns 1 if succesful.
\item[\texttt{mpi.comm.size()}] returns the total number of members in
  a communicator.
\item[\texttt{mpi.comm.rank()}] returns the rank (identifier) of the
  process in a communicator.
\item[\texttt{mpi.remote.exec(cmd, ..., comm = 1, ret = TRUE)}]
  executes a command \texttt{cmd} on R slaves with \ldots arguments to
  \texttt{cmd} and return executed results if \texttt{TRUE}.
\end{description}

<<echo=TRUE>>=
## spawn as many slaves as CPUs available in the parallel environment
## minus one representing the already running root process.
mpi.spawn.Rslaves(nslaves=mpi.universe.size()-1)
## how many R processes are connected to the communicator?
mpi.comm.size()
## execute mpi.comm.rank() on the slaves
mpi.remote.exec(mpi.comm.rank())
## rank of master process
mpi.comm.rank()
## close Rslaves
mpi.close.Rslaves()

@ 

\subsection{Built-in high level functions}

Rmpi provides the following high level functions (this is only a selection): 

\begin{description}
\item[\texttt{mpi.apply(x, fun, ..., comm = 1)}] applies a function
  \texttt{fun} with additional arguments \ldots to a specific part of
  a vector \texttt{x}. The return value is of type list with the same
  length as of \texttt{x}. The length of
  \texttt{x} must not exceed the 
  number of R slaves spawned as each element of the vector is used
  exactly by one slave. To achieve some sort of load balancing please
  use the corresponding apply functions below.
\item[\texttt{mpi.applyLB(x, fun, ..., comm = 1)}] applies a function
  \texttt{fun} with additional arguments \ldots to a specific part of
  a vector \texttt{x}. There are a few more variants explained in
  \cite{yu06Rmpi}.
\item[\texttt{mpi.bcast.cmd(cmd = NULL, rank = 0, comm = 1)}]
  broadcasts a command \texttt{cmd} from the sender \texttt{rank} to
  all R slaves and evaluates it.
\item[\texttt{mpi.bcast.Robj(obj, rank = 0,comm = 1)}]
  broadcasts an R object \texttt{obj} from process rank \texttt{rank}
  to all other processes (master and slaves).
\item[\texttt{mpi.bcast.Robj2slave(obj, comm = 1)}] broadcasts an R
  object \texttt{obj} to all R slaves from the master process. 
\item[\texttt{mpi.parSim( ... )}] carries out a Monte Carlo simulation
  in parallel. For details on this function see the package manual
  (\cite{yu06Rmpi}) and
  the applications in chapter \ref{chap:options}.
\end{description}


\textbf{Example:} Using mpi.apply\newline
running on cluster@WU using the node.q -- the parallel environment was
started with SGE using 8 nodes

<<echo=TRUE>>=

n <- 8
# spawn n slaves
mpi.spawn.Rslaves(nslaves = n)
# build up a n x n matrix in parallel
x <- rep(n,n)
rows <- mpi.apply(x, runif)
X <- matrix(unlist(rows), ncol = n, byrow=TRUE)
X
mpi.close.Rslaves()

@ 


\subsection{Other important functions}

To complete the set of important functions supplied by the Rmpi
package the following functions have to be explained:

\begin{description}
\item[\texttt{.PVM.gather(x, count = length(x), msgtag, group,
    rootginst = 0}] gathers data distributed on the nodes (x) to a
  specific process (mostly the root) into a single array. It performs
  a send of messages from each member of a group of processes. A
  specific process (the root) accumulates this messages into a single vector.
\item[\texttt{.PVM.scatter(x, count, msgtag, group, rootqinst = 0}]
  sends to each member of a group a partition of  a vector x from a
  specified member of the group (mostly the root) where \texttt{count}
  is an integer specifying the number of elements to be sent to each
  member. 
\end{description}

\subsection{conclusion}

%The \texttt{PVM.rapply()} example shown in this section followed the Single Program
%Multiple Data (SPMD) paradigm. Data is splitted into different parts
%which are sent to different processes. I/O is handled by a master
%process. When loading rpvm in an R session this session becomes the
%master process. Slaves can easily be spawned provided that there are
%working slave scripts available. A major disadvantage is that the rpvm
%package only has two higher-level function. One of them can be used
%for calculations. That means when using this package for HPC one has
%to deal with low-level message-passing which in turn provides high
%flexibility. New parallel functions can be constructed on the basis of
%the provided interface.

For further interface functions supplied by the Rmpi package, a more detailed
description and further examples please consult the package description
\cite{yu06Rmpi}.

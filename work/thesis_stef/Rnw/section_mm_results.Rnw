%\documentclass[12pt,a4paper]{report}
%\newcommand{\pkg}[1]{{\normalfont\fontseries{b}\selectfont #1}}
%\begin{document}
%\chapter{Test}

%% Initialization
<<echo=FALSE,results=hide>>=

library("paRc")
library("xtable")
load("../examples/Rda/bmres-shared-1000.Rda")
load("../examples/Rda/bmres-shared-2500.Rda")
load("../examples/Rda/bmres-node-1000.Rda")
load("../examples/Rda/bmres-node-2500.Rda")
load("../examples/Rda/bmres-node-5000.Rda")
@ 

\SweaveOpts{echo=FALSE}

\section{Comparison of Parallel Programming Paradigms}

In this section the results of the comparison of parallel programming
models are shown. They are compared on a shared memory machine using
up to 4 cores and on a distrubuted memory platform using up to 20
cluster nodes. On the shared memory machine complexities of $1000 \times 
1000$ and $2500 \times 2500$ are used and on the cluster nodes $2500
\times 2500$ and $5000 \times 5000$.

\subsection{Shared Memory Platform}

On our shared memory testbed the number of available CPUs is restricted to
four. Benchmarks are run on the bignodes of cluster@WU.

\subsubsection{$1000 \times 1000$ Matrices}
\label{sec:mms1000}

Table~\ref{tab:mm_s1000} and Figure~\ref{fig:mm_s1000} show the
resulting execution times of the matrix multiplication with a
complexity $1000 \times 1000$. A slight overhead is observed for the OpenMP
routine from package \pkg{paRc} using one CPU but when increasing the
number of CPU this programming
model scales better than the other two. This is because OpenMP makes
use of shared variables and therefore no communication overhead is
produced. Nevertheless, a slight overhead produced from the compiler
remains. This is what we expected. In this benchmark MPI and PVM show
nearly the same speedup.

<<results=tex>>=
## 1000x1000

## Table without BLAS functions (serial, OpenMP, MPI, PVM)
table_s1000 <- cbind(as.integer(bmres_s1000[5:8,3]),bmres_s1000[5:8,6],bmres_s1000[17:20,6],bmres_s1000[29:32,6])

colnames(table_s1000) <- c("#CPU","OpenMP","MPI","PVM")

xtable(table_s1000, caption="Comparison of parallel programming models on a shared memory machine", label="tab:mm_s1000")
@ 

\begin{figure}
\centering
<<fig=TRUE>>=
## plot without BLAS functions (serial, OpenMP, MPI, PVM)
plot(bmres_s1000[c(1,5:8,17:20,29:32),])
@
\caption{Comparison of parallel programming models.}
\label{fig:mm_s1000}
\end{figure}

When using the BLAS routines (as the supplied matrix multiplication
routines from \pkg{snow} do) the results draw the following picture:
Table~\ref{tab:mm_sblas1000} and Figure~\ref{tab:mm_sblas1000} show
that all of the programming models scale not so good. The communication
overhead is too large for this kind of complexity (the BLAS routines
deliver results fast). Indeed, when using all computation cores the
PVM routine from package \pkg{snow} is slower than with fewer
cores. This is because the distribution of the data consumes
more time with respect to the gain of distributing the computation.
 

<<results=tex>>=
## Table with BLAS
table_sblas1000 <- cbind(as.integer(bmres_s1000[5:8,3]),bmres_s1000[9:12,6],bmres_s1000[13:16,6],bmres_s1000[21:24,6],bmres_s1000[25:28,6])

colnames(table_sblas1000) <- c("#CPU","MPI","snow-MPI","PVM","snow-PVM")

xtable(table_sblas1000, caption="Comparison of parallel programming models on a shared memory machine using BLAS routines", label="tab:mm_sblas1000")

@ 

\begin{figure}
\centering
<<fig=TRUE>>=
## Plot with BLAS
plot(bmres_s1000[c(2,9:12,13:16,21:24,25:28),])
@ 
\caption{Comparison of parallel programming models using BLAS routines.}
\label{fig:mm_sblas1000}
\end{figure}


%

\subsubsection{$2500 \times 2500$ Matrices}

Let the complexity be increased to $2500 \times 2500$. Then, in contrary to
\ref{sec:mms1000} the OpenMP routine performs worse than the other
two. This is because the communication doesn't play an important role
any more when carrying out this benchmark. The compiler driven
parallelization produces a slight overhead in contrast to the manual
parallelization. The MPI and PVM routines scale nearly in the same
way.

<<results=tex>>=
## 2500x2500

## Table without BLAS functions (serial, OpenMP, MPI, PVM)
table_s2500 <- cbind(as.integer(bmres_s2500[5:8,3]),bmres_s2500[5:8,6],bmres_s2500[25:28,6],bmres_s2500[21:24,6])
colnames(table_s2500) <- c("#CPU","OpenMP","MPI","PVM")

xtable(table_s2500, caption="Comparison of parallel programming models on a shared memory machine.", label="tab:mm_s2500")
@ 

\begin{figure}
\centering
<<fig=TRUE>>=
## plot without BLAS functions (serial, OpenMP, MPI, PVM)
plot(bmres_s2500[c(1,5:8,21:24,25:28),])
@ 
\caption{Comparison of parallel programming models.}
\label{fig:mm_s2500}
\end{figure}

The last comparison of parallel programming models on a shared memory
machine is carried out using the BLAS routines. As we have seen in the
corresponding example from Section~\ref{sec:mms1000} the speedup is
not expected to be high. Figure~\ref{fig:mm_sblas2500} shows that when
increasing the number of cores from one to two a significant speedup
can be obtained. Using more than two processors does not improve
performance as the lines in the graph become rather flat. Again, the
PVM routine from \pkg{snow} performs worst.

<<results=tex>>=
## Table with BLAS
table_sblas2500 <- cbind(as.integer(bmres_s2500[5:8,3]),bmres_s2500[9:12,6],bmres_s2500[29:32,6],bmres_s2500[17:20,6],bmres_s2500[13:16,6])

colnames(table_sblas2500) <- c("#CPU","MPI","snow-MPI","PVM","snow-PVM")
xtable(table_sblas2500, caption="Comparison of parallel programming models on a shared memory machine with BLAS routines.", label="tab:mm_sblas2500")
@ 

\begin{figure}
\centering
<<fig=TRUE>>=
## Plot with BLAS
plot(bmres_s2500[c(2,9:12,13:16,17:20,29:32),])

@
\caption{Comparison of parallel programming models.}
\label{fig:mm_sblas2500}
\end{figure}


\subsection{Distributed Memory Platform}

On a distributed memory platform things change. First more CPU's and
more memory are available to the user. Second the processors are
connected through a slower interconnection network compared to a
shared memory machine. This has an impact to all communication carried
out in message passing environment. As we have more nodes available we
decided to use bigger matrices and therefore start with $2500 \times
2500$ and continue with $5000 \times 5000$ matrices.

\subsubsection{$2500 \times 2500$ Matrices}

Without using the BLAS routines the MPI routine from package
\pkg{paRc} scales rather well (Figure~\ref{fig:mm_n2500}). The PVM
routine shows good speedup 
until processor number 6 but then performance gain breaks down. Again,
PVM communication seems to be suboptimal implemented.

\begin{figure}
\centering
<<fig=TRUE>>=
## 2500x2500
## plot without BLAS functions (serial, OpenMP, MPI, PVM)
plot(bmres_n2500[c(1,5:6,47:66,107:126),])
@ 
\caption{Comparison of parallel programming models on a cluster using
  20 nodes.}
\label{fig:mm_n2500}
\end{figure}

Figure~\ref{fig:mm_nblas2500} is interesting. Only the MPI routine
from \pkg{paRc} seems to deliver good results. The PVM and the \pkg{snow}
routines deliver lower performance with increasing CPU counts after
adding the 4th or 5th CPU. Furthermore the \pkg{snow} PVM routine
produced 2 outliers. It does not seem to deliver a stable performance.
  
\begin{figure}
\centering
<<fig=TRUE>>=
## Plot with BLAS
plot(bmres_n2500[c(2,7:26,27:46,67:86,87:106),])
@ 
\caption{Comparison of parallel programming models on a cluster using
  20 nodes using BLAS routines.}
\label{fig:mm_nblas2500}
\end{figure}

Table~\ref{tab:mm_seqnblas2500} shows the results of the matrix
multiplication benchmark using the Basic Linear Algebra Subroutines
from different Libraries. These Libraries are trimmed for the corresponding
architecture on the nodes so that performance is maximized. 

<<results=tex>>=
## BLAS Table incl speedup
table_seqnblas2500 <- cbind(bmres_n2500[1:4,c(2,6)],speedup(bmres_n2500[1:4,6]))
colnames(table_seqnblas2500) <- c("Type","Time","Speedup")
xtable(table_seqnblas2500, caption="Comparison of BLAS routines on a cluster node.", label="tab:mm_seqnblas2500")
@ 

\subsubsection{$5000 \times 5000$ Matrices}

The final task in this section is to multiply two $5000 \times 5000$
matrices on the cluster nodes. Communication now really matters as one
matrix needs around 190~MB of storage when assuming that an entry is a
double precision floating point of 8 Byte.

Figure~\ref{fig:mm_n5000} shows that the PVM routine has once more
problems with large CPU counts. MPI is rather
stable. Figure~\ref{fig:mm_n5000} shows the routines using the R BLAS
library. Again PVM is unstable, the \pkg{snow} routines do not scale
well and MPI delivers stable and increasing performance.

\begin{figure}
\centering
<<fig=TRUE>>=
## 5000x5000
## plot without BLAS functions (serial, OpenMP, MPI, PVM)
plot(bmres_n5000[c(1,5:6,47:66,107:126),])
@ 
\caption{Comparison of parallel programming models on a cluster using
  20 nodes.}
\label{fig:mm_n5000}
\end{figure}

\begin{figure}
\centering
<<fig=TRUE>>=
## Plot with BLAS
plot(bmres_n5000[c(2,7:26,27:46,67:86,87:106),])
@
\caption{Comparison of parallel programming paradigms.}
\label{fig:mm_nblas5000}
\end{figure}


%\end{document}

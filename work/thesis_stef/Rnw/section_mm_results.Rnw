%\documentclass[12pt,a4paper]{report}
%\newcommand{\pkg}[1]{{\normalfont\fontseries{b}\selectfont #1}}
%\begin{document}
%\chapter{Test}

%% Initialization
<<echo=FALSE,results=hide>>=

library("paRc")
library("xtable")
load("../examples/Rda/bmres-shared-1000.Rda")
load("../examples/Rda/bmres-shared-2500.Rda")
load("../examples/Rda/bmres-node-1000.Rda")
load("../examples/Rda/bmres-node-2500.Rda")
load("../examples/Rda/bmres-node-5000.Rda")
@ 

\SweaveOpts{echo=FALSE}

\section{Comparison of Parallel Routines}

In this section the results of the comparison of parallel programming
models are shown. They are compared on a shared memory machine using
up to 4 cores and on a distributed memory platform using up to 20
cluster nodes. On the shared memory machine matrices with dimensions
of $1000 \times 1000$ (complexity grade $1000$) and $2500 \times 2500$
(complexity grade $2500$) are used and on the
cluster nodes matrices with dimensions $2500 \times 2500$ and $5000
\times 5000$ (complexity grade $5000$).

\subsection{Shared Memory Platform}
\label{sec:res_shared}
On our shared memory testbed the number of available CPUs is restricted to
four. Benchmarks are run on the bignodes of cluster@WU.

\subsubsection{Results of Complexity Grade $1000$}
\label{sec:mms1000}

Table~\ref{tab:mm_s1000} and Figure~\ref{fig:mm_s1000} show the
resulting execution times of the matrix multiplication with complexity
grade $1000$. A slight overhead is observed for the OpenMP
routine from package \pkg{paRc} using one CPU but when increasing the
number of CPU this programming
model scales better than the other two. This is because OpenMP makes
use of shared variables and therefore not as much communication overhead is
produced as when using message passing---computation itself does not
matter in relation to communication. Nevertheless, a slight overhead
produced from the compiler 
remains. This is what we expected. In this benchmark MPI and PVM show
nearly the same speedup.

<<results=tex>>=
## 1000x1000

## Table without BLAS functions (serial, OpenMP, MPI, PVM)
table_s1000 <- cbind(as.integer(bmres_s1000[5:8,3]),bmres_s1000[5:8,6],bmres_s1000[17:20,6],bmres_s1000[29:32,6])

colnames(table_s1000) <- c("#CPU","OpenMP","MPI","PVM")

xtable(table_s1000, caption="Comparison of parallel programming models on a shared memory machine and complexity grade $1000$", label="tab:mm_s1000")
@ 

\begin{figure}
\centering
<<fig=TRUE>>=
## plot without BLAS functions (serial, OpenMP, MPI, PVM)
plot(bmres_s1000[c(1,5:8,17:20,29:32),])
@
\caption{Comparison of parallel programming models on a shared memory
  machine and complexity grade $1000$.}
\label{fig:mm_s1000}
\end{figure}

When using BLAS (as the supplied matrix multiplication
routines from \pkg{snow} do) the results draw the following picture:
Table~\ref{tab:mm_sblas1000} and Figure~\ref{tab:mm_sblas1000} show
that all of the programming models scale not so well. The communication
overhead is too large for this kind of complexity grade (the BLAS routines
deliver results fast). Indeed, when using all computation cores the
PVM routine from package \pkg{snow} is slower than with fewer
cores. This is because the distribution of the data consumes
more time with respect to the gain of distributing the computation.
 

<<results=tex>>=
## Table with BLAS
table_sblas1000 <- cbind(as.integer(bmres_s1000[5:8,3]),bmres_s1000[9:12,6],bmres_s1000[13:16,6],bmres_s1000[21:24,6],bmres_s1000[25:28,6])

colnames(table_sblas1000) <- c("#CPU","MPI","snow-MPI","PVM","snow-PVM")

xtable(table_sblas1000, caption="Comparison of parallel programming models on a shared memory machine using BLAS and complexity grade $1000$", label="tab:mm_sblas1000")

@ 

\begin{figure}
\centering
<<fig=TRUE>>=
## Plot with BLAS
plot(bmres_s1000[c(2,9:12,13:16,21:24,25:28),])
@ 
\caption{Comparison of parallel programming models on a shared memory
  machine using BLAS and complexity grade $1000$}
\label{fig:mm_sblas1000}
\end{figure}


%

\subsubsection{Results of Complexity Grade $2500$}

Let the complexity be increased to $2500 \times 2500$ matrices. Then,
in contrary to 
\ref{sec:mms1000} Figure~\ref{fig:mm_s2500} shows that the OpenMP
routine performs worse than the other 
two. This is because the communication does not play an important role
any more when carrying out this benchmark. The compiler driven
parallelization produces a slight overhead in contrast to the manual
parallelization. The MPI and PVM routines scale nearly in the same
way (see also Table~\ref{tab:mm_s2500}.

<<results=tex>>=
## 2500x2500

## Table without BLAS functions (serial, OpenMP, MPI, PVM)
table_s2500 <- cbind(as.integer(bmres_s2500[5:8,3]),bmres_s2500[5:8,6],bmres_s2500[25:28,6],bmres_s2500[21:24,6])
colnames(table_s2500) <- c("#CPU","OpenMP","MPI","PVM")

xtable(table_s2500, caption="Comparison of parallel programming models on a shared memory machine and complexity grade $2500$.", label="tab:mm_s2500")
@ 


\begin{figure}
\centering
<<fig=TRUE>>=
## plot without BLAS functions (serial, OpenMP, MPI, PVM)
plot(bmres_s2500[c(1,5:8,21:24,25:28),])
@ 
\caption{Comparison of parallel programming models on a shared memory
  machine and complexity grade $2500$.}
\label{fig:mm_s2500}
\end{figure}

The last comparison of parallel programming models on a shared memory
machine shows parallel matrix multiplication using BLAS. As we have
seen in the 
corresponding example from Section~\ref{sec:mms1000} the speedup is
not expected to be high. Figure~\ref{fig:mm_sblas2500} shows that when
increasing the number of cores from one to two a significant speedup
can be obtained. Using more than two processors does not improve
performance as the lines in the graph become rather flat. Again, the
PVM routine from \pkg{snow} performs worst. The execution times and
speedups are shown in Table~\ref{tab:mm_sblas2500}.

<<results=tex>>=
## Table with BLAS
table_sblas2500 <- cbind(as.integer(bmres_s2500[5:8,3]),bmres_s2500[9:12,6],bmres_s2500[29:32,6],bmres_s2500[17:20,6],bmres_s2500[13:16,6])

colnames(table_sblas2500) <- c("#CPU","MPI","snow-MPI","PVM","snow-PVM")
xtable(table_sblas2500, caption="Comparison of parallel programming models on a shared memory machine with BLAS and complexity grade $2500$.", label="tab:mm_sblas2500")
@ 

\begin{figure}
\centering
<<fig=TRUE>>=
## Plot with BLAS
plot(bmres_s2500[c(2,9:12,13:16,17:20,29:32),])

@
\caption{Comparison of parallel programming models on a shared memory
  machine with BLAS and complexity grade $2500$.}
\label{fig:mm_sblas2500}
\end{figure}


\subsection{Distributed Memory Platform}

On a distributed memory platform things change. First more CPUs and
more memory are available to the user. Second the processors are
connected through a slower interconnection network compared to a
shared memory machine. This has an influence to all communication carried
out in the message passing environment. As we have more nodes available we
decided to use bigger matrices and therefore start with $2500 \times
2500$ and continue with $5000 \times 5000$ matrices.

\subsubsection{Results of Complexity Grade $2500$}

Without using BLAS the MPI routine from package
\pkg{paRc} scales rather well (Figure~\ref{fig:mm_n2500}). The PVM
routine shows good speedup 
until processor number 6 but then performance gain breaks down. Again,
PVM communication seems to be suboptimal implemented.

\begin{figure}
\centering
<<fig=TRUE>>=
## 2500x2500
## plot without BLAS functions (serial, OpenMP, MPI, PVM)
plot(bmres_n2500[c(1,5:6,47:66,107:126),])
@ 
\caption{Comparison of parallel programming models on a cluster using
  20 nodes and complexity grade $2500$.}
\label{fig:mm_n2500}
\end{figure}

Figure~\ref{fig:mm_nblas2500} is interesting. Only the MPI routine
from \pkg{paRc} seems to deliver good results. The PVM and the \pkg{snow}
routines deliver lower performance with increasing CPU counts after
adding the 4th or 5th CPU. Furthermore the \pkg{snow} PVM routine
produced 2 outliers. It does not seem to deliver a stable performance.
  
\begin{figure}
\centering
<<fig=TRUE>>=
## Plot with BLAS
plot(bmres_n2500[c(2,7:26,27:46,67:86,87:106),])
@ 
\caption{Comparison of parallel programming models on a cluster using
  20 nodes using BLAS and complexity grade $2500$.}
\label{fig:mm_nblas2500}
\end{figure}


%% explain in more details the  speedup tables
To sum up Table~\ref{tab:mm_n2500} shows the best results of the matrix
multiplication benchmark for each parallel programming model. It can
be seen that BLAS are highly optimized for the corresponding platform
and can hardly be beaten. Looking at the message passing models, the
MPI routines perform better with respect to the highest speedup which
can be achieved than the PVM ones.

<<results=tex>>=
X<-NULL
for(i in unique(bmres_n2500$type)){X<-rbind(X,bmres_n2500[bmres_n2500$type==i,][bmres_n2500[bmres_n2500$type==i,]$time_ela==min(bmres_n2500[bmres_n2500$type==i,]$time_ela),])}
table_n2500 <- cbind(X[,c(3,2,6)],speedup(X))
colnames(table_n2500) <- c("#CPU","Type","Time","Speedup")
xtable(table_n2500, caption="Comparison of parallel programming models on a cluster using 20 nodes and complexity grade $2500$.", label="tab:mm_n2500")
@ 



\subsubsection{Results of Complexity Grade $5000$}

The final task in this section is to multiply two $5000 \times 5000$
matrices on the cluster nodes. Communication now really matters as one
matrix needs around 190~MB of memory when assuming that an entry is a
double precision floating point of 8 Byte.

Figure~\ref{fig:mm_n5000} shows that the PVM routine has once more
problems with large CPU counts. MPI is rather
stable. Figure~\ref{fig:mm_nblas5000} shows the routines using the R BLAS
library. Again PVM is unstable, the \pkg{snow} routines do not scale
well and MPI delivers stable and increasing performance.

\begin{figure}
\centering
<<fig=TRUE>>=
## 5000x5000
## plot without BLAS functions (serial, OpenMP, MPI, PVM)
plot(bmres_n5000[c(1,5:6,47:66,107:126),])
@ 
\caption{Comparison of parallel programming models on a cluster using
  20 nodes and complexity grade $5000$.}
\label{fig:mm_n5000}
\end{figure}

\begin{figure}
\centering
<<fig=TRUE>>=
## Plot with BLAS
plot(bmres_n5000[c(2,7:26,27:46,67:86,87:106),])
@
\caption{Comparison of parallel programming paradigms on a cluster using
  20 nodes with BLAS and complexity grade $5000$.}
\label{fig:mm_nblas5000}
\end{figure}

<<results=tex>>=
X<-NULL
for(i in unique(bmres_n5000$type)){X<-rbind(X,bmres_n5000[bmres_n5000$type==i,][bmres_n5000[bmres_n5000$type==i,]$time_ela==min(bmres_n5000[bmres_n5000$type==i,]$time_ela),])}
table_n5000 <- cbind(X[,c(3,2,6)],speedup(X))
colnames(table_n5000) <- c("#CPU","Type","Time","Speedup")
xtable(table_n5000, caption="Comparison of parallel programming models on a cluster using 20 nodes and complexity grade $5000$.", label="tab:mm_n5000")
@ 

%% explain in more details the  speedup tables
Finally, Table~\ref{tab:mm_n2500} shows the best results of the matrix
multiplication benchmark for each parallel programming model with
complexity grade $5000$. It can
be seen that BLAS achieve again the best performance for the
corresponding platform. In fact, comparing the best speedups which can
be achieved for each programming model draws the same picture as in
Table~\ref{tab:mm_n2500}: the MPI routines perform better than the PVM
routines. The best message passing routine is the implementation in
package \pkg{paRc}.


%\end{document}

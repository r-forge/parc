\section{The snow Package}
\label{sec:snow}
The aim of simple network of workstations
(\pkg{snow}---\cite{rossini03snow}, \cite{tierney07snow}) is to
provide a simple parallel computing environment in R. To make a
collection of computers to appear as a virtual cluster in R 
three different message passing environments can be used:

\begin{itemize}
\item PVM via R package \pkg{rpvm} (see section \ref{sec:rpvm})
\item MPI via R package \pkg{Rmpi} (see section \ref{sec:Rmpi})
\item SOCK via TCP sockets
\end{itemize}

The details of the mechanism used and how it implemented are hidden
from the high level user. As the name suggests it should be simple to
use.

After setting up a virtual cluster developing parallel R functions
can be achieved via a standardized interface to the computation
nodes.

Moreover, when using \pkg{snow} one can rely on a good handful of
high level functions. This makes it rather easy to use the underlying
parallel computational engine.
Indeed \pkg{snow} uses existing interfaces to R namely \pkg{Rmpi} when
using MPI (see Section~\ref{sec:Rmpi}), \pkg{rpvm} when using PVM (see
Section~\ref{sec:rpvm}) and a new possibility of message passing namely
TCP sockets, which is a rather simple way of achieving communication
between nodes (in most applications this is not the optimal way).
What follows is a description of high level functions supplied by the
\pkg{snow} package. They are assigned to one of the topics:

\begin{itemize}
\item Initialization
\item Built-in High Level Functions
\item Fault Tolerance
\end{itemize}

\subsection{Initialization}

Initializing a \pkg{snow} cluster is rather easy if the system is
prepared accordingly. When using MPI (achieved through \pkg{Rmpi}) a
LAM/MPI environment has to be booted prior starting the virtual
cluster (see Section~\ref{sec:Rmpi}). Is PVM the method of choice the
\pkg{rpvm} package must be available and an appropriate PVM has to be
started (see Section~\ref{sec:rpvm}). For both MPI and PVM the
parallel environment 
can be configured through a grid engine (see Appendix
\ref{app:gridengine}). TCP sockets can be set up directly using the
package. MPI or PVM offer the possibility to query the status of the
parallel environment. This can be done using the functions supplied
from the corresponding package.

\subsubsection{Management Functions}
\begin{description}
\item[\code{makecluster(spec, type = getClusterOption("type"))}]
  starts a cluster of type \code{type} with \code{spec} numbers of
  slaves. If the cluster is of connection type SOCK then \code{spec}
  must be a character vector containing the hostnames of the
  slavenodes to join the cluster. The return value is a list
  containing the cluster specifications. This object is necessary in
  further function calls.
\item[\code{stopCluster(cl)}] stops a cluster specified in \code{cl}.
\end{description}

Example~\ref{ex:snowstartstop} shows how a virtual cluster can be
created using MPI. 

\begin{Example} Start/stop cluster in \pkg{snow}
<<echo=TRUE>>=
library("snow")
set.seed(1782)
## Start an MPI cluster using 8 slaves
n <- 8
cl <- makeCluster(n, type = "MPI")
## Stop cluster
stopCluster(cl)
@ 
\label{ex:snowstartstop}
\end{Example}

\subsection{Built-in High Level Functions}

\pkg{snow} provides a good handful of high-level functions. They can
be used as building blocks for further high level routines.

\subsubsection{High Level Functions}
\begin{description}
\item[\code{clusterEvalQ(cl, expr)}] evaluates an R expression
  \code{expr} on
  each cluster node provided by \code{cl}. 
\item[\code{clusterCall(cl, fun, ...)}] calls a function
  \code{fun} with arguments \ldots{} on each node found in \code{cl}
  and returns a list of the results.
\item[\code{clusterApply(cl, x, fun, ...)}] applies a function
  \code{fun} with additional arguments \ldots{} to a specific part of
  a vector \code{x}. The return value is of type list with the same
  length as of \code{x}. The length of
  \code{x} must not exceed the 
  number of R slaves spawned as each element of the vector is used
  exactly by one slave. To achieve some sort of load balancing please
  use the corresponding apply functions below.
\item[\code{clusterApplyLB(cl, x, fun, ...)}] is a load balancing
  version of \\ \code{clusterApply()} which applies a function
  \code{fun} with additional arguments \ldots{} to a specific part of
  a vector \code{x} with the difference that the length of
  \code{x} can exceed the number of cluster nodes. If a node
  finished with the computation the next job is placed on the
  available node. This is repeated until all jobs have completed.
\item[\code{clusterExport(cl, list)}] broadcasts a list of global
  variables on the master (\code{list}) to all slaves.
\item[\code{parApply(cl, x, fun, ...)}] is one of the parallel
  versions of the \code{apply} functions available in R. We refer to
  the package documentation (\cite{tierney07snow}) for further details.
\item[\code{parMM(cl, A,B)}] is a simple parallel implementation of
  matrix multiplication. 
\end{description}

Example~\ref{ex:snowapply} shows the use of \code{clusterApply()} on a
virtual cluster of 8 nodes using MPI as communication layer. Like in
Example~\ref{ex:Rmpi3} a vector of $n$ random numbers is generated on
each of the $n$ slaves and are returned to the master as a
list (each list element representing one row). Finally an $n  \times n$
matrix is formed and printed. The output of the matrix shows again for
each row the same random numbers. This is because of the fact, that
each slave has the same seed. This problem is treated more specifically in
Chapter~\ref{chap:options}. For more information about parallel random
number generators see the descriptions of the packages \pkg{rsprng}
and \pkg{rlecuyer} in Section~\ref{sec:otherpackages}. 

\begin{Example} Using high level functions of snow
<<echo=TRUE>>=
n <- 8
# spawn n slaves
cl <- makeCluster(n, type="MPI")
# build up a n x n matrix in parallel
x <- rep(n,n)
rows <- clusterApply(cl, x, runif)
X <- matrix(unlist(rows), ncol = n, byrow=TRUE)
round(X,3)
@ 
\label{ex:snowapply}
\end{Example}

\subsection{Fault Tolerance}

Providing fault tolerance, computational reproducibility and dynamic
adjustment of the cluster configuration is required for practical
real-world parallel applications according to
\cite{sevcikova04pragmatic}. Failure detection and recovery, dynamic
cluster resizing and the possibility to obtain intermediate results of
a parallel computation is implemented in the package \pkg{snowFT}
(\cite{sevcikova04snowFT}). A detailed introduction to fault tolerance
in statistical simulations can be found in
\cite{sevcikova04simulations}. As we pointed out in
Section~\ref{sec:MPI} and Section~\ref{sec:PVM} MPI does not provide
tools for implementing fault tolerance and therefore package
\pkg{rpvm} is required for \pkg{snowFT}. For more details on the fault
tolerant version of \pkg{snow} we refer to the package documentation
(\cite{sevcikova04snowFT}).

\subsection{Conclusion}

The routines available in package \pkg{snow} are easy to understand
and use, provided that there is a corresponding communication
environment set up. Generally, the user need not know the underlying
parallel infrastructure, she just ports her sequential code so that it
uses the functions supplied by \pkg{snow}. All in all as the title
suggests simple network of workstations is simple to get started with
and is simple with respect to the possibilities of parallel
computations. 

For further interface functions supplied by the \pkg{snow} package, a
more detailed 
description and further examples please consult the package description
\cite{tierney07snow}.

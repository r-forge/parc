\subsection{The snow package}
\label{sec:snow}
The aim of simple network of workstations (snow --
\cite{tierney07snow}) is to provide a simple parallel computing
environment in R. To make up a virtual cluster in R snow is based on
three different message-passing environments.

\begin{itemize}
\item PVM via R package rpvm (see section \ref{sec:rpvm})
\item MPI via R package Rmpi (see section \ref{sec:Rmpi})
\item SOCK via TCP sockets
\end{itemize}

When using snow one can rely on a good handful of high-level
functions. Indeed snow uses existing interfaces to R and solely
provides one new possibilty of message-passing namely TCP sockets. 

\subsubsection{Initializing and process spawning}

Initializing a snow cluster is rather easy if the system is prepared
accordingly. When using MPI (achieved through Rmpi) a LAM/MPI
environment has to be booted prior starting the virtual cluster (see
section \ref{sec:Rmpi}). Is PVM the method of choice the rpvm package
must be available and an appropriate PVM has to be started (see
section \ref{sec:rpvm}). For both MPI and PVM the parallel environment
can be configured through a grid engine (see appendix
\ref{app:gridengine}). TCP sockets can be set up directly with the
package. When using either MPI or PVM status queries to the
corresponding environment can be made using the functions supplied
from the packages mentioned above.


snow management functions:

\begin{description}
\item[\texttt{makecluster(spec, type = getClusterOption(``type''))}]
  starts a cluster of type \texttt{type} with \texttt{spec} numbers of
  slaves. If the cluster is of connection type SOCK then \texttt{spec}
  must be a charactor vector containing the hostnames of the
  slavenodes to join the cluster. The return value is a list
  containing the cluster specifications. It is necessary in further
  function calls.
\item[\texttt{stopCluster(cl)}] stops a cluster \texttt{cl}.
\end{description}


\textbf{Example:} Start/stop cluster in snow \newline
running on cluster@WU using the node.q -- the parallel environment was
started with the SGE using 8 nodes.

<<echo=TRUE>>=
library("snow")
set.seed(1782)
## Start a MPI cluster using 8 slaves
n <- 8
cl <- makeCluster(n, type = "MPI")


@ 


\subsubsection{Built-in high-level functions}

snow provides a good handful of high-level functions: 


%% TODO
\begin{description}
\item[\texttt{mpi.apply(x, fun, ..., comm = 1)}] applies a function
  \texttt{fun} with additional arguments \ldots to a specific part of
  a vector \texttt{x}. The return value is of type list. The length of
  \texttt{x} must not exceed the 
  number of R slaves spawned as each element of the vector is used
  exactly by one slave. To achieve some sort of load balancing please
  use the corresponding apply functions below.
\item[\texttt{mpi.applyLB(x, fun, ..., comm = 1)}] applies a function
  \texttt{fun} with additional arguments \ldots to a specific part of
  a vector \texttt{x}. There are a few more variants explained in
  \cite{yu06Rmpi}.
\item[\texttt{mpi.bcast.cmd(cmd = NULL, rank = 0, comm = 1)}]
  broadcasts a command \texttt{cmd} from the sender \texttt{rank} to
  all R slaves and evaluates it.
\item[\texttt{mpi.bcast.Robj(obj, rank = 0,comm = 1)}]
  broadcasts an R object \texttt{obj} from process rank \texttt{rank}
  to all other processes (master and slaves).
\item[\texttt{mpi.bcast.Robj2slave(obj, comm = 1)}] broadcasts an R
  object \texttt{obj} to all R slaves from the master process. 
\item[\texttt{mpi.parSim( ... )}] carries out a Monte Carlo simulation
  in parallel. For details on this function see the package manual
  (\cite{yu06Rmpi}) and
  the applications in chapter \ref{chap:options}.
\end{description}


\textbf{Example:} Using mpi.apply\newline
running on cluster@WU using the node.q -- the parallel environment was
started with SGE using 8 nodes

<<echo=TRUE>>=

n <- 4
# spawn n slaves
mpi.spawn.Rsalves(nslaves = n)
# build up a n x n matrix in parallel
x <- rep(n,n)
rows <- mpi.apply(x, runif())
X <- matrix(unlist(rows), ncol = n, byrow=TRUE)
X
mpi.close.Rsalves(nslaves = n)yu06Rmpi

@ 


\subsubsection{Other important functions}

To complete the set of important functions supplied by the Rmpi
package the following functions have to be explained:

\begin{description}
\item[\texttt{.PVM.gather(x, count = length(x), msgtag, group,
    rootginst = 0}] gathers data distributed on the nodes (x) to a
  specific process (mostly the root) into a single array. It performs
  a send of messages from each member of a group of processes. A
  specific process (the root) accumulates this messages into a single vector.
\item[\texttt{.PVM.scatter(x, count, msgtag, group, rootqinst = 0}]
  sends to each member of a group a partition of  a vector x from a
  specified member of the group (mostly the root) where \texttt{count}
  is an integer specifying the number of elements to be sent to each
  member. 
\end{description}

\subsubsection{conclusion}

%The \texttt{PVM.rapply()} example shown in this section followed the Single Program
%Multiple Data (SPMD) paradigm. Data is splitted into different parts
%which are sent to different processes. I/O is handled by a master
%process. When loading rpvm in an R session this session becomes the
%master process. Slaves can easily be spawned provided that there are
%working slave scripts available. A major disadvantage is that the rpvm
%package only has two higher-level function. One of them can be used
%for calculations. That means when using this package for HPC one has
%to deal with low-level message-passing which in turn provides high
%flexibility. New parallel functions can be constructed on the basis of
%the provided interface.

For further interface functions supplied by the Rmpi package, a more detailed
description and further examples please consult the package description
\cite{yu06Rmpi}.

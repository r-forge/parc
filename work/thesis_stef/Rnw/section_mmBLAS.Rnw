%% Initialization
<<echo=FALSE,results=hide>>=

library("paRc")
library("xtable")
load("../examples/Rda/bmres-shared-1000.Rda")
load("../examples/Rda/bmres-shared-2500.Rda")
@ 

\SweaveOpts{echo=FALSE}

\subsection{A Comparison of Basic Algorithms with BLAS}

To show how the BLAS routines perform on a single machine in
comparison to the basic C implementation we ran a benchmark on a
bignode of cluster@WU. The complexity of computation is defined as the
number of rows and colums a matrix has. We chose to use two $1000
\times 1000$  and $2500 \times
2500$ matrices respectively as grades of complexity (i.e., complexity grade $1000$ and complexity grade $2500$).

\subsubsection{Results of Complexity Grade $1000$}

Table~\ref{tab:mm_seqsblas1000} shows the results of the matrix
multiplication benchmark using the Basic Linear Algebra Subroutines
from the libraries mentioned in Section~\ref{sec:blas}. The entry
``native-BLAS'' in this table refers to the BLAS library delivered
with the R source code. As this library has to be of high portability
it delivers the worst performance. With libraries trimmed for the
corresponding platform a good speedup can be achieved. The Intel MKL
library boosts the matrix multiplication with a speedup of over 26 in
comparison to the C implementation from Example~\ref{ex:serialmm}. The
best BLAS library, in 
this case GotoBLAS, shows a really extraordinary speedup. In fact,
nearly doubling the speedup of the MKL. It
can really compete with if not beat all parallel algorithms for matrix
multiplication shown in this thesis.

<<results=tex>>=
## BLAS Table incl speedup
table_seqsblas <- cbind(bmres_s1000[1:4,c(2,6)],speedup(bmres_s1000[1:4,6]))
colnames(table_seqsblas) <- c("Type","Time","Speedup")

xtable(table_seqsblas, caption="Comparison of BLAS routines on a bignode with complexity grade~$1000$.", label="tab:mm_seqsblas1000")
@  
\subsubsection{Results of Complexity Grade $2500$}

Increasing the complexity to $2500 \times 2500$ matrices an
interesting behaviour is shown. Table~\ref{tab:mm_seqsblas2500} shows
that the speedup achieved with the BLAS routines  has increased though
running the same task on the same machine. The principle of locality
is better utilized as more data is been held in higher levels of the
memory hierarchies. In fact, the matrices are decomposed in blocks and
the probabilty of using the same data (blocks) again in one of the next
calculation steps increases with the complexity of the matrices.

<<results=tex>>=
## BLAS Table incl speedup
table_seqsblas <- cbind(bmres_s2500[1:4,c(2,6)],speedup(bmres_s2500[1:4,6]))
colnames(table_seqsblas) <- c("Type","Time","Speedup")

xtable(table_seqsblas, caption="Comparison of BLAS routines on a bignode with comlexity grade~$2500$.", label="tab:mm_seqsblas2500")
@ 

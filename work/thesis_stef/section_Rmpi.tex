\section{The Rmpi Package}
\label{sec:Rmpi}
The Message Passing Interface (MPI) is a set of library interface
standards for message passing and there are many implementations using
these standards (see also Section~\ref{sec:MPI}).
\pkg{Rmpi} is an interface to MPI (\cite{yu02Rmpi} and
\cite{yu06Rmpi}). As 
of the time of this writing \pkg{Rmpi} uses
the LAM implementation of MPI. For process spawning the standard
MPI-1.2 is required which is available in the LAM/MPI implementation 
as LAM/MPI (version 7.1.3) supports a large portions of the MPI-2
standard. This is necessary if one likes to use interactive spawning
of R processes. With MPI versions prior to MPI-1.2 separate R
processes have to be created by hand using \code{mpirun} (part of many
MPI implementations) for example.

\pkg{Rmpi} contains a lot of low level interface functions to the MPI
C-library. 
Furthermore, a handful of high level functions are supplied. A
selection of routines is going to be presented in this section arranged
into the following topics:

\begin{itemize}
\item Initialization and Status Queries
\item Process Spawning and Communication
\item Built-in High Level Functions
\item Other Important Functions
\end{itemize}  

A windows implementation of this package (which uses MPICH2)
is available from~\url{http://www.stats.uwo.ca/faculty/yu/Rmpi}.

\subsection{Initialization and Status Queries}

The LAM/MPI environment has to be booted prior to using any
message passing library functions. One possibility is to use the
command line, the other is to load the \pkg{Rmpi} package. It automatically
sets up a (small---1 host) LAM/MPI environment (if the executables are
in the search path). 

When using the Sun Grid Engine (SGE) or other queueing systems to boot
the LAM/MPI parallel environment the developer is not engaged with
setting up and booting the environment anymore (see
appendix \ref{app:gridengine} on how to do this). On a cluster of
workstations this is the method of choice. 

\subsubsection{Management and Query Functions}

\begin{description}
\item[\code{lamhosts()}] finds the hostname associated with its node
  number.
\item[\code{mpi.universe.size()}] returns the total number of CPUs
  available to the MPI environment (ie. in a cluster or in a parallel
  environment started by the grid engine).
\item[\code{mpi.is.master()}] returns TRUE if the process is the
  master process or FALSE otherwise. 
\item[\code{mpi.get.processor.name()}] returns the hostname where the
  process is executed.
\item[\code{mpi.finalize()}] cleans all MPI states (this is done when
  calling \code{mpi.exit} or \code{mpi.quit}.
\item[\code{mpi.exit()}] terminates the mpi communication
  environment and detaches the \pkg{Rmpi} package which makes reloading of
  the package \pkg{Rmpi} in the same session impossible.  
\item[\code{mpi.quit()}] terminates the mpi communication
  environment and quits R.  
\end{description}

Example~\ref{ex:Rmpi-init} shows how the configuration of the
parallel environment can be obtained. First it returns the hosts
connected to the parallel environment and then prints the number of
CPUs available in it. After a query if this process is the master
process, the hostname the current process runs on is
returned. 

\begin{Example} Queries to the MPI communication environment
\label{ex:Rmpi-init}
\begin{Schunk}
\begin{Sinput}
> library("Rmpi")
> lamhosts()
\end{Sinput}
\begin{Soutput}
node065 node065 node045 node045 node025 node025 node047 node047 
      0       1       2       3       4       5       6       7 
\end{Soutput}
\begin{Sinput}
> mpi.universe.size()
\end{Sinput}
\begin{Soutput}
[1] 8
\end{Soutput}
\begin{Sinput}
> mpi.is.master()
\end{Sinput}
\begin{Soutput}
[1] TRUE
\end{Soutput}
\begin{Sinput}
> mpi.get.processor.name()
\end{Sinput}
\begin{Soutput}
[1] "node065"
\end{Soutput}
\end{Schunk}
\end{Example}

\subsection{Process Spawning and Communication}

In \pkg{Rmpi} it is easy to spawn R slaves and use them as
workhorses. The 
communication between all the involved processes is carried out in a
so called communicator (comm). All processes within the same
communicator are able to send or receive messages from other
processes. The processes are identified through their commrank (see
also the fundamentals of message passing in
Section~\ref{sec:messagepassing}). The big advantage of \pkg{Rmpi}
slaves is, that they can be used interactively when using the default
R slave script.

\subsubsection{Process Management  Functions}
\begin{description}
\item[\code{mpi.spawn.Rslaves(Rscript =
    system.file(nslaves =
    mpi.universe.size(), ...)}] spawns \code{nslaves} number of R
  workhorses to those hosts automatically chosen by MPI. For other
  arguments represented by \ldots to this function we refer to
  \cite{yu06Rmpi}.
\item[\code{mpi.close.Rslaves(dellog = TRUE, comm = 1)}] closes
  previously spawned R slaves and returns 1 if succesful.
\item[\code{mpi.comm.size()}] returns the total number of members in
  a communicator.
\item[\code{mpi.comm.rank()}] returns the rank (identifier) of the
  process in a communicator.
\item[\code{mpi.remote.exec(cmd, ..., comm = 1, ret = TRUE)}]
  executes a command \code{cmd} on R slaves with \ldots arguments to
  \code{cmd} and returns executed results if \code{ret} is
  \code{TRUE}.
\end{description}

In Example~\ref{ex:Rmpi2} as many slaves are spawned as are available
in the parallel environment. The size of the communicator is returned
(1 master plus the spawned slaves) and a remote query of the commrank
is carried out. Before the slaves are closed the commrank of the
master is printed.

\begin{Example} Process management and communication 

\begin{Schunk}
\begin{Sinput}
> mpi.spawn.Rslaves(nslaves = mpi.universe.size())
\end{Sinput}
\begin{Soutput}
	8 slaves are spawned successfully. 0 failed.
master (rank 0, comm 1) of size 9 is running on: node065 
slave1 (rank 1, comm 1) of size 9 is running on: node065 
slave2 (rank 2, comm 1) of size 9 is running on: node065 
slave3 (rank 3, comm 1) of size 9 is running on: node045 
slave4 (rank 4, comm 1) of size 9 is running on: node045 
slave5 (rank 5, comm 1) of size 9 is running on: node025 
slave6 (rank 6, comm 1) of size 9 is running on: node025 
slave7 (rank 7, comm 1) of size 9 is running on: node047 
slave8 (rank 8, comm 1) of size 9 is running on: node047 
\end{Soutput}
\begin{Sinput}
> mpi.comm.size()
\end{Sinput}
\begin{Soutput}
[1] 9
\end{Soutput}
\begin{Sinput}
> mpi.remote.exec(mpi.comm.rank())
\end{Sinput}
\begin{Soutput}
  X1 X2 X3 X4 X5 X6 X7 X8
1  1  2  3  4  5  6  7  8
\end{Soutput}
\begin{Sinput}
> mpi.comm.rank()
\end{Sinput}
\begin{Soutput}
[1] 0
\end{Soutput}
\begin{Sinput}
> mpi.close.Rslaves()
\end{Sinput}
\begin{Soutput}
[1] 1
\end{Soutput}
\end{Schunk}
\label{ex:Rmpi2}
\end{Example}
 
\subsection{Built-in High Level Functions}

\pkg{Rmpi} provides many high level functions. We selected a few
of them which we think are commonly used. Most of them have been
utilized to build parallel programs presented in the subsequent
chapters.

\subsubsection{High Level Functions}
\begin{description}
\item[\code{mpi.apply(x, fun, ..., comm = 1)}] applies a function
  \code{fun} with additional arguments \ldots to a specific part of
  a vector \code{x}. The return value is of type list with the same
  length as of \code{x}. The length of
  \code{x} must not exceed the 
  number of R slaves spawned as each element of the vector is used
  exactly by one slave. To achieve some sort of load balancing please
  use the corresponding apply functions below.
\item[\code{mpi.applyLB(x, fun, ..., comm = 1)}] applies a function
  \code{fun} with additional arguments \ldots to a specific part of
  a vector \code{x}. There are a few more variants explained in
  \cite{yu06Rmpi}.
\item[\code{mpi.bcast.cmd(cmd = NULL, rank = 0, comm = 1)}]
  broadcasts a command \code{cmd} from the sender \code{rank} to
  all R slaves and evaluates it.
\item[\code{mpi.bcast.Robj(obj, rank = 0,comm = 1)}]
  broadcasts an R object \code{obj} from process rank \code{rank}
  to all other processes (master and slaves).
\item[\code{mpi.bcast.Robj2slave(obj, comm = 1)}] broadcasts an R
  object \code{obj} to all R slaves from the master process. 
\item[\code{mpi.parSim( ... )}] carries out a Monte Carlo simulation
  in parallel. For details on this function see the package manual
  (\cite{yu06Rmpi}) and on Monte Carlo simulation
  the applications in chapter \ref{chap:options}.
\end{description}

How to use the high level function \code{mpi.apply()} is shown in
Example~\ref{ex:Rmpi3}. A vector of $n$ random numbers is generated on
each of the $n$ slaves and are returned to the master as a
list (each list element representing one row). Finally a $n  \times n$
matrix is formed and printed. The output of the matrix shows for each
row the same random numbers. This is because of the fact, that each
slave has the same seed. This problem is more specific treated in
Chapter~\ref{chap:options}. For more information about parallel random
number generators see the descriptions of the packages \pkg{rsprng}
and \pkg{rlecuyer} in Section~\ref{sec:otherpackages}.

\begin{Example} Using mpi.apply
\label{ex:Rmpi3}
\begin{Schunk}
\begin{Sinput}
> n <- 8
> mpi.spawn.Rslaves(nslaves = n)
\end{Sinput}
\begin{Soutput}
	8 slaves are spawned successfully. 0 failed.
master (rank 0, comm 1) of size 9 is running on: node065 
slave1 (rank 1, comm 1) of size 9 is running on: node065 
slave2 (rank 2, comm 1) of size 9 is running on: node065 
slave3 (rank 3, comm 1) of size 9 is running on: node045 
slave4 (rank 4, comm 1) of size 9 is running on: node045 
slave5 (rank 5, comm 1) of size 9 is running on: node025 
slave6 (rank 6, comm 1) of size 9 is running on: node025 
slave7 (rank 7, comm 1) of size 9 is running on: node047 
slave8 (rank 8, comm 1) of size 9 is running on: node047 
\end{Soutput}
\begin{Sinput}
> x <- rep(n, n)
> rows <- mpi.apply(x, runif)
> X <- matrix(unlist(rows), ncol = n, byrow = TRUE)
> round(X, 3)
\end{Sinput}
\begin{Soutput}
     [,1]  [,2]  [,3]  [,4]  [,5]  [,6]  [,7]  [,8]
[1,]  0.5 0.976 0.347 0.266 0.471 0.444 0.069 0.955
[2,]  0.5 0.976 0.347 0.266 0.471 0.444 0.069 0.955
[3,]  0.5 0.976 0.347 0.266 0.471 0.444 0.069 0.955
[4,]  0.5 0.976 0.347 0.266 0.471 0.444 0.069 0.955
[5,]  0.5 0.976 0.347 0.266 0.471 0.444 0.069 0.955
[6,]  0.5 0.976 0.347 0.266 0.471 0.444 0.069 0.955
[7,]  0.5 0.976 0.347 0.266 0.471 0.444 0.069 0.955
[8,]  0.5 0.976 0.347 0.266 0.471 0.444 0.069 0.955
\end{Soutput}
\begin{Sinput}
> mpi.close.Rslaves()
\end{Sinput}
\begin{Soutput}
[1] 1
\end{Soutput}
\end{Schunk}
\end{Example}

\subsection{Other Important Functions}

To complete the set of important functions supplied by the \pkg{Rmpi}
package the following functions have to be explained.

\subsubsection{Collective Communication Routines}
\begin{description}
\item[\code{.mpi.gather(x, type, rdata, root = 0, comm = 1)}] gathers
  data distributed on the nodes (\code{x}) to a 
  specific process (mostly the master) into a single array or list
  (depending on the length of the data) of type \code{type} which can
  be integer, double or character. It
  performs  
  a send of messages from each member in a comm. A
  specific process (\code{root}) accumulates this messages into a 
  single array or list prepared with the \code{rdata} command.
\item[\code{.mpi.scatter(x, type, rdata, root = 0, comm = 1)}]
  sends to each member of a comm a partition of  a vector \code{x},
  type \code{type} which can be integer, double or character,
  from a specified member of the group (mostly the master). Each
  member of the comm receive its part of \code{x} after preparing the
  receive buffer with the argument \code{rdata}.
\end{description}

\subsection{Conclusion}

The package \pkg{Rmpi} implements many of the routines available in
MPI-2. But there are some that have to be ommitted or are not included
because they are not needed for use in R (e.g., data management
routines are not necessary as R has its own tools for data handling).
A really interesting aspect of the \pkg{Rmpi} package is the
possibilty to spawn interactive R slaves. That enables the user to
interactively 
define functions which can be executed remotely on the slaves in
parallel. An example how one can do this is shown in
Chapter~\ref{chap:matrix}, where the implementation of matrix
multiplication is shown. A major disadvantage is that MPI in its
current implementations lack in fault tolerance. This results in a
rather instable execution of MPI applications. Moreover, debugging
is really difficult as there is no support for it in \pkg{Rmpi}.

All in all this package is a good start in creating parallel programs
as this can easy be achieved entirely in R.

For further interface functions supplied by the \pkg{Rmpi} package, a
more detailed description and further examples please consult the
package description \cite{yu06Rmpi}.

\section{The Rmpi package}
\label{sec:Rmpi}
The Message Passing Interface (MPI) is a set of library interface
standards for message-passing and there are many implementations using
these standards (see also section \cite{sec:MPI}).
Rmpi is an interface to MPI (\cite{yu06Rmpi}). As of the time of this
writing Rmpi uses
the LAM implementation of MPI. For process spawning the standard
MPI-1.2 is required which is available in the LAM/MPI 
as LAM/MPI (version 7.1.3) supports a large portions of the MPI-2
standard. There are 
a lot of low-level interface functions to the MPI C-library available.
Furthermore, a handful of high-level functions are supplied by the
Rmpi package. 

A windows implementation of this package (which uses MPICH2)
can also be obtained, but the Microsoft operating system is not in the
scope of this thesis.

\subsection{Initializing and Status queries}

The LAM/MPI environment has to be booted prior to using any
message-passing library functions. One possibility is to use the
command line, the other is to load the Rmpi package. It automatically
sets up a (small--1 host) LAM/MPI environment (if the executables are
in the search path). 

When using the Sun Grid Engine (SGE) to boot the LAM/MPI parallel
environment the developer is not engaged with
setting up and booting the environment anymore (see
appendix \ref{app:gridengine} on how to do this). On a cluster of
workstations this is the method of choice. 


Rmpi management and query functions:

\begin{description}
\item[\texttt{lamhosts()}] finds the hostname associated with its node
  number.
\item[\texttt{mpi.universe.size()}] returns the total number of CPUs
  available to the MPI environment (ie. in a cluster or in a parallel
  environment started by the grid engine).
\item[\texttt{mpi.finalize()}] cleans all MPI states.
\item[\texttt{mpi.exit()}] terminates the mpi communication
  environment and detaches the Rmpi package which makes reloading of
  the package Rmpi in the same session impossible.  
\item[\texttt{mpi.quit()}] terminates the mpi communication
  environment and quits R.  
\end{description}
  

\textbf{Example:} Simple queries using Rmpi \newline
running on cluster@WU using the node.q -- the parallel environment was
started with the SGE using 8 nodes.

\begin{Schunk}
\begin{Sinput}
> library("Rmpi")
> set.seed(1782)
> lamhosts()
\end{Sinput}
\begin{Soutput}
node059 node059 node055 node055 node064 node064 node062 node062 
      0       1       2       3       4       5       6       7 
\end{Soutput}
\begin{Sinput}
> mpi.universe.size()
\end{Sinput}
\begin{Soutput}
[1] 8
\end{Soutput}
\begin{Sinput}
> mpi.is.master()
\end{Sinput}
\begin{Soutput}
[1] TRUE
\end{Soutput}
\begin{Sinput}
> mpi.get.processor.name()
\end{Sinput}
\begin{Soutput}
[1] "node059"
\end{Soutput}
\end{Schunk}

\subsection{Process spawning and communication}

In Rmpi it is easy to spawn R slaves and use them as workhorses.

%% TODO: explain here or in MPI section more about comms ranks etc.
%% mpi.send recv bcast etc. explained here

\begin{description}

\item[\texttt{mpi.spawn.Rslaves(Rscript =
    system.file(nslaves =
    mpi.universe.size(), ...)}] spawns \texttt{nslaves} number of R
  workhorses to those hosts automatically chosen by MPI. For other
  arguments represented by \ldots to this function I refer to
  \cite{yu06Rmpi}.
\item[\texttt{mpi.close.Rslaves(dellog = TRUE, comm = 1)}] closes
  previously spawned R slaves and returns 1 if succesful.
\item[\texttt{mpi.comm.size()}] returns the total number of members in
  a communicator.
\item[\texttt{mpi.comm.rank()}] returns the rank (identifier) of the
  process in a communicator.
\item[\texttt{mpi.remote.exec(cmd, ..., comm = 1, ret = TRUE)}]
  executes a command \texttt{cmd} on R slaves with \ldots arguments to
  \texttt{cmd} and return executed results if \texttt{TRUE}.
\end{description}

\begin{Schunk}
\begin{Sinput}
> mpi.spawn.Rslaves(nslaves = mpi.universe.size() - 1)
\end{Sinput}
\begin{Soutput}
	7 slaves are spawned successfully. 0 failed.
master (rank 0, comm 1) of size 8 is running on: node059 
slave1 (rank 1, comm 1) of size 8 is running on: node059 
slave2 (rank 2, comm 1) of size 8 is running on: node059 
slave3 (rank 3, comm 1) of size 8 is running on: node055 
slave4 (rank 4, comm 1) of size 8 is running on: node055 
slave5 (rank 5, comm 1) of size 8 is running on: node064 
slave6 (rank 6, comm 1) of size 8 is running on: node064 
slave7 (rank 7, comm 1) of size 8 is running on: node062 
\end{Soutput}
\begin{Sinput}
> mpi.comm.size()
\end{Sinput}
\begin{Soutput}
[1] 8
\end{Soutput}
\begin{Sinput}
> mpi.remote.exec(mpi.comm.rank())
\end{Sinput}
\begin{Soutput}
  X1 X2 X3 X4 X5 X6 X7
1  1  2  3  4  5  6  7
\end{Soutput}
\begin{Sinput}
> mpi.comm.rank()
\end{Sinput}
\begin{Soutput}
[1] 0
\end{Soutput}
\begin{Sinput}
> mpi.close.Rslaves()
\end{Sinput}
\begin{Soutput}
[1] 1
\end{Soutput}
\end{Schunk}

\subsection{Built-in high level functions}

Rmpi provides the following high level functions (this is only a selection): 

\begin{description}
\item[\texttt{mpi.apply(x, fun, ..., comm = 1)}] applies a function
  \texttt{fun} with additional arguments \ldots to a specific part of
  a vector \texttt{x}. The return value is of type list with the same
  length as of \texttt{x}. The length of
  \texttt{x} must not exceed the 
  number of R slaves spawned as each element of the vector is used
  exactly by one slave. To achieve some sort of load balancing please
  use the corresponding apply functions below.
\item[\texttt{mpi.applyLB(x, fun, ..., comm = 1)}] applies a function
  \texttt{fun} with additional arguments \ldots to a specific part of
  a vector \texttt{x}. There are a few more variants explained in
  \cite{yu06Rmpi}.
\item[\texttt{mpi.bcast.cmd(cmd = NULL, rank = 0, comm = 1)}]
  broadcasts a command \texttt{cmd} from the sender \texttt{rank} to
  all R slaves and evaluates it.
\item[\texttt{mpi.bcast.Robj(obj, rank = 0,comm = 1)}]
  broadcasts an R object \texttt{obj} from process rank \texttt{rank}
  to all other processes (master and slaves).
\item[\texttt{mpi.bcast.Robj2slave(obj, comm = 1)}] broadcasts an R
  object \texttt{obj} to all R slaves from the master process. 
\item[\texttt{mpi.parSim( ... )}] carries out a Monte Carlo simulation
  in parallel. For details on this function see the package manual
  (\cite{yu06Rmpi}) and
  the applications in chapter \ref{chap:options}.
\end{description}


\textbf{Example:} Using mpi.apply\newline
running on cluster@WU using the node.q -- the parallel environment was
started with SGE using 8 nodes

\begin{Schunk}
\begin{Sinput}
> n <- 8
> mpi.spawn.Rslaves(nslaves = n)
\end{Sinput}
\begin{Soutput}
	8 slaves are spawned successfully. 0 failed.
master (rank 0, comm 1) of size 9 is running on: node059 
slave1 (rank 1, comm 1) of size 9 is running on: node059 
slave2 (rank 2, comm 1) of size 9 is running on: node059 
slave3 (rank 3, comm 1) of size 9 is running on: node055 
slave4 (rank 4, comm 1) of size 9 is running on: node055 
slave5 (rank 5, comm 1) of size 9 is running on: node064 
slave6 (rank 6, comm 1) of size 9 is running on: node064 
slave7 (rank 7, comm 1) of size 9 is running on: node062 
slave8 (rank 8, comm 1) of size 9 is running on: node062 
\end{Soutput}
\begin{Sinput}
> x <- rep(n, n)
> rows <- mpi.apply(x, runif)
> X <- matrix(unlist(rows), ncol = n, byrow = TRUE)
> X
\end{Sinput}
\begin{Soutput}
          [,1]      [,2]      [,3]       [,4]      [,5]     [,6]      [,7]
[1,] 0.6775106 0.2118004 0.6072327 0.03405821 0.9282862 0.969057 0.6612636
[2,] 0.6775106 0.2118004 0.6072327 0.03405821 0.9282862 0.969057 0.6612636
[3,] 0.6775106 0.2118004 0.6072327 0.03405821 0.9282862 0.969057 0.6612636
[4,] 0.6775106 0.2118004 0.6072327 0.03405821 0.9282862 0.969057 0.6612636
[5,] 0.6775106 0.2118004 0.6072327 0.03405821 0.9282862 0.969057 0.6612636
[6,] 0.6775106 0.2118004 0.6072327 0.03405821 0.9282862 0.969057 0.6612636
[7,] 0.6775106 0.2118004 0.6072327 0.03405821 0.9282862 0.969057 0.6612636
[8,] 0.6775106 0.2118004 0.6072327 0.03405821 0.9282862 0.969057 0.6612636
          [,8]
[1,] 0.2767014
[2,] 0.2767014
[3,] 0.2767014
[4,] 0.2767014
[5,] 0.2767014
[6,] 0.2767014
[7,] 0.2767014
[8,] 0.2767014
\end{Soutput}
\begin{Sinput}
> mpi.close.Rslaves()
\end{Sinput}
\begin{Soutput}
[1] 1
\end{Soutput}
\end{Schunk}


\subsection{Other important functions}

To complete the set of important functions supplied by the Rmpi
package the following functions have to be explained:

\begin{description}
\item[\texttt{.PVM.gather(x, count = length(x), msgtag, group,
    rootginst = 0}] gathers data distributed on the nodes (x) to a
  specific process (mostly the root) into a single array. It performs
  a send of messages from each member of a group of processes. A
  specific process (the root) accumulates this messages into a single vector.
\item[\texttt{.PVM.scatter(x, count, msgtag, group, rootqinst = 0}]
  sends to each member of a group a partition of  a vector x from a
  specified member of the group (mostly the root) where \texttt{count}
  is an integer specifying the number of elements to be sent to each
  member. 
\end{description}

\subsection{conclusion}

%The \texttt{PVM.rapply()} example shown in this section followed the Single Program
%Multiple Data (SPMD) paradigm. Data is splitted into different parts
%which are sent to different processes. I/O is handled by a master
%process. When loading rpvm in an R session this session becomes the
%master process. Slaves can easily be spawned provided that there are
%working slave scripts available. A major disadvantage is that the rpvm
%package only has two higher-level function. One of them can be used
%for calculations. That means when using this package for HPC one has
%to deal with low-level message-passing which in turn provides high
%flexibility. New parallel functions can be constructed on the basis of
%the provided interface.

For further interface functions supplied by the Rmpi package, a more detailed
description and further examples please consult the package description
\cite{yu06Rmpi}.
